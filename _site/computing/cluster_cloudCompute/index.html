<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.15.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Cloud Computing at Fred Hutch - Fred Hutch Biomedical Data Science Wiki</title>
<meta name="description" content="The Hutch is just getting started with cloud computing.  Options like thebeagle and koshu clusters, while built in the cloud, are very much a simpleextension of existing infrastructure into cloud providers but does not fully orparticularly efficiently utilize the real capabilities and advantages providedby cloud services.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Fred Hutch Biomedical Data Science Wiki">
<meta property="og:title" content="Cloud Computing at Fred Hutch">
<meta property="og:url" content="http://localhost:4000/computing/cluster_cloudCompute/">


  <meta property="og:description" content="The Hutch is just getting started with cloud computing.  Options like thebeagle and koshu clusters, while built in the cloud, are very much a simpleextension of existing infrastructure into cloud providers but does not fully orparticularly efficiently utilize the real capabilities and advantages providedby cloud services.">







  <meta property="article:published_time" content="2019-04-02T18:09:25-07:00">



  <meta property="article:modified_time" content="2018-11-01T00:00:00-07:00">



  

  


<link rel="canonical" href="http://localhost:4000/computing/cluster_cloudCompute/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="index.html" type="application/atom+xml" rel="alternate" title="Fred Hutch Biomedical Data Science Wiki Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
              <a href="http://localhost:4000"><img src="/images/Biomedical-Data-Science-Wiki-logo.png" alt="FH biomedical data science wiki icon" width="600" align="left|middle" hspace="0"></a>
        <!--<a class="site-title" href="/">Fred Hutch Biomedical Data Science Wiki</a>-->

        <ul class="visible-links">
          
          
          <li class="masthead__menu-item">
            <a href="/generation/gen_index/" >Data Generation</a>
          </li>
          
          
          <li class="masthead__menu-item">
            <a href="/bioinformatics/inf_index/" >Bioinformatics</a>
          </li>
          
          
          <li class="masthead__menu-item">
            <a href="/computing/comp_index/" >Computing</a>
          </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z"
              transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">Computing</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/computing/comp_index/" class="">Overview</a></li>
          
            
            

            
            

            <li><a href="/compdemos/" class="">Resource Library/HOWTOs</a></li>
          
            
            

            
            

            <li><a href="/scicompannounce/" class="">Announcements</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Access and Credentials</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/computing/access_overview/" class="">Overview</a></li>
          
            
            

            
            

            <li><a href="/computing/access_credentials/" class="">Credentials</a></li>
          
            
            

            
            

            <li><a href="/computing/access_methods/" class="">Methods</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Data Storage</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/computing/store_overview/" class="">Overview</a></li>
          
            
            

            
            

            <li><a href="/computing/store_databases/" class="">Database Systems</a></li>
          
            
            

            
            

            <li><a href="/computing/store_posix/" class="">File Storage Systems</a></li>
          
            
            

            
            

            <li><a href="/computing/store_objectstore/" class="">Object Storage Systems</a></li>
          
            
            

            
            

            <li><a href="/computing/store_scratch/" class="">Temporary (Scratch) Storage</a></li>
          
            
            

            
            

            <li><a href="/computing/store_collaboration/" class="">Collaborative Storage Systems</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Large Scale Compute</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/computing/cluster_overview/" class="">Overview</a></li>
          
            
            

            
            

            <li><a href="/computing/linux_linux101/" class="">Linux Operating System</a></li>
          
            
            

            
            

            <li><a href="/computing/resource_overview/" class="">Technologies</a></li>
          
            
            

            
            

            <li><a href="/computing/cluster_software/" class="">Scientific Software</a></li>
          
            
            

            
            

            <li><a href="/computing/cluster_usingSlurm/" class="">Job Management</a></li>
          
            
            

            
            

            <li><a href="/computing/cluster_cloudCompute/" class="active">Cloud Computing</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>
    
  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Cloud Computing at Fred Hutch">
    <meta itemprop="description" content="The Hutch is just getting started with cloud computing.  Options like thebeagle and koshu clusters, while built in the cloud, are very much a simpleextension of existing infrastructure into cloud providers but does not fully orparticularly efficiently utilize the real capabilities and advantages providedby cloud services.">
    <meta itemprop="datePublished" content="April 02, 2019">
    <meta itemprop="dateModified" content="November 01, 2018">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Cloud Computing at Fred Hutch
</h1>
          
          <!-- <section class="page__share"> -->
<font size="2">
  <a href="http://github.com/FredHutch/wiki/blob/master/_computing/cluster_cloudCompute.md"><i class="fas fa-pen-square"></i>
  <span> Edit this Page via GitHub</span></a> &nbsp; &nbsp; &nbsp;
  <a href="http://github.com/FredHutch/wiki/issues"><i class="far fa-comment-dots"></i><span> Comment by Filing an Issue</span></a>&nbsp; &nbsp; &nbsp;
  <a href="https://fhbig.slack.com/#question-and-answer"><i class="fas fa-question-circle"></i>
  <span> Have Questions? Ask them here.</span></a>
</font>
<hr>

<!--  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fcomputing%2Fcluster_cloudCompute%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a> -->
<!-- </section> -->

        </header>
      


      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
              <ul class="toc__menu">
  <li><a href="#aws-batch-overview">AWS Batch Overview</a>
    <ul>
      <li><a href="#essential-terms">Essential terms</a></li>
      <li><a href="#how-it-works">How it works</a></li>
      <li><a href="#analyzing-and-storing-data">Analyzing and storing data</a></li>
      <li><a href="#using-docker-images">Using Docker images</a></li>
      <li><a href="#when-to-use-aws-batch">When to Use AWS batch</a></li>
    </ul>
  </li>
  <li><a href="#how-do-i-use-aws-batch">How do I use AWS Batch?</a></li>
  <li><a href="#get-aws-credentials">Get AWS Credentials</a></li>
  <li><a href="#create-and-deploy-a-docker-image">Create and Deploy a Docker Image</a></li>
  <li><a href="#create-a-job-definition">Create a Job Definition</a></li>
  <li><a href="#using-secrets-in-jobs">Using secrets in jobs</a></li>
  <li><a href="#using-scratch-space">Using scratch space</a></li>
  <li><a href="#submit-your-job">Submit your job</a>
    <ul>
      <li><a href="#which-queue-to-use">Which queue to use?</a></li>
      <li><a href="#submitting-your-job-via-the-aws-cli">Submitting your job via the AWS CLI</a></li>
      <li><a href="#submitting-your-job-via-boto3">Submitting your job via boto3</a>
        <ul>
          <li><a href="#notes-on-using-python">Notes on using Python</a></li>
        </ul>
      </li>
      <li><a href="#submitting-your-job">Submitting your job</a></li>
    </ul>
  </li>
  <li><a href="#monitor-job-progress">Monitor job progress</a>
    <ul>
      <li><a href="#in-the-web-dashboard">In the web dashboard</a></li>
      <li><a href="#from-the-command-line">From the command line</a></li>
    </ul>
  </li>
  <li><a href="#view-job-logs">View Job Logs</a>
    <ul>
      <li><a href="#in-the-web-dashboard-1">In the web dashboard</a></li>
      <li><a href="#on-the-command-line">On the command line</a>
        <ul>
          <li><a href="#on-rhino-or-gizmo">On Rhino or Gizmo</a></li>
          <li><a href="#on-other-systems">On other systems</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <p>The Hutch is just getting started with cloud computing.  Options like the
<code class="highlighter-rouge">beagle</code> and <code class="highlighter-rouge">koshu</code> clusters, while built <em>in</em> the cloud, are very much a simple
extension of existing infrastructure into cloud providers but does not fully or
particularly efficiently utilize the real capabilities and advantages provided
by cloud services.</p>

<p>If you are at all interested or curious about cloud solutions and would like to
talk over options, Scientific Computing hosts a cloud-specific office hours
every week.  Dates and details for SciComp office hours can be found in
<a href="https://centernet.fredhutch.org/cn/e/center-it/scicomp_nextgen_officehours10092018.html">CenterNet</a>.</p>

<h2 id="aws-batch-overview">AWS Batch Overview</h2>
<p>The Amazon Web Service (AWS) is a “cloud” computing provider which sells access to computational resources on a minute-by-minute basis. The “cloud” is actually just a simple idea that you can buy <em>access</em> to computers, instead of buying the computers themselves. Anytime you have a bunch of computers sitting in a warehouse running code for a bunch of different users around the world, that’s the “cloud.”</p>

<p>Among the different products offered by AWS, the three most relevant to bioinformatics are:</p>
<ul>
  <li><a href="https://aws.amazon.com/ec2/">Amazon Elastic Compute Cloud</a>, or <em>EC2</em>: a service that provides access to cloud-based computers of various sizes that allow temporary use by researchers to run computing jobs that require larger processors (CPU’s) or more memory than is typically available on a land-based computer.  The computing resources available for a task depend on the choices made about the CPU’s or memory allocation in the specific EC2 instance (a virtual computing environment).</li>
  <li><a href="https://aws.amazon.com/s3/">Amazon Simple Storage Service</a>, or <em>S3</em>:
a service that provides cloud-based data storage in the form of “buckets”, or a pool of data that can be accessed anywhere, anytime via the web by users with credentials allowing the access to that specific bucket. The size and particular security and credentials associated with individual S3 buckets are particularly well suited to scaling and flexibility with respect to access.</li>
  <li><a href="https://aws.amazon.com/batch/"><em>AWS Batch</em></a>: a service which wraps around AWS EC2 resources such that researchers can more easily do computing processes with EC2 instances on data stored in S3.</li>
</ul>

<h3 id="essential-terms">Essential terms</h3>

<ul>
  <li>Docker image: lightweight operating system / virtual machine <a href="https://docs.docker.com/glossary/?term=image">see Docker documentation</a></li>
  <li>CPU: Central processing unit, basically just a unit of computation. Your laptop probably has 4 CPUs, while large servers have dozens.</li>
</ul>

<h3 id="how-it-works">How it works</h3>

<p>The basic idea behind AWS Batch is that it allows you to run a job,
which consists of (1) a command, inside of a (2) Docker image on a machine,
with a (3) specified amount of compute resources. For example, a
trivial job that you could run would be:</p>

<ol>
  <li>A command to print some text (<code class="highlighter-rouge">echo Hello World</code>), on</li>
  <li>A Docker image that’s able to run the command (<code class="highlighter-rouge">ubuntu:16.04</code>), with</li>
  <li>A specified number of CPUs (e.g. 1) and memory (e.g. 1Gb)</li>
</ol>

<p>Based on those parameters, Batch will <strong>automatically</strong>:</p>

<ol>
  <li>Find a machine with the required number of CPUs and memory</li>
  <li>Download the Docker image</li>
  <li>Run the command within the Docker image</li>
  <li>Shut down the Docker container</li>
  <li>Shut down the machine if it is no longer needed</li>
</ol>

<p>Batch will often combine multiple jobs onto a single machine, if that is
the most cost effective approach, with all jobs effectively isolated from
each other within their own Docker image.
The big idea here is that you can save money by only paying for compute
resources at the exact time you need it, without worrying about how to pick
the most cost effective combination of EC2 instances.</p>

<h3 id="analyzing-and-storing-data">Analyzing and storing data</h3>

<p>Coming from Fred Hutch, one of the biggest things to transition is how you
access and store data. While we are very used to the shared filesystem on
<code class="highlighter-rouge">gizmo</code>, you have none of that available on Batch. Instead, you must download
your data from S3 before analyzing it, and then upload the results back to
S3 when you are done. All data within the Docker image when the command is
complete will be deleted – we refer to this type of storage as “ephemeral.”</p>

<p>It’s very easy to download and upload from S3, but it just means that you
have to get used to keeping your data there in order to use AWS Batch
effectively. As a benefit, it’s much cheaper to store data there compared
to <code class="highlighter-rouge">/fh/fast</code>, and just as stable.</p>

<h3 id="using-docker-images">Using Docker images</h3>

<p>In order to run your code, you must have it packaged up in a Docker image.
While this may be slightly difficult at first, it has the added benefit
that your analysis is highly reproducible. You never have to worry that
some dependency may have changed, and which would change the results. It
is also very easy to publish your results and cite the Docker image as the
definitive record of all of the dependencies and software needed to run your
analysis and generate the published results.</p>

<h3 id="when-to-use-aws-batch">When to Use AWS batch</h3>
<p>AWS <em>Batch</em> is an AWS service that uses Docker containers to build a batch
computing system.  Batch is made up of a queueing system where jobs are defined
and queued, and a computational resource made up of Docker containers to
process those jobs.  Resources are provisioned when there are jobs to be
processed and destroyed when the work is complete.  This results in a very
efficient and cost-effective solution for some work.</p>

<p><em>Batch</em> is useful if you have a fairly standard processing workflow or at least
a step which is fairly consistent.  The classic example for <em>Batch</em> is image
processing: converting a raw image to some other format.  <em>Batch</em> is capable of
much more complicated analyses and pipelines.</p>

<p>As <em>Batch</em> is very much a cloud service, some familiar resources aren’t
available when using this.  Our ubiquitous file systems (home directories,
fast-file, scratch) are not available- data used in <em>Batch</em> is typically stored
in S3 or some other web-available source.  There have been some recent changes which
expand options for data storage which may make some workloads more accessible
to <em>Batch</em>.</p>

<h2 id="how-do-i-use-aws-batch">How do I use AWS Batch?</h2>

<p>SciComp provides access to AWS Batch in two ways:</p>

<ul>
  <li>Via the <a href="https://docs.aws.amazon.com/cli/latest/reference/batch/index.html">AWS Command Line Interface (CLI)</a>.</li>
  <li>Via programmatic interfaces such as Python’s <a href="https://boto3.readthedocs.io/en/latest/reference/services/batch.html#client">boto3</a>. The
earlier version of this library (<code class="highlighter-rouge">boto</code>) is deprecated and should not be used.</li>
</ul>

<p>Access to the AWS Management Console (the web/GUI interface), is not available
to end users at the Center. However, there is a customized, read-only
<a href="https://batch-dashboard.fhcrc.org/">dashboard</a> available which displays information
about compute environments, queues, job definitions, and jobs.
Please report any <a href="https://github.com/FredHutch/batch-dashboard/issues/new">issues</a>
you discover with this dashboard.</p>

<h2 id="get-aws-credentials">Get AWS Credentials</h2>

<p>You will need AWS credentials in order to
use AWS Batch. You can get the credentials
<a href="https://teams.fhcrc.org/sites/citwiki/SciComp/Pages/Getting%20AWS%20Credentials.aspx">here</a>.</p>

<p>Initially, these credentials only allow you
to access your PI’s S3 bucket. To use the
credentials with AWS Batch, you must request
access to Batch.</p>

<p>Request access by emailing <code class="highlighter-rouge">scicomp</code> with the subject
line <strong>Request Access to AWS Batch</strong>.  In your email, <strong>include</strong> the name of your PI.</p>

<p>SciComp will contact you when your access has been granted.</p>

<p>Note that you will not be able to create
compute environments or job queues. If you need a custom compute environment, please contact SciComp.</p>

<h2 id="create-and-deploy-a-docker-image">Create and Deploy a Docker Image</h2>
<p>See our detailed information in the Computing Resource Library <a href="/compdemos/Docker/">here</a> about creating and deploying Docker images, as well as running your own Docker Host.</p>

<h2 id="create-a-job-definition">Create a Job Definition</h2>

<p><a href="https://docs.aws.amazon.com/batch/latest/userguide/job_definitions.html">Job Definitions</a> specify how jobs are to be run. Some of the attributes specified in a job definition include:</p>

<ul>
  <li>Which Docker image to use with the container in your job</li>
  <li>How many vCPUs and how much memory to use with the container †</li>
  <li>The command the container should run when it is started †</li>
  <li>What (if any) environment variables should be passed to the container when it starts †</li>
  <li>Any data volumes that should be used with the container (the compute
environments provided by SciComp include 1TB of scratch space available
at <code class="highlighter-rouge">/scratch</code>). (<strong>Note</strong>: the process of providing scratch space
is going to change soon, check back for updated information).</li>
  <li>What (if any) IAM role your job should use for AWS permissions. This
is important if your job requires permission to access your PI’s
<a href="https://aws.amazon.com/s3/">S3</a> bucket.</li>
</ul>

<p>† = these items can be overridden in individual job submissions.</p>

<h2 id="using-secrets-in-jobs">Using secrets in jobs</h2>
<p>More to come.</p>
<h2 id="using-scratch-space">Using scratch space</h2>

<p>“Scratch space” refers to extra disk space that your job may
need in order to run. By default, not much disk space is
available (but you have infinite space for input and output
files in S3.</p>

<p>The provisioning of scratch space in AWS Batch turns out to
be a very complicated topic. There is no officially supported
way to get scratch space (though Amazon hopes to provide one
in the future), and there are a number of unsupported ways,
each with its own pros and cons.</p>

<p>If you need scratch space, contact SciComp and we can discuss
which approach will best meet your needs.</p>

<p>But first, determine if you <strong>really</strong> need scratch space.
Many simple jobs, where a single command is run on an input file
to produce an output file, can be <em>streamed</em>, meaning S3 can
serve as both the standard input and output of the command.
Here’s an example that streams a file from S3 to the
command <code class="highlighter-rouge">mycmd</code>, which in turn streams it back to S3:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 cp s3://mybucket/myinputfile - | mycmd | aws s3 cp --sse AES256 - s3://mybucket/outputfile
</code></pre></div></div>
<p>In the first <code class="highlighter-rouge">aws</code> command, the <code class="highlighter-rouge">-</code> means “copy the file
to standard output”, and in the second, it means
“copy standard input to S3”. <code class="highlighter-rouge">mycmd</code> knows how to operate
upon its standard input.</p>

<p>By using streams in this way, we don’t require any extra disk
space. Not all commands can work with streaming, specifically
those which open files in random-access mode, allowing seeking
to random parts of the file.</p>

<p>If a program does not open files in random-access mode, but
does not explicitly accept input from <code class="highlighter-rouge">STDIN</code>, or writes more
than one output file, it can still work with streaming
input/output via the use of
<a href="https://github.com/FredHutch/s3uploader">named pipes</a>.</p>

<p>More and more bioinformatics programs can read and write
directly from/to S3 buckets, so this should reduce the need
for scratch space.</p>

<h2 id="submit-your-job">Submit your job</h2>

<p>There are currently two ways to submit jobs:</p>

<ol>
  <li>via the AWS Command Line Interface (CLI): <code class="highlighter-rouge">aws batch submit-job</code>.
Recommended for launching one or two jobs.</li>
  <li>Using Python’s <code class="highlighter-rouge">boto3</code> library. Recommended for launching
larger numbers of jobs.</li>
</ol>

<p>AWS Batch also supports <a href="https://docs.aws.amazon.com/batch/latest/userguide/array_jobs.html">array jobs</a>, which are collections of related jobs.
Each job in an array job has the exact same command line and
parameters, but has a different value for the
environment variable <code class="highlighter-rouge">AWS_BATCH_JOB_ARRAY_INDEX</code>.
So you could, for example, have a script which uses
that environment variable as an index into a list of files,
to determine which file to download and process. Array jobs
can be submitted by using either of the methods listed above.</p>

<p>We are looking into additional tools to orchestrate workflows and pipelines.</p>

<h3 id="which-queue-to-use">Which queue to use?</h3>

<p>No matter how you submit your job, you need to choose
a queue to submit to. At the present time, there are two:</p>

<ul>
  <li><strong>mixed</strong> - This queue uses a compute environment
(also called <strong>mixed</strong>) which uses many
<a href="https://aws.amazon.com/ec2/instance-types/">instance types</a>
from the <code class="highlighter-rouge">C</code> and <code class="highlighter-rouge">M</code> families. Each of the instance types
used is one that the Center has high
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html">limits</a>
for in our account.</li>
  <li><strong>optimal</strong> - This queue uses a compute environment
(also called <strong>optimal</strong>) which uses the instance type
<code class="highlighter-rouge">optimal</code>, meaning Batch will choose from among the
<code class="highlighter-rouge">C</code>, <code class="highlighter-rouge">M</code>, and <code class="highlighter-rouge">R</code> instance types. While the Center’s
account has high limits for most <code class="highlighter-rouge">C</code> and <code class="highlighter-rouge">M</code> types, its
limits for the <code class="highlighter-rouge">R</code> types are lower. Batch has no awareness
of per-account instance limits, so it may try to place
jobs on <code class="highlighter-rouge">R</code> instances which could result in longer
time-to-result.</li>
</ul>

<h3 id="submitting-your-job-via-the-aws-cli">Submitting your job via the AWS CLI</h3>

<p>The easiest way to submit a job is to generate a JSON skeleton
which can (after editing) be passed to  <a href="https://docs.aws.amazon.com/cli/latest/reference/batch/submit-job.html"><code class="highlighter-rouge">aws batch submit-job</code></a>.
Generate it with this command:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws batch submit-job --generate-cli-skeleton &gt; job.json
</code></pre></div></div>

<p>Now edit <code class="highlighter-rouge">job.json</code>, being sure to fill in the following fields:</p>

<ul>
  <li><code class="highlighter-rouge">jobName</code> - a unique name for your job, which should include
your HutchNet ID. . The first character must be alphanumeric, and up to 128 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed.</li>
  <li><code class="highlighter-rouge">jobQueue</code> - the name of the job queue to submit to (which
 has the same name as the compute environment that will be used).
 In most cases, you can use the <code class="highlighter-rouge">mixed</code> queue.</li>
  <li><code class="highlighter-rouge">jobDefinition</code> The name and version of the job definition to use.
 This will be a string followed by a colon and version number, for
 example: <code class="highlighter-rouge">myJobDef:7</code>. You can see all job definitions with
 <a href="https://docs.aws.amazon.com/cli/latest/reference/batch/describe-job-definitions.html"><code class="highlighter-rouge">aws batch describe-job-definitions</code></a>,
optionally passing a <code class="highlighter-rouge">--job-definitions</code> parameter with the name
of one (or more) job definitions. This will show you each version
of the specified definition(s). You can also view job
definitions in the <a href="https://batch-dashboard.fhcrc.org">dashboard</a>.</li>
  <li>If you are using <a href="https://aws.amazon.com/blogs/compute/creating-a-simple-fetch-and-run-aws-batch-job/">fetch-and-run</a>, do NOT edit
the <code class="highlighter-rouge">command</code> field. If you are not using <code class="highlighter-rouge">fetch-and-run</code> you may
want to edit this field to override the default command.</li>
  <li>Set the <code class="highlighter-rouge">environment</code> field to pass environment variables to your jobs.
This is particularly important when using <code class="highlighter-rouge">fetch-and-run</code> jobs; these
require that several environment variables be set. Environment variables
take the form of a list of key-value pairs with the values <code class="highlighter-rouge">name</code> and
<code class="highlighter-rouge">value</code>, see the following example.</li>
</ul>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s2">"environment"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
  </span><span class="p">{</span><span class="w">
    </span><span class="s2">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"FAVORITE_COLOR"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"value"</span><span class="p">:</span><span class="w"> </span><span class="s2">"blue"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="p">{</span><span class="w">
    </span><span class="s2">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"FAVORITE_MONTH"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"value"</span><span class="p">:</span><span class="w"> </span><span class="s2">"December"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p>Now, <strong>delete</strong> the following sections of the file, as we want to
use the default values for them:</p>

<ul>
  <li><code class="highlighter-rouge">dependsOn</code> - this job does not depend on any other jobs.</li>
  <li><code class="highlighter-rouge">parameters</code> - we will not be passing parameters to this job.</li>
  <li><code class="highlighter-rouge">vcpus</code> in the <code class="highlighter-rouge">containerOverrides</code> section.</li>
  <li><code class="highlighter-rouge">memory</code> in the <code class="highlighter-rouge">containerOverrides</code> section.</li>
  <li><code class="highlighter-rouge">retryStrategy</code> section.</li>
</ul>

<p>With all these changes made, your <code class="highlighter-rouge">job.json</code> file will look something
like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="s2">"jobName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"jdoe-test-job"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"jobQueue"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mixed"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"jobDefinition"</span><span class="p">:</span><span class="w"> </span><span class="s2">"myJobDef:7"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"containerOverrides"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="s2">"command"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="s2">"echo"</span><span class="p">,</span><span class="w">
            </span><span class="s2">"hello world"</span><span class="w">
        </span><span class="p">],</span><span class="w">
        </span><span class="s2">"environment"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
            </span><span class="p">{</span><span class="w">
                </span><span class="s2">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"FAVORITE_COLOR"</span><span class="p">,</span><span class="w">
                </span><span class="s2">"value"</span><span class="p">:</span><span class="w"> </span><span class="s2">"blue"</span><span class="w">
            </span><span class="p">},</span><span class="w">
            </span><span class="p">{</span><span class="w">
                </span><span class="s2">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"FAVORITE_MONTH"</span><span class="p">,</span><span class="w">
                </span><span class="s2">"value"</span><span class="p">:</span><span class="w"> </span><span class="s2">"December"</span><span class="w">
            </span><span class="p">}</span><span class="w">
        </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Once your <code class="highlighter-rouge">job.json</code> file has been properly edited, you can
submit your job as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws batch submit-job --cli-input-json file://job.json
</code></pre></div></div>

<p>This will return some JSON that includes the job ID. Be sure and save
that as you will need it to track the progress of your job.</p>

<h3 id="submitting-your-job-via-boto3">Submitting your job via <code class="highlighter-rouge">boto3</code></h3>

<h4 id="notes-on-using-python">Notes on using Python</h4>

<ul>
  <li>We strongly encourage the use of Python 3. It has been the current
version of the language since 2008. Python 2 will eventually no longer
be supported.</li>
  <li>We recommend using <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">Virtual Environments</a>,
particularly <a href="https://docs.pipenv.org/">pipenv</a>,
to keep the dependencies of your various projects isolated from each
other.</li>
</ul>

<p>Assuming <code class="highlighter-rouge">pipenv</code>  and <code class="highlighter-rouge">python3</code> are installed, create a virtual environment as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipenv --python $(which python3) install boto3
</code></pre></div></div>

<p>Activate the virtual environment with this command:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipenv shell
</code></pre></div></div>

<p>You can now install more Python packages using <code class="highlighter-rouge">pipenv install</code>.
See the <a href="https://docs.pipenv.org/">pipenv documentation</a> for more
information.</p>

<h3 id="submitting-your-job">Submitting your job</h3>

<p>Paste the following code into a file called <code class="highlighter-rouge">submit_job.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env python3</span>
<span class="s">"Submit a job to AWS Batch."</span>

<span class="kn">import</span> <span class="nn">boto3</span>

<span class="n">batch</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s">'batch'</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">submit_job</span><span class="p">(</span><span class="n">jobName</span><span class="o">=</span><span class="s">'jdoe-test-job'</span><span class="p">,</span> <span class="c"># use your HutchNet ID instead of 'jdoe'</span>
                            <span class="n">jobQueue</span><span class="o">=</span><span class="s">'mixed'</span><span class="p">,</span> <span class="c"># sufficient for most jobs</span>
                            <span class="n">jobDefinition</span><span class="o">=</span><span class="s">'myJobDef:7'</span><span class="p">,</span> <span class="c"># use a real job definition</span>
                            <span class="n">containerOverrides</span><span class="o">=</span><span class="p">{</span>
                                <span class="s">"command"</span><span class="p">:</span> <span class="p">[</span><span class="s">'echo'</span><span class="p">,</span> <span class="s">'hello'</span><span class="p">,</span> <span class="s">'world'</span><span class="p">],</span> <span class="c"># optionally override command</span>
                                <span class="s">"environment"</span><span class="p">:</span> <span class="p">[</span> <span class="c"># optionally set environment variables</span>
                                    <span class="p">{</span><span class="s">"name"</span><span class="p">:</span> <span class="s">"FAVORITE_COLOR"</span><span class="p">,</span> <span class="s">"value"</span><span class="p">:</span> <span class="s">"blue"</span><span class="p">},</span>
                                    <span class="p">{</span><span class="s">"name"</span><span class="p">:</span> <span class="s">"FAVORITE_MONTH"</span><span class="p">,</span> <span class="s">"value"</span><span class="p">:</span> <span class="s">"December"</span><span class="p">}</span>
                                <span class="p">]</span>
                            <span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Job ID is {}."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s">'jobId'</span><span class="p">]))</span>

</code></pre></div></div>

<p>Run it with</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 submit_job.py
</code></pre></div></div>

<p>If you had dozens of jobs to submit, you could do it with a <code class="highlighter-rouge">for</code>
loop in python (but consider using
<a href="https://docs.aws.amazon.com/batch/latest/userguide/array_jobs.html">array jobs</a>).</p>

<h2 id="monitor-job-progress">Monitor job progress</h2>

<p>Once your job has been submitted and you have a job ID, you can use it to
retrieve the job status.</p>

<h3 id="in-the-web-dashboard">In the web dashboard</h3>

<p>Go to the <a href="https://batch-dashboard.fhcrc.org/#jobs_header">jobs table</a> in
the dashboard. Paste your job ID or job name into the <strong>Search</strong> box.
This will show the current status of your job. Click the job ID
to see more details.</p>

<h3 id="from-the-command-line">From the command line</h3>

<p>The following command will give comprehensive information about your job,
given a job ID:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws batch describe-jobs --jobs 2c0c87f2-ee7e-4845-9fcb-d747d5559370
</code></pre></div></div>
<p>If you are just interested in the status of the job, you can pipe
that command through <code class="highlighter-rouge">jq</code> (which you may have to
<a href="https://stedolan.github.io/jq/download/">install</a> first) as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws batch describe-jobs --jobs  2c0c87f2-ee7e-4845-9fcb-d747d5559370 \
| jq -r '.jobs[0].status'
</code></pre></div></div>

<p>This will give you the status (one of <code class="highlighter-rouge">SUBMITTED, PENDING, RUNNABLE,
  STARTING, RUNNING, FAILED, SUCCEEDED</code>).</p>

<h2 id="view-job-logs">View Job Logs</h2>

<p>Note that you can only view job logs once a job has reached the <code class="highlighter-rouge">RUNNING</code>
state, or has completed (with the <code class="highlighter-rouge">SUCCEEDED</code> or <code class="highlighter-rouge">FAILED</code> state).</p>

<h3 id="in-the-web-dashboard-1">In the web dashboard</h3>

<p>Go to the <a href="https://batch-dashboard.fhcrc.org/#jobs_header">job table</a> in the
web dashboard. Paste your job’s ID into the <strong>Search</strong> box. Click on
the job ID. Under <strong>Attempts</strong>, click on the <strong>View logs</strong> link.</p>

<h3 id="on-the-command-line">On the command line</h3>

<h4 id="on-rhino-or-gizmo">On Rhino or Gizmo</h4>

<p>On the <code class="highlighter-rouge">rhino</code> machines or the <code class="highlighter-rouge">gizmo</code> cluster, there’s a quick command
to get the job output. Be sure and use your actual job ID instead of
the example one below:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>get_batch_job_log 2c0c87f2-ee7e-4845-9fcb-d747d5559370
</code></pre></div></div>

<p>You can also pass a log stream ID (see below) instead of a job ID.</p>

<h4 id="on-other-systems">On other systems</h4>

<p>If you are on another system without the <code class="highlighter-rouge">get_batch_job_log</code> script
(such as your laptop), you can still monitor job logs, but you need to
get the log stream ID first.</p>

<p>To get the log stream for a job, run this command:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws batch describe-jobs --jobs 2c0c87f2-ee7e-4845-9fcb-d747d5559370
</code></pre></div></div>

<p>(Note that you can add additional job IDs (separated by a space) to
get the status of multiple jobs.)</p>

<p>Once a job has reached
the <code class="highlighter-rouge">RUNNING</code> state, there will be a <code class="highlighter-rouge">logStreamName</code> field that
you can use to view the job’s output. To extract only the <code class="highlighter-rouge">logStreamName</code>,
pipe the command through <a href="https://stedolan.github.io/jq/download/">jq</a>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws batch describe-jobs --jobs 2c0c87f2-ee7e-4845-9fcb-d747d5559370 \
jq -r '.jobs[0].container.logStreamName'
</code></pre></div></div>

<p>Once you have the log stream name, you can view the logs:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws logs get-log-events --log-group-name /aws/batch/job \
--log-stream-name jobdef-name/default/522d32fc-5280-406c-ac38-f6413e716c86
</code></pre></div></div>

<p>This outputs other information (in JSON format) along with your log
messages and can be difficult to read. To read it like an ordinary
log file, pipe the command through <code class="highlighter-rouge">jq</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws logs get-log-events --log-group-name /aws/batch/job \
 --log-stream-name jobdef-name/default/522d32fc-5280-406c-ac38-f6413e716c86 \
| jq -r '.events[]| .message'
</code></pre></div></div>

<p><strong>NOTE</strong>: <code class="highlighter-rouge">aws logs get-log-events</code> will only retrieve 1MB worth of
log entries at a time (up to 10,000 entries). If your job has created
more than 1MB of output, read the
<a href="https://docs.aws.amazon.com/cli/latest/reference/logs/get-log-events.html">documentation</a>
of the <code class="highlighter-rouge">aws batch get-log-events</code> command to learn about retrieving multiple
batches of log output. (The <a href="#on-rhino-or-gizmo">get_batch_job_log</a> script on rhino/gizmo automatically
handles multiple batches of job output, using the
<a href="https://boto3.readthedocs.io/en/latest/reference/services/logs.html#CloudWatchLogs.Client.get_log_events">equivalent command</a>
in <code class="highlighter-rouge">boto3</code>.</p>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-11-01">November 01, 2018</time></p>
        
      </footer>

      <!-- <section class="page__share"> -->
<font size="2">
  <a href="http://github.com/FredHutch/wiki/blob/master/_computing/cluster_cloudCompute.md"><i class="fas fa-pen-square"></i>
  <span> Edit this Page via GitHub</span></a> &nbsp; &nbsp; &nbsp;
  <a href="http://github.com/FredHutch/wiki/issues"><i class="far fa-comment-dots"></i><span> Comment by Filing an Issue</span></a>&nbsp; &nbsp; &nbsp;
  <a href="https://fhbig.slack.com/#question-and-answer"><i class="fas fa-question-circle"></i>
  <span> Have Questions? Ask them here.</span></a>
</font>
<hr>

<!--  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fcomputing%2Fcluster_cloudCompute%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a> -->
<!-- </section> -->


      
  <nav class="pagination">
    
      <a href="/computing/access_overview/" class="pagination--pager" title="Computing Access Overview
">Previous</a>
    
    
      <a href="/computing/cluster_overview/" class="pagination--pager" title="Large Scale Compute Overview
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form onsubmit="return googleCustomSearchExecute();" id="cse-search-box-form-id">
    <input type="text" id="cse-search-input-box-id" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    </form>
    <div id="results" class="results">
        <gcse:searchresults-only></gcse:searchresults-only>    
    </div></div>

      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
          <li><a href="/contributors/"><i class="fas fa-user-edit"></i> See our Contributors</a></li>

          <li><a href="https://github.com/FredHutch/wiki"><i class="fab fa-fw fa-github"></i> See our GitHub Repo</a></li>

          <li><a href="https://fhbig.slack.com/"><i class="fab fa-fw fa-slack"> </i> Join the Conversation on Slack</a></li>

          <li><a href="https://fredhutch.github.io/FHBig/"><img src="/images/FHBig.png" alt="fhbig" width="40" align="left|middle" hspace="0"> Find Upcoming Events</a></li>

          <li><a href="https://www.fredhutch.io/"><img src="/images/fhio.png" alt="fredhutchio" width="20" align="left|middle" hspace="0"> Find Training</a></li>


  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Fred Hutch Biomedical Data Science Wiki. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.7.1/js/all.js" integrity="sha384-eVEQC9zshBn0rFj4+TU78eNA19HMNigMviK/PU/FFjLXqa/GKPgX58rvt5Z8PLs7" crossorigin="anonymous"></script>


<script>
  (function () {
    var cx = '018020560330782233421:waz3ch9w_wg';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();

  function googleCustomSearchExecute() {
    var input = document.getElementById('cse-search-input-box-id');
    var element = google.search.cse.element.getElement('searchresults-only0');
    if (input.value == '') {
      element.clearAllResults();
    } else {
      element.execute(input.value);
    }
    return false;
  }

  
</script>





  </body>
</html>
