var store = [{
        "title": "Computing Resource Credentials",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/access_credentials/",
        "teaser":null},{
        "title": "Computing Resource Access Methods",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/access_methods/",
        "teaser":null},{
        "title": "Computing Access Overview",
        "excerpt":"This section focuses on information describing what types of credentials are needed for various Center IT supported computing resources, as well as instructions for accessing those resources.   Credentials  An overview of what credentials you need for which resource, and how to get them.   Methods  Overviews of methods for accessing computing resources here at the Hutch.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/access_overview/",
        "teaser":null},{
        "title": "Overview of Bioinformatics and Computing at Fred Hutch",
        "excerpt":"Center IT supports a wide array of resources made available to researchers.  Much of the basic computing information needed on an ongoing basis can be found via Centernet and the Center IT pages.  However, much of the scientific computing resource documentation beyond this material is provided in this section of the Wiki, and links to existing resources and documentation are provided when available.   Access and Credentials  This section includes a variety of information about accessing computing resources at the Fred Hutch, including managing credentials for services when required.     Credentials   Methods   Data Storage  The Hutch, through Center IT and Scientific Computing, support a number of options for storing your data. The service you use to store your data will depend on the nature of the data and the anticipated use.      Database Systems   File Storage Systems   Object Storage Systems   Temporary Storage: Scratch   Collaborative Storage Systems   Programming  While there are many types of programming languages various software used in scientific research are written in, there are a handful of specific languages that are commonly used in the process of doing a wide range of research tasks. We will introduce the most common here.      R, RStudio   Python   Linux, Unix and Bash   Managing and Sharing Code   Software Develoment Pracrices   Large Scale Compute  This section contains articles that describe a range of high performance computing resource options available to Fred Hutch researchers.      Technologies   Computing Environments and Containers   Job Management   Cloud Computing   Reference Material  We maintain a space for both announcements from Scientific Computing as well as a Resource Library at the following links:     Updates and Announcements   Resource Library, Demo’s and HOWTOs  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/comp_index/",
        "teaser":null},{
        "title": "Cloud Computing",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/compute_cloud/",
        "teaser":null},{
        "title": "Computing Environments and Containers",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/compute_environments/",
        "teaser":null},{
        "title": "Computing Job Management",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/compute_jobs/",
        "teaser":null},{
        "title": "Large Scale Computing Overview",
        "excerpt":"This section contains articles that describe a range of high performance computing resource options available to Fred Hutch researchers.   Technologies  This page describes all the various technologies and specifications for them that are supported by Fred Hutch for large scale computing.   Computing Environments and Containers  A wide range of pre-built scientific software is available for use in large scale computing work.  This page describes how to find out what is available, how to use it and what to do if something you need is not yet available.   Job Management  When doing large scale computing tasks, one often shifts to the use of jobs to perform specific tasks.  This page provides some background on managing and interacting with your tasks.   Cloud Computing  Beyond on premise resources, cloud computing access is available for Fred Hutch researchers and this page provides some basics on how to get started if you are in need of cloud computing specifically.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/compute_overview/",
        "teaser":null},{
        "title": "Supported Technologies and Platforms",
        "excerpt":"The Fred Hutch provides researchers on campus access to high performance computing using on-premise resources.  The various technologies provided are outlined on here along with the basic information required for researchers to identify which resource might be best suited to their particular computing needs.   The Fred Hutch managed systems listed serve needs that rise above those that can be met using your desktop computer or web-based services. Often reasons to move to these high performance computing (HPC) resources include:     reproducible compute jobs   version controlled and/or specialized software   increased compute capability   rapid access to large data sets in central data storage locations   Overview of On-Premise Resources                  Compute Resource       Access Interface       Resource Admin       Connection to FH Data Storage                       Gizmo       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                 Beagle       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Center IT       home, fast, economy, AWS-S3, and Beagle-specific scratch                 Rhino       CLI, FH credentials on campus/VPN off campus       Scientific Computing       Direct to all local storage types                 NoMachine       NX Client, FH credentials on campus/VPN off campus       Scientific Computing       Direct to all local storage types                 Python/Jupyter Notebooks       Via Rhino (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                 R/R Studio       Via Rhino (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types           Gizmo and Beagle Cluster   While we generally don’t recommend interactive computing on the HPC clusters- interactive use can limit the amount of work you can do and introduce “fragility” into your computing- there are many scenarios where interactively using cluster nodes is a valid approach.  For example, if you have a single task that is too much for a rhino, opening a session on a cluster node is the way to go.   If you need an interactive session with dedicated resources, you can start a job on the cluster using the command grabnode.  The grabnode command will start an interactive login session on a cluster node.  This command will prompt you for how many cores, how much memory, and how much time is required   This command can be run from any NoMachine or rhino host.      NOTE: at this time we aren’t running interactive jobs on Beagle nodes.  If you have a need for this, please email scicomp.    Available Resources  VMs, shiny, rancher, data transfer   Community Resources (not specifically supported by IT)  Are there things people use that we don’t really support?   Proposal Preparation  A description of computational and storage resources from Scientific Computing for grant writers can be found here.   &lt;!– ## Self Service Resources Jupyterhub, RStudio, db4sci, Galaxy, etc.   Gory Details on Node Classes   Resource Table  This table is auto-generated based on the yaml in _data/scicomp_resources.yaml, and is a work in progress.                  Name       Type       Authentication       Authorization       Location                       rstudio       web       web       hutchnetID       FHCRC                 proxmox       VM cluster       web       hutchnetID       FHCRC           Cluster Node Table  This table is auto-generated based on the yaml in _data/cluster_nodes.yaml:   GIZMO  Location: FHCRC                  Partition       Node Name       Node Count       CPU       Cores       Memory                       campus       f       456       Intel E3-1270v3       4       32GB                 largenode       g       18       Intel E5-1234v666       6       256GB                 largenode       h       3       Intel E5-1234v666       14       768GB                 none (interactive use)       rhino       3       Intel E5-1234v666       14       384GB           Additional resources                  Node Name       Network       Local Storage                       f       1G (up to 100MB/s throughput)       800GB @ /loc  (ca 100 MB/s throughput)                 g       10G (upto 1GB/s throughput)       5TB @ /loc (300MB/s throughput / 1000 IOPS) and 200GB @ /loc/ssd (1GB/s throughput / 500k IOPS)                 h       10G (upto 1GB/s throughput)       5TB @ /loc (300MB/s throughput / 1000 IOps) and 200GB @ /loc/ssd (1GB/s throughput / 500k IOPS)                 rhino       10G (up to 1GB/s throughput)       5TB @ /loc (300MB/s throughput/ 1000 IOps)           BEAGLE  Location: AWS                  Partition       Node Name       Node Count       CPU       Cores       Memory                       campus       f       777       Intel c5       4       15GB                 largenode       g       103       Intel c5       18       60GB                 largenode       h       34       Intel r4       16       244GB           Additional resources                  Node Name       Network       Local Storage                       f       EC2       EBS                 g       EC2       EBS                 h       EC2       EBS          ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/compute_platforms/",
        "teaser":null},{
        "title": "R and RStudio",
        "excerpt":"R is a common statistical and computing language used in a variety of biomedical data analyses, visualizations and computing settings.  R itself can be downloaded to install it on your local computer from the Comprehensive R Archive Network project, or CRAN, or via the FH Center IT’s Self Service Tools (on Macs or on PCs).  Call the IT Helpdesk if you do not have permissions to install or update R on your local computer.   RStudio  The RStudio IDE is a free and open source, popular interface to R, which supports a number of added features including a GUI that can be customized to view aspects of your work of most importance (such as plots, files, environment variables, scripts, workspaces, etc). RStudio can be downloaded here and requires an installation of R itself first.  Keep in mind that updates to R/RStudio subsequently will require a two step process, one to update R itself, and the other to update the interface to R, RStudio.   RStudio has a few particularly useful features:     Support for R Markdowns/Notebooks   Integration with git or SVN   Concurrent views of code, plots, files, and environment variables with customized panels.   Direct deployment of Shiny apps via Shinyapps.io   R package management and direct installation capabilities   R Packages and Extensions  There are a number of available resources built on R that are free and open source that can greatly expand the utility of R and RStudio for research purposes.  There are currently three main sources of R packages that are of interest to a majority of the research community.   Bioconductor  Bioconductor is a public repository of R bioinformatics packages. Bioconductor packages are curated for intercompatibility and grouped into workflows (eg. CyTOF, ChIP-seq, eQTL, etc…). New Bioinformatic tools often result in a submission of the corresponding packages to Bioconductor.  These are reliable, well vetted packages that undergo a rigorous process for submission.   CRAN  CRAN, (Comprehensive R Archive Network) is a public repository of numerous R packages along with R itself. Numerous packages are available, though packages are not vetted as heavily as Bioconductor and generally are required to successfully be built, but may not always perform reliably, or be fully documented.   GitHub  GitHub hosts many open source R packages. As they are not vetted or peer-reviewed, these packages can be more experimental than those on CRAN or Bioconductor and thus you will want to proceed with caution. Some basic instructions on how to install packages into your local R/RStudio are included in this vignette.   Local (Desktop) Use  When using R/RStudio locally, you have the option to install a number of different packages from multiple sources.  Depending on the source of the package, you may approach downloading and installing them slightly differently but you manage the various packages installed, the versions of them as well as the version of R you are using them with.   Remote (Rhino and Gizmo) Use  If computing resources beyond what is available via your desktop are required, you may consider running R scripts or RStudio from the rhinos or gizmo.  When using R/RStudio on shared computing resources, different options for builds and modules are available that you can take advantage of.  SciComp makes pre-built modules available for researcher use in order to facilitate more reproducible and reliable use of software on the local cluster.   Current R Builds on Rhino/Gizmo  SciComp maintains a current list of the various builds of R available on Gizmo for use by researchers on the EasyBuild site.  Each build has different packages installed and versions of R itself, thus identifying if an existing R build matches your needs is a first step to using R on Gizmo.  If you do not see a build that meets your needs, then contact scicomp with the specific needs of your project.   Rhino  Depending on what OS is on your local computer, the steps are:     If your local computer runs Windows, you should connect to the Rhino’s with NoMachine, then launch RStudio from within NoMachine.   If your local computer is a Mac, you should install XQuartz and connect to Rhino, then launch RStudio from the terminal   If your local computer runs Linux, you simply need to connect to Rhino with X11 forwarding using the -X flag as detailed above for Mac computers.   Gizmo  From Rhino, execute the grabnode command and a node will be allocated on Gizmo after you selected the CPUs and number of days you need the node.   rstudio.fredhutch.org  Lastly, a Hutch supported RStudio server can be accessed at rstudio.fhcrc.org from computers connected to the Hutch network. For more information about using it, please contact scicomp.   The Tidyverse  The Tidyverse is a group of R packages that coordinate together and are commonly used for manipulating and  visualizing data in data science applications.  There are a number of useful packages for research based users that are part of the Tidyverse, and it’s worth the time to learn about them and see how one might employ them to clean, analyze and convey data and results.  DataCamp has an online Introduction to the Tidyverse that can be useful when first evaluating whether these packages might be useful.   Shiny  Shiny is an R package bundled with RStudio that enables the creation of interactive applications powered by R code. These apps can be viewed on any computer running RStudio, or they can be hosted on a server. Scicomp provides instructions for hosting Shiny apps here   Local resources     Seattle useR group   Cascadia RConf, a local yearly conference      NOTE: This article is a work in progress. If you have suggestions or would like to contribute email sciwiki.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/prog_R/",
        "teaser":null},{
        "title": "Software Development Practices",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/prog_development/",
        "teaser":null},{
        "title": "Linux, Unix and Bash",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/prog_linux101/",
        "teaser":null},{
        "title": "Managing and Sharing Code",
        "excerpt":"Version control software allows anyone using a computer to track changes made to computer files over time, which allows both referencing and reverting back to earlier work. Such software is becoming increasingly common among scientists, academics, and other researchers to manage computational work. While originally developed to support code development, version control software is useful for not just computer code, but also data, documentation, or any other files associated with a project. Moreover, version control software streamlines collaborative editing of documents, allowing changes to be easily identified and attribution of changes to be assigned to individual contributors.   What is Git and GitHub?  Git is the name of a free, open source version control software commonly used among academics and software developers. GitHub is a web-based hosting service for projects managed with Git version control software, and includes a wide variety of additional tools which allows users to collaborate, publish files, release new versions of software, and even create webpages (in fact, this wiki is published through GitHub!). Some relevant terminology associated with Git and GitHub include:     repository: a project folder including all files and their history as tracked with Git. A repository can represent a variety of projects, including a piece of software, quarterly report, or research manuscript.            local: exists on your own computer       remote: exists somewhere besides your own computer and must be accessed over a network, such as on GitHub, the cloud, or a shared computer cluster           commit: a change to a file (or set of files) that represents a revision to previous versions   branch: a parallel version of the “master” repository that allows you to test changes without affecting the original (or live) version; changes in a branch can be merged back to the “master” when a particular task has been completed   fork: a copy of someone else’s repository in which you can make changes (and submit a request for the original repository owner to accept these changes)   Additional terms are defined in GitHub’s Glossary.   Using GitHub at Fred Hutch  Anyone may create an individual GitHub account and username for free. These individual accounts allow creation of repositories that are by default published publicly for anyone to view and use. This supports the goal of GitHub to facilitate open, reproducible science and collaboration. Collaborative options include forking repositories belonging to other people and allowing other people to collaborate on your own projects. Depending on the content and nature of your repositories, however, you may require additional flexibility in sharing and collaboration. Some of these options are described below.      Get a GitHub username here.   Join the Fred Hutch GitHub organization by sending your GitHub username to helpdesk and request to be added.   Public and Private Repositories  Repositories are publicly available by default. While this supports open science practices, there are also cases in which code or data in a repository needs to be kept private, such as when data or code are proprietary or need to be kept secure for other legal or ethical reasons. Although GitHub provides unlimited free public repositories to all users, and recently has added the option for private repositories for small teams. Repositories created by a user, however, should be primarily for private (non-Fred Hutch related) work. However, the Fred Hutch provides access to both public and private repositories for Fred Hutch related work via the Fred Hutch GitHub organization.   Fred Hutch GitHub Organization  GitHub Organizations are accounts shared by individuals that assist in coordinating large collaborative projects. Scientific Computing maintains a Fred Hutch GitHub Organization through which affiliated employees can create public or private repositories and share access with collaborators external to the Hutch. For individual users working on Fred Hutch related work, creating repositories as part of the insitution is preferred as monitoring services exist for the institution’s repositories that do not exist for your own.  These services can identify and help address potential issues such as accidental credential submission or other security related mistakes which can have an impact on you or the Fred Hutch.  For more information on our GitHub Organization, please see this wiki’s section on GitHub in Computing Credentials.   Using GitHub  There are many different ways of sharing projects collaboratively in a GitHub repository. This GitHub Guide for using GitHub is useful for understanding the basics.    There are many different ways to interact with Git and GitHub. Choosing which approach to use depends on whether you’re collaborating, if you are an experienced programmer, and how complex your work in Git will be; the advantages of each of these methods are described below. fredhutch.io’s recommended methods for beginners are described on their software page. In general, a good place to start is the GitHub Desktop client.   GitHub Desktop Client  Developers at Git have developed a desktop application with a graphical user interface that will allow you to accomplish most of the common tasks associated with Git version control and collaboration with GitHub. This is a great place for new Git users to learn about version control; you can download the desktop client for your system for free here. You can find some help getting started with GitHub Desktop here.   This desktop client will allow you to perform the basic Git workflow of making changes that are documented with Git version control. Additionally, the desktop client will allow you to clone repositories from web-based GitHub so that you can work on the project on your own computer.  After you have modified the file or files in your project, you can document and commit those changes back to the remote repository.    Command Line Git  Git was originally written, and continues to be developed, as a command-line tool accessible through Terminal (Mac) and Command Prompt or PowerShell (Windows). This version of Git is the only way you can run all available Git commands, and is appropriate if you are already familiar with the command line and consistently perform work using it. Additionally, you will likely need to use the command line version if you are connecting to remote computational resources like a cluster or cloud. You can learn more about command line Git here and download the command line software here.   GitHub on the Web  GitHub provides a web-based graphical interface for interacting with projects that are published through GitHub. While GitHub’s interface limits your ability to perform more advanced version control operations, there are additional tools available for collaborating, publishing wikis, and managing projects (see section below on “Additional options available in GitHub”).   Git Kraken (Another Desktop Client)  Git Kraken is a desktop client (Mac, PC, and Linux) available for free use to academics. It allows greater control and granularity than GitHub Desktop, especially when dealing with collaborative projects involving lots of branching and forking.  It is good for advanced beginners and beyond.   Git Integration with RStudio  If you are working with R statistical programming, the RStudio interface possesses robust integration with version control.   GitHub API  An API (Application Programming Interface) is a tool that allows you to communicate, or pass information between, two pieces of software. GitHub has an API that you can read more about here, which is useful for developing software that needs to reference information embedded in GitHub repositories.   GitHub and Information Security  With open, agile, collaborative research rapidly becoming the preferred model for scientific inquiry, many researchers at Fred Hutch and elsewhere are adopting Git and GitHub as an essential part of their analytic and publishing workflow. Combining code, data, documentation, and visualizations together in a GitHub repository along with an interactive notebook application provides an enhanced platform for reproducible research and dynamic, computational narratives. However, the same flexibility, ease-of-use, and openness that are major benefits in research collaboration can introduce serious risks when Git is used by design or unintentionally to maintain sensitive data such as protected health information (PHI) or data storage credentials. Git does not provide the same granular access controls, audit logging capabilities, and data encryption available with a database or filesystem certified for HIPAA compliance. A user with read access to a Git repository can readily clone the entire repository, including all current and previous versions of datasets and freely distribute the data through another repository or other communication channels with no record of such activity. While a GitHub repository containing sensitive data can be made “private,” simple misconfiguration or oversight can expose it to the public.   Considering both the large potential benefits and risks of using GitHub and Git, it is important for researchers at Fred Hutch to use these tools effectively while protecting the privacy of patients and Fred Hutch data resources. It is important to understand the types of sensitive information related to security which should never be put on GitHub.  Examples include individually identifiable health or financial information, server names, database or data storage credentials (i.e., usernames, passwords, tokens, access keys, etc), IP address, name of networks, etc.   Given the potential risk associated with even a private repository, however, you should carefully consider alternatives to maintaining any level of sensitive data in public or private GitHub repositories. The preferred method for ensuring privacy and security is keeping all potentially sensitive data out of GitHub repositories. Options for helping accomplish this while realizing the benefits of Git include restructuring workflows to preprocess and sanitize sensitive data in secure backend systems prior to it being posted to Git and storing secrets such as access keys and database credentials in the Vault secrets management system, accessing them when needed from Git using a secure API. For more guidance about how to structure your code or content you are putting in GitHub to ensure security, email scicomp.   Managing Credentials in Your Code  One issue to note when using GitHub to do version control in your code is that it can be very straightforward to inadvertently push content to GitHub that includes things such as API tokens, usernames and passwords, or even your AWS credentials themselves.  Please take care to structure your code in such a way that these “secrets” or anything you perceive to be private information are loaded from an external file or environment variables that themselves are not sent to GitHub.  See our Security page for more information on what is considered a secret or private information and see our Computing Credentials page for more specific guidance about how to structure your code to avoid a problem.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/prog_managecode/",
        "teaser":null},{
        "title": "Overview of Common Programming Languages in Bioinformatics",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/prog_overview/",
        "teaser":null},{
        "title": "Python and the Condas",
        "excerpt":"Educational Resources  There are some publicly accessible resources available to assist in getting started learning and using Python for scientific research computing.  Note the How-To’s and Training section in the sidebar for additional details and in-person training opportunities.     Getting started with Python   Python scientific computing with Scipy   Python Data Analysis Library   Learn Python the Hard Way   Biopython tutorial   Jupyter Notebooks   Jupyter Notebooks are web interfaces to an interpreter shell such as Python. They are most used by data scientists who would like to experiment with their code and easily generate charts and graphs. At Fred Hutch there are at least 4 ways how you can use Jupyter Notebooks, including the latest incarnation called ‘Jupyter Lab’.  You can find more information about Jupyter and related technologies here at the Project Jupyter site.   Jupyter Notebook on your computer   Install the software on your own computer install Jupyter and run locally.   Jupyter on Rhino and Gizmo  Current Python Builds on Rhino/Gizmo  SciComp maintains a current list of the various builds of Python available on gizmo for use by researchers.  Each build has different modules installed and versions of Python itself, thus identifying if an existing Python build matches your needs is a first step to using Python on gizmo.  If you do not see a build that meets your needs here, then contact scicomp with the specific needs of your project.   Rhino  Just load a Python distribution maintained by SciComp and run Jupyter lab:       petersen@rhino1:~$ ml Python/3.6.7-foss-2016b-fh2     petersen@rhino1:~$ jupyter lab --ip=$(hostname) --port=$(fhfreeport) --no-browser      ... or simply use the 'jupyterlab' wrapper script:     petersen@rhino1:~$ jupyterlab   Then connect to the URL, copying the link given by the previous command, which looks as follows:         Copy/paste this URL into your browser when you connect for the first time,     to login with a token:         http://rhino1:11112/?token=0eee692be6c81c1061db  Gizmo   From Rhino execute the grabjupyter command and a node will be allocated on Gizmo after you selected the CPUs and number of days you need the node.   Jupyter on Jupyterhub   SciComp maintains an installation of Jupyterhub. Login with your Hutch Net Id.  (Jupyterhub does not have the latest Python packages)   Also only the first method allows you to install your own python packages as administrator of your machine. The other 3 methods require you to either request a package from Scientific Computing or install the package in your home directory with the –user option (e.g. pip3 install --upgrade --user mypkg) or to create a virtual Python environment, for example:       petersen@rhino1:~$ ml Python/3.6.7-foss-2016b-fh2     petersen@rhino1:~$ python3 -m venv ~/mypython     petersen@rhino1:~$ source ~/mypython/bin/activate     (mypython) petersen@rhino1:~$ jupyter lab     (mypython) petersen@rhino1:~$ deactivate     petersen@rhino1:~$  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/prog_python/",
        "teaser":null},{
        "title": "Overview of How-To and Training",
        "excerpt":"There are a variety of resources for training on various aspects of Bioinformatics and data skills available at the Fred Hutch, in the Seattle area as well as online resources.   Local Training and Resources     fredhutch.io offers frequent, on campus courses on a topics such as R, python or GitHub, and information about previous and upcoming courses is on their site.  In order to sign up for courses, go to MyHutch and click on Hutch Learning to see currently available courses and register for them.   Bioinformatics Shared Resources   Scientific Computing training materials (requires login), related to introduction to Unix and Gizmo, though additional, more recent, information can be see in the Computing section of this site.   FHBig (Fred Hutch Bioinformatics Interest Group), is a Fred Hutch based community group that has created the FHBig website and maintains a slack workspace to support communication between staff at the Hutch.   UW Biostatistics Summer Institutes offer yearly intensive courses over the summers on a wide variety of topics.   Online Learning     Biostar Handbook: A Beginner’s Guide to Bioinformatics   CalTech Learning from Data   The Carpentries, with lessons from Data Carpentry and Software Carpentry   Cognitive Class: Data science and cognitive computing courses   Coursera, including courses on Data Science and Bioinforamtics   CodeAcademy   DataCamp   DataQuest   edX, including courses in Data Analysis and Statistics and Bioinformatics   The Open Source Data Science Masters   ROSALIND interactive bioinformatics problem set   Udacity has courses on Data Science  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/reference_overview/",
        "teaser":null},{
        "title": "Data Storage for Collaboration",
        "excerpt":"There are Fred Hutch supported data storage systems that allow you to share data with people outside the Hutch, with or without a Hutch ID in order to facilitate data transfer and receipt in collaborations within or outside of the Fred Hutch.   Aspera   The Aspera is a storage appliance that runs a heavily tuned storage server and client that enables fast transfer of large data between this system and a host using the Aspera client (either command line or via a browser).  The primary method of operation is to upload the data to the server, then use the web interface to create an email with a link you would then send to those outside the Hutch network.      NOTE: space is limited. Because of this, data stored here is deleted after a short period of time making the Aspera inappropriate for primary storage.  Always keep the primary source on one of the other options above (fast, economy, etc.)    Visit the Aspera information page for more details and information on using this storage service.   OneDrive  OneDrive is a cloud service that securely stores your files and folders in one place, share them with others, and update your files from any device. OneDrive is a benefit available individual users at the Fred Hutch that allows for private storage of files with the ability to share those files with others for collaboration.  With OneDrive you can:      Create documents on your computer and edit on your laptop, phone, or tablet   Collaborate with others in real time   View, store and share files and folders easily   Automatically sync files to your desktop for offline access   Simultaneously edit shared files with other collaborators   The Fred Hutch service Office365 (which includes OneDrive) has been designed with security in mind and comes with features that help achieve compliance with regulations such as HIPAA and FISMA. With that said, the safety of your data depends not only on the design of OneDrive but also on how you use it. You also have control over more sharing options and the ability to restore a previous version of a file.  Your files are viewable only by users to whom you have granted access. Unless a file or folder has been shared, it will remain private.  Once you have installed the OneDrive application on a mobile device you will be able to upload and share documents between computers and devices as well.   Visit the OneDrive CenterNet page for more details and information on using this storage service.  As of Oct 2018 the OneDrive Getting Started Guide is available and currently free storage per user is limited to 2TB.  Please check the linked CenterNet pages for up to date information on OneDrive.   Examples of best practices for using OneDrive include:     Do not sync your Fred Hutch OneDrive with any non-Hutch device   Do leave copies of sensitive data on a non-Hutch device from which you have accessed OneDrive   Sharing Links: Do not select “Everyone” when sharing, instead choose the “Specific people” option whenever possible. If you choose the “People in Fred Hutchinson Cancer Research Center” option, anyone at the Center with a link to your shared file can access it.   Once a file is shared with someone and they download it to their device, they can share it with others.  File protection may also remain an appropriate practice.   Links that share documents do NOT expire.  Remember to remove ability to share when no longer needed.      See additional Best Practices in CenterNet.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/store_collaboration/",
        "teaser":null},{
        "title": "Data Storage in Databases",
        "excerpt":"Database Management Systems (DBMS) are useful if you need to manage large data tables of structured data and / or multiple tables that can be linked to each other (relational databases). Databases are most successfully used by researches for data set for which the data structure (e.g. column names or tables) on a very frequent basis.   Important issues to consider before choosing a data storage location for a data set are:     How does the data need to be accessed by staff members and what is their experience level with databases?   What is the structure of the data set?  E.g., could it be contained in an Excel file, or a set of related Excel worksheets in a file, or are the data more complex in their structure?   Are logging, user-specific permissions, or data capture features desired or required?   How will backups and maintenance be addressed by the research staff?   Depending on the particular needs of the project, there are different data set storage options available supported by the Fred Hutch.   REDCap  REDCap, or Research Electronic Data Capture, is an open source, mature, secure web application for building and managing CRFs, data forms, and participant-completed surveys. REDCap is HIPAA-compliant and supports advanced features such as logging, validation, branching logic, e-signatures, randomization, calculated fields, and programming hooks. REDCap’s mobile app allows allows for offline data capture using iPad and Android tablets. Collected data can be easily exported from REDCap to Excel and common statistical packages (SPSS, SAS, Stata, and R) and has an API.   The beauty of REDCap is that it’s easy to get started but it also has advanced features and programming hooks to serve researchers needs when creating simple surveys all the way to conducting large complex research studies and projects.  For other types of research data, the ability to customize the data dictionary to meet the needs of a particular use make it an excellent option to consider when multiple users need to have a common data source with easy access and no database management knowledge is required.   REDCap has been used by thousands of non-profit institutions throughout the world on hundreds of thousands of projects. The REDCap group at Vanderbilt University manages the development of the REDCap code base; new features are being added every month that make REDCap better and better. At Fred Hutch, the instance of REDCap is supported by Collaborative Data Services (CDS), which is an active member of the REDCap consortium, regularly maintains our REDCap installation so that it remains up-to-date.   CDS provides backup services, database maintenance, support for developing REDCap projects, and is very accessible for staff without previous database experience.   For more information about REDCap at the Fred Hutch, training materials and documentation provided by CDS, visit their site here.   Look for in-person trainings available on a regular basis around campus, here. You will also find recorded training videos and presentation materials at this url..   The login page for the Fred Hutch instance is here.   MS SQL Server  For groups for whom a REDCap system is not suitable, Microsoft SQL technology is available in conjunction with Database Administrator Services such as:      Requirements analysis, review and documentation for internally developed and third party application databases   Provision of database environments for testing and development   Management of database users and permissions   Support for SQL reporting (SSRS), analysis (SSAS) and integrated services (SSIS) platforms   For further info about how to get started with a SQL server, please see the Database Hosting page on Centernet.   DB4Sci  (previously MYDB)  DB4Sci is a service supported by Scientific Computing (SciComp) that allows you to provision a dedicated server (aka, instance).  DB4Sci is a self-service website for creating containerized databases. The service is available to anyone at the center without cost. Users are given full admin rights to the database but are not given shell access to the database system. DB4Sci service performs daily database backups which are stored in the Amazon cloud. To access DB4Sci click the link and use your Hutch credentials to log in.      MariaDB Is the latest open source version of MySQL. MariaDB is traditional relational database that supports SQL. MariaDB has an option for data encryption and data encryption in transit (TLS).  If your project requires encryption at rest you should use MariaDB. For more information visit the MariaDB official site   Postgres Postgres is a very popular open source relational database. Postgres is very performant and capable of storing large databases. For more information visit the Postgres official site.   MongoDB Mongo is a specialized database for storing JSON documents. Mongo is a NoSQL database. For more information visit the MongoDB official site   Neo4j is a NoSQL database for representing graph data. For more information visit the Neo4j official site   More detailed information about DB4Sci can be found on the application website. DB4Sci is and open source project that is under development. Future features are: Self serve restore and Point in Time recovery. Project web site DB4Sci.org  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/store_databases/",
        "teaser":null},{
        "title": "Data Storage in Object Storage Systems (*Economy*)",
        "excerpt":"Object storage systems are not directly attached to your computer via drive mapping, a mount point or the Mac Finder, so you cannot just (double)click on a file to edit it with your favorite application. Most software used in life sciences cannot work directly with an object storage system as if the files were stored in traditional file storage systems. So why would you even want to use it if it seems more complicated than file storage? Object storage systems scale better in capacity and performance and are much cheaper to operate than traditional file storage systems. Cloud Computing depends very much on object storage systems such as Amazon’s AWS S3 or Google Cloud Storage.   Object Storage PI Allocations  As the amount of research data grows, which can occur rapidly when new large scale data is generated, existing externally generated datasets are transferred into the Fast storage, OR if existing data is inadvertently duplicated.  When the space requirements become larger, it is recommended that researchers begin implementing a set of more active data management practices.  Each PI is provided with 5TB of free storage space via Economy storage above which a relatively low cost per TB per month is charged.      Note:  Currently it is recommended to use a combination of Economy, Scratch and Fast.  Please see our other storage pages for more information about what storage locations are best for what type of data and uses.    Economy is less expensive than Fast and is suitable for large scale data sets that are not frequently accessed (i.e., ~monthly or less) but that require a relatively large amount of storage space.  For example, Economy would be suitable for a set of large files such as fastq’s or bam’s that on a daily basis are not accessed, but when a new bioinformatic process is desired, a large “burst” of activity will be occurring that will need to interact with the data.  Economy serves as an archive for these data, and typically compute processes do not directly access these files.   Features &amp; Benefits of Object Storage Systems   Some features and benefits of object storage systems include:           if you need to transfer data from Hutch Campus to cloud the network throughput performance of Object Storage is 10x higher than file storage.            greatly increased file listing performance, for example if you need to list 50000 files in a single directory file storage can take minutes to return the list but if you list an object storage bucket it can return millions of file names within seconds.            it mostly uses the standard http/https protocol which makes it much easier to share data with collaborators all over the world than using a file server and complex VPN technology            you can add additional and arbitrary attributes to each file. Why is this a benefit? Well, normally you just organize your files in folders but what if one file really belongs in multiple folders or projects or departments? Many users end up storing files in multiple different folders to keep relevant data together in one place. Object storage systems do away with folders all together, you just store all files in a single bucket and you can then tag it with many different attributes. These attributes or metadata are stored with the file as key=value pairs such as “cancer=breast” and “grant=P01-123456”. This additional metadata makes it extremely easy to retrieve data for automated pipelines based on different criteria.       Given these benefits it is expected that Object Storage systems will become more common in the future, especially as datasets are getting larger and larger.  Today Fred Hutch offers access to two different Object Storage systems through the Economy Local service. We recommend these systems typically for large genomic data and imaging files that require computational pipelines for processing (e.g. large BAM files) as well as for archival of infrequently used data. Both options for Economy storage are encrypted at rest and are approved to store strictly confidential data such as PHI.   Access to Economy storage is governed by Hutch Net ID (Active Directory) authentication and group membership in a security group called lastname_f_grp (e.g. groudine_m_grp). This is the same security group that governs access to Fast storage.      In the future Fred Hutch Shared Resources data delivery processes (e.g. through  HutchBase) will be modified to deliver data directly to Economy and Scratch storage as opposed to Fast as it happens today.    Accessing Economy Storage  You can access Economy Local or Economy Cloud resources with command line tools such as swc, swift, aws s3 or rclone or libraries such as swiftclient or boto3 for Python or aws.s3 for R.  As of March 2016, Center IT officially supports the use of Mountain Duck and Cyberduck desktop clients on Windows or Mac to move small amounts of data (Gigabytes rather than Terabytes) and occasionally open a file for editing. It is also possible to use Synology to backup data to Economy Local.   Economy Local (Swift)   Economy Local is an object storage system based on Openstack Swift. Economy Local is recommended for research groups who keep large amounts of data (&gt;100TB) on the Fred Hutch campus and frequently use the Gizmo cluster with local storage. We also recommend it for data that is explicitly forbidden to be stored in public cloud storage resources.      In the near future Economy Local will be retrofitted to become a hybrid storage solution. You will be able to access your data conveniently through a file access mount point such as /fh/economy/ in addition to the faster object storage access.    Economy Local is well suited to store petabytes of data at low cost and a high level of data protection. Economy File does not require tape backup as data is replicated to multiple sites. If you accidentally delete data it will be held in a “Trash can” for multiple months during which you have read-only access to the deleted data. Economy File is approved for PHI / PII data.  It is a suitable location to store genomic data  including those governed by the NIH Genomic Data Sharing policies or originating from public repositories while in use locally. Please the demo section for examples of Economy Local   Economy Cloud (S3)   Economy Cloud is a public cloud based object storage service that uses Amazon Web Services Simple Storage Service (S3) to offer managed and secure (encrypted) AWS S3 buckets to Fred Hutch investigators.  While it is not accessible by non-Hutch investigators by default, you can contact scicomp to request access for external research groups.Economy Cloud  is the default choice for Object Storage for every Hutch investigator who does not have any specific requirements.   You can use the Economy Cloud S3 buckets created for each PI to collaborate with external research groups. In addition to the Economy Cloud S3 buckets SciComp maintains S3 transfer buckets for external institutions and sequencing centers. These buckets may not be encrypted to increase interoperability. Please ask your PI to contact SciComp to enable the bucket of your PI for external collaboration or to enable a transfer bucket into which your sequencing center or other large scale data provider can drop large files.   How it Works   S3 (the Simple Storage Service) is an object store very much like the Economy file service, though provided by Amazon Web Services.  Storage resources in S3 are organized much like the other Fred Hutch Object and Block systems, with a “PI bucket” for each investigator at the Hutch which is analogous to the investigator directories available in the traditional file system. A specialized client (the AWS command line interface) is used to upload the data from local storage to S3 storage.  Once there, a temporary URL is created that has the necessary credentials embedded within and is then shared with those needing access.  A secure (HTTPS) connection is then used to download the data (via browser or other client like wget or curl). This URL is temporary and set with a date after which the generated URL is no longer able to access the data, though the data stored here is not removed as with the Aspera.  That temporary URL can be regenerated as necessary.   Backup and Security  Data on this service is not backed up in the traditional sense, but rather versioned: if a new version of a file is uploaded, the older version is saved in S3.  Similarly, if data is deleted, the versions aren’t and can be retrieved.  The Fred Hutch supported PI buckets in S3 are appropriate for storage of restricted data, including PHI.   Credentials and Permissions  Once you have obtained S3 credentials, you can use them to transfer files from/to the PI S3 buckets. If you work in the lab of the PI Jane Doe, your lab’s S3 bucket name will be fh-pi-doe-j. Please substitute your lab’s actual bucket name when using the examples in our Resource Library demos.   User Demos  We have a number of demos in our Resource Library related to how to interact with Economy Storage, specifically via a desktop client, via the AWS CLI, via R, or via Python and various methods for Economy Local      NOTE: This article is a work in progress. If you have suggestions or would like to contribute email sciwiki.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/store_objectstore/",
        "teaser":null},{
        "title": "Overview of Data Storage at Fred Hutch",
        "excerpt":"The Hutch supports a number of options for storing your data, specifically Database storage, File storage, Scratch storage, Object storage and Collaborative storage options.  The storage you use to store your data will depend on the nature of the data and the anticipated use. Here we provide a basic overview of what resources are available to researchers for data storage. For more detailed information on each of these topics summarized here including setup instructions and limitations, please refer to each of the articles in this section of the sidebar.   Storing protected health information (PHI) data   Please check with your supervisor before you store any PHI data on Fred Hutch systems as your data set may have compliance requirements that would not allow you to store the data on any of the existing systems. In almost all cases you should be working with de-identified data which can be stored on any of the above storage systems. If you require to store PHI data you should only use systems that support 1. Encryption at rest (e.g. on the hard drive), 2. Encryption in transit (e.g. through the network) and 3. access auditing (a systems log who accessed a file what time). Also, PHI can only be stored on systems that are approved by ISO.   This is an overview of supported features:                  Feature       Secure File       Fast File       Economy File       OneDrive                       Encryption at Rest       -       -       X       X                 Encryption in Transit       X       -       X       X                 Access auditing       X       -       X       X                 Approved for PHI by ISO       Yes       No       Yes       pending           Additional resources for identifying whether your data is de-identified or not and the Fred Hutch IRB’s information on HIPAA compliance can be found at the linked CenterNet pages.   Storage Allocation and Costs   Please see an overview of file allocation amounts, features and costs on Centernet. For expenses charged for your group for data storage in these systems, please see the storage chargeback portal.  This portal also displays the amount of data in TB that is stored in each location for each Investigator.      Note: You can typically access the monthly billing information during the first 10 days of every month    Database Storage Systems: REDCap, SQL and DB4Sci   There are several options available at the Fred Hutch for storing data in a database system.  These supported systems span a wide range of services to meet the various needs of Fred Hutch researchers.  These include REDCap (supported by Collaborative Data Services based in PHS), MSSQL Server (supported by CIT Enterprise Application Services) and DB4Sci (aka MyDB, supported by SciComp, and provides access to four database engine types including Postgres, MariaDB (MySQL), MongoDB, and Neo4j).   File Storage: Storage in Home, Fast, Secure   File storage keeps your data on disks and allows access to your data using familiar tools you’re used to: Unix commands like cat, cp, ls, and rm,  browser tools like Windows Explorer or OSX’s Finder (to browse drives mapped to your local workstation), and most common Bioinformatic tools. These storage systems are similar to the hard drive on your computer, just typically larger and faster.   Economy Storage: Object Storage   Object storage systems scale better in capacity and performance and are much cheaper to operate than traditional file storage systems. Cloud computing depends very much on object storage systems such as Amazon’s AWS S3. There are a number of features and benefits of object storage systems, such as better sharing of data and much better handling of meta data (e.g. annotations). At Fred Hutch we use object storage predominantly for very large files (e.g. BAM files) and for archiving purposes.   Temporary Storage: Scratch   The scratch file system is a file storage system that works differently than the systems intended for long term data storage. It is maintained by SciComp for temporary storage of research data during active analysis.  This is a large, high-performance storage system.  It is not designed to be as available or as robust as the home or fast file systems (these features were traded for lower cost and greater volume)- that said, it has shown itself to be quite reliable and reasonably fault tolerant.   Collaborative Storage Options   These storage systems have capabilities allowing you to share data with people outside the Hutch, with or without a HutchNet ID.  These include Aspera and OneDrive.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/store_overview/",
        "teaser":null},{
        "title": "Data Storage in File Storage Systems",
        "excerpt":"File keeps your data on disks and allows access to your data using familiar tools you’re used to: Unix commands like cat, cp, ls, and rm,  browser tools like Windows Explorer or OSX’s Finder (to browse drives mapped to your local workstation), and most common Bioinformatic tools.  These storage systems are similar to the hard drive on your computer, just typically larger and faster.   There are multiple file storage resources available to researchers including:     Home for personal files   Fast for shared data, including the majority of large scale research data   Scratch for personal and shared temporary data   Secure for data with higher-level security needs (PHI/encryption/auditing)      Note:  Currently it is recommended to use a combination of Economy, Scratch and Fast.  Please see our other storage pages for more information about what storage locations are best for what type of data and uses.    Home   Home storage is your own personal file storage space at the Fred Hutch.  It is a default place for your Linux login files (profiles &amp;c) as well as an appropriate place to store your own private information.   While you are allowed up to 100GB of storage in this home directory, it is not tailored for heavy use and won’t accommodate large data sets- the 100GB limit cannot be extended.  Also, data in this file system cannot be shared with others.   Fast   Fast storage is a large high-performance system that holds the bulk of the scientific data at the FredHutch.  Each PI is provided with 5TB of free storage space via Fast storage above which a cost per TB per month is charged.  This storage can be accessed by mapping the drive to their workstation (using //center/fh/fast/lastname_f on a Mac or \\\\center\\fh\\fast\\lastname_f or X:\\fast\\lastname_f on a PC). This storage access point can provide members of research groups access to groups of datasets that can have different types of permissions.  Within a PI’s Fast directory, directories can exist for data shared to any Fred Hutch user (/pub), to any member of the PI’s immediate research group (/grp), or private to each individual user in a PI’s immediate research group (/user).  Additionally, links to other data sources can appear here, such as data from the Fred Hutch Genomics Shared Resource (/SR).  This can be customized for a given researcher in conjunction with Scientific Computing (see Available Resources).   The data here is organized by investigator- each folder at the top level is named using the investigators last name and first initial: investigator “Sneezy Smith” would have a directory on fast called smith_s.   On SciComp supported Linux systems you will see this storage in the path /fh/fast.  Windows systems can access this via the UNC path \\\\center.fhcrc.org\\fh\\fast or X:\\fast and OSX hosts using the path smb://center.fhcrc.org/fh/fast.   This storage platform is appropriate for most scientific data- particularly large data sets.  There is no charge for the first 5TB of storage on this system: there is a $30 per-month charge for every terabyte above this.   Secure   Secure storage provides a higher-level of security controls than available on other services- the key difference is access auditing.  This is also organized by investigator with a 1TB free allocation above which a cost per TB per month is charged.   Secure file is available via the path /fh/fast/secure/research/lastname_f on SciComp Linux systems, \\\\center.fhcrc.org\\fh\\secure\\research\\lastname_f or X:\\secure\\research on Windows hosts, and smb://center/fh/secure/research/lastname_f on OSX.   This storage platform can be used for storing PHI.  It must be noted, however, that many connected systems may not be appropriate for analysis of PHI data.  The first terabyte of data is provided by CIT with a charge of $50 per-terabyte for any amount above that.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/store_posix/",
        "teaser":null},{
        "title": "Data Storage in Temporary Storage (Scratch)",
        "excerpt":"The scratch file system is maintained by SciComp for temporary storage of research data during active analysis.  This is a large, high-performance storage system.  It is not designed to be as available or as robust as the home or fast file systems meant for long term data storage (these features were traded for lower cost and greater volume).   Data here is purged when unused for some amount of time (10, 30, and 90 days depending on the location).   Data on this platform is not backed up.  This storage is not appropriate for storing the primary or only copy of any data.   Similar to the Fast File system above, the scratch file system is available on the path /fh/scratch on SciComp supported Linux systems, \\\\center.fhcrc.org\\fh\\scratch on Windows, and smb://center.fhcrc.org/fh/scratch on Mac.   There is no charge to the investigator for data stored here.   Types of Scratch Storage Available   On Gizmo there are three forms of scratch space available: “node local job scratch”, “network job scratch” and “network persistent scratch”.  The “network job scratch”  and  “node local”  scratch directories and their contents exist only for the duration of the job- when the job exits, the directory and its contents are removed.  For more persistent scratch space, ​please see the persistent Scratch section.   Node Local Job Scratch   There are varying volumes of local storage depending on node configuration and utilization by other jobs.  If you require a large volume of local disk, request it with the “–tmp” argument:  sbatch -n 2 -t 1-0 --tmp=4096 # requests 4GB of disk space  Note that this only ensures that the disk is available when the job starts.  Other processes may fill up this scratch space, causing problems with your job. The location of this local scratch space is stored in the environment variable “TMPDIR” and “SCRATCH_LOCAL- use this environment variable if you need local storage on the node- do not use “/tmp” for storage of files or for scratch space. Node local job scratch spaces are only available on gizmo nodes, not on rhino.   Network Job Scratch   Network global scratch space is a scratch directory that is created on storage that is available to all nodes in your job’s allocation.  The directory is based on the job ID.  You should access the job scratch directory by using the environment variable “$SCRATCH” in your shell scripts, for example use $SCRATCH/myfile.csv to write to a file. Node local job scratch spaces are only available on gizmo nodes, not on rhino.   Persistent scratch   Sometimes you need to work with temporary data that is not part of a specific pipeline, for example if you are doing manual QA on data for a few days or even weeks. The persistent scratch file system is accessible via environment variables $DELETE10, $DELETE30 and $DELETE90 and the files in these folders will be removed after 10, 30 or 90 days of inactivity. The $DELETE30 folder is currently available on Gizmo and $DELETE10 folders are currently avialble on Beagle and Koshu. These folders can also be reached from other operating systems: In Windows you can select (x:\\scratch\\delete30 ) and on Mac you select smb://center.fhcrc.org/fh/scratch/delete30.   How long will my data stay in persistent scratch?   In $DELETE30 the data will stay on the file system for 30 days after you have stopped accessing it. 3 days before the data is deleted you (the owner of the files created) will receive an email with a final warning:       From: fs-cleaner.py-no-reply@fhcrc.org [mailto:fs-cleaner.py-no-reply@fhcrc.org]     Sent: Tuesday, August 23, 2016 11:32 PM     To: Doe, Jane &lt;jdoe@fredhutch.org&gt;     Subject: WARNING: In 3 days will delete files in /fh/scratch/delete30!      This is a notification message from fs-cleaner.py, Please review the following message:      Please see attached list of files!  The files listed in the attached text file will be deleted in 3 days when they will not have been touched for 30 days:  # of files: 247, total space: 807 GB You can prevent deletion of these files by using the command 'touch -a filename' on each file. This will reset the access time of the file to the current date.   As an alternative to the environment variable $DELETE30 you can also reach scratch through the file system at /fh/scratch/delete30, however the file system may be subject to change whereas the environment variable will be supported forever.   How can I use Scratch?   In jobs on Gizmo, environment variables can be used to write and then read temporary files, e.g. $SCRATCH/myfile.csv, $SCRATCH_LOC/myfile.csv or $DELETE30/lastname_f/myfile.csv ($DELETE10 and $DELETE90 are in preparation).  Similarly, jobs on Beagle can currently use $SCRATCH/myfile.csv and $DELETE10/lastname_f/myfile.csv.   The files under $SCRATCH_LOC and $SCRATCH are automatically deleted when your Gizmo or Beagle job ends. You can also reach Scratch storage space via Windows (via the X: drive) or Mac, e.g. smb://center.fhcrc.org/fh/scratch.   Note: lastname_f stands for the last name and the first initial of your PI. If you do not see the folder of your PI please ask Helpdesk to create it for you.   Examples   In your Bash Shell:     #! /bin/bash     echo -e $TMPDIR     echo -e \"Network Job Scratch:​ $SCRATCH\"     echo -e \"Node Local Job Scratch: $SCRATCH_LOCAL\"     echo -e \"Node Local Job Scratch: $TMPDIR\"     echo -e \"Persistent Scratch: $DELETE30\"      runprog.R &gt; $DELETE30/lastname_f/datafile.dat   In Python:       #! /usr/bin/env python3     import os      print(\"Network Job Scratch: %s\" % (os.environ['SCRATCH']))     print(\"Node Local Job Scratch: %s\" % (os.environ['SCRATCH_LOCAL']))     print(\"Node Local Job Scratch: %s\" % (os.environ['TMPDIR']))     print(\"Persistent Scratch: %s\" % (os.environ['DELETE30']))      MYSCRATCH = os.getenv('DELETE30', '.')     myfile = os.path.join(MYSCRATCH,'lastname_f','datafile.dat')     with open(myfile, 'w') as f:         f.write('line in file')   In R:      #! /usr/bin/env Rscript     MYSCRATCH &lt;- Sys.getenv('DELETE30')     MYSCRATCH[is.na(MYSCRATCH)] &lt;- '.'​     save('line in file', file=paste0(MYSCRATCH,'/lastname_f/datafile.dat'))   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinfcomputing/store_scratch/",
        "teaser":null},{
        "title": "Introduction to Docker",
        "excerpt":"At its core, Docker is a tool that lets you run code inside a totally fixed and reproducible computational environment. If you have used virtual machines, then you can think of a Docker container as being a lightweight, portable, intuitive system for building and running virtual machines. The major difference between a Docker container and virtual machine is that Docker doesn’t come with operating system, which makes it more lightweight and portable.  However, Docker containers do require the Docker engine to run them, which is a client-side application.   The folks at Docker are probably the best ones to answer the question, “What is Docker?”.   The reason that scientists use Docker is that it allows them to run computational tools using the exact same computational environment as another scientist, so that the same input data will (by definition) yield the exact same set of results. This is a level of reproducibility that is essentially impossible to achieve with any other tool apart from full virtual machines. It also neatly solves the problem of having to install dependencies in order to run a new tool in the right way.   Common Terms   There are a few basic terms that will make it easier to learn about Docker:   Image   A Docker image is basically a file that is built from a Dockerfile. The image functions like a recipe for the process you want to have the Docker container execute. It specifies all the details, such as tools, processes and parameters needed for the process to occur in the Docker container that could be created from it.   Container   A Docker container is a Docker image that is being executed or actively applied. Once you launch an image and start doing things (writing files, running commands, etc.) it becomes a container. The useful reason behind the distinction between images and containers is that once you are done using a container, you usually delete it. However, the image that you launched the container from remains untouched, and can be used to launch more containers in the future should the same exact process be needed again for different data sets.   Mount Point   By default, Docker containers do not have access to the file system for the computer that they are being run on. In order to pass files back and forth between the running Docker container and the host filesystem, you must make a “mount point.” The mount point is a folder that is shared between the Docker container and the host filesystem, allowing you to read and write data.   Pull / Push   Docker images can be stored either locally (on your laptop or desktop) or on a remote Docker server (such as Docker Hub). Moving the Docker image to and from a remote server is called “pushing” and “pulling.” The nice thing about this feature is that you can pull down an image that was used by another scientist and run all of the same commands that they did using the exact same computational environment.   Using Docker at Fred Hutch   The best page to get started at the Hutch is the Computing Resource Library about Docker.   Because Docker requires root access, it cannot be run on shared compute environments such as Rhino / Gizmo. Instead, developers at LBL have developed a utility called Singularity which can be used to run Docker images within an environment like Rhino / Gizmo at Fred Hutch. Additional information on using Singularity can be found at the sylabs.io site and more details about using Singularity at Fred Hutch on Gizmo.   Words of Wisdom: Singularity also has its own image format, however we do not recommend to use it and instead import Docker containers as they can be used more universally, e.g. in cloud computing.   Some cloud computing platforms are built explicitly on top of Docker. For example, AWS Batch works by running code within Docker containers on AWS EC2 instances. As AWS Batch gets rolled out at Fred Hutch, Docker images can be used to save and version the code that is executed with that system.   Other Docker Resources      The most commonly used server to share Docker images is called Docker Hub.   Another commonly used server to share Docker images is called Quay.   BioContainers is a free and open source project to collect a set of images that contain useful bioinformatics tools.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/compute_docker/",
        "teaser":null},{
        "title": "Git and GitHub for Computational Research",
        "excerpt":"Version control software allows anyone using a computer to track changes made to computer files over time, which allows both referencing and reverting back to earlier work. Such software is becoming increasingly common among scientists, academics, and other researchers to manage computational work. While originally developed to support code development, version control software is useful for not just computer code, but also data, documentation, or any other files associated with a project. Moreover, version control software streamlines collaborative editing of documents, allowing changes to be easily identified and attribution of changes to be assigned to individual contributors.   What is Git and GitHub?  Git is the name of a free, open source version control software commonly used among academics and software developers. GitHub is a web-based hosting service for projects managed with Git version control software, and includes a wide variety of additional tools which allows users to collaborate, publish files, release new versions of software, and even create webpages (in fact, this wiki is published through GitHub!). Some relevant terminology associated with Git and GitHub include:     repository: a project folder including all files and their history as tracked with Git. A repository can represent a variety of projects, including a piece of software, quarterly report, or research manuscript.            local: exists on your own computer       remote: exists somewhere besides your own computer and must be accessed over a network, such as on GitHub, the cloud, or a shared computer cluster           commit: a change to a file (or set of files) that represents a revision to previous versions   branch: a parallel version of the “master” repository that allows you to test changes without affecting the original (or live) version; changes in a branch can be merged back to the “master” when a particular task has been completed   fork: a copy of someone else’s repository in which you can make changes (and submit a request for the original repository owner to accept these changes)   Additional terms are defined in GitHub’s Glossary.   Using GitHub at Fred Hutch  Anyone may create an individual GitHub account and username for free. These individual accounts allow creation of repositories that are by default published publicly for anyone to view and use. This supports the goal of GitHub to facilitate open, reproducible science and collaboration. Collaborative options include forking repositories belonging to other people and allowing other people to collaborate on your own projects. Depending on the content and nature of your repositories, however, you may require additional flexibility in sharing and collaboration. Some of these options are described below.      Get a GitHub username here.   Join the Fred Hutch GitHub organization by sending your GitHub username to helpdesk and request to be added.   Public and Private Repositories  Repositories are publicly available by default. While this supports open science practices, there are also cases in which code or data in a repository needs to be kept private, such as when data or code are proprietary or need to be kept secure for other legal or ethical reasons. Although GitHub provides unlimited free public repositories to all users, and recently has added the option for private repositories for small teams. Repositories created by a user, however, should be primarily for private (non-Fred Hutch related) work. However, the Fred Hutch provides access to both public and private repositories for Fred Hutch related work via the Fred Hutch GitHub organization.   Fred Hutch GitHub Organization  GitHub Organizations are accounts shared by individuals that assist in coordinating large collaborative projects. Scientific Computing maintains a Fred Hutch GitHub Organization through which affiliated employees can create public or private repositories and share access with collaborators external to the Hutch. For individual users working on Fred Hutch related work, creating repositories as part of the insitution is preferred as monitoring services exist for the institution’s repositories that do not exist for your own.  These services can identify and help address potential issues such as accidental credential submission or other security related mistakes which can have an impact on you or the Fred Hutch.  For more information on our GitHub Organization, please see this wiki’s section on GitHub in Computing Credentials.   Using GitHub  There are many different ways of sharing projects collaboratively in a GitHub repository. This GitHub Guide for using GitHub is useful for understanding the basics.    There are many different ways to interact with Git and GitHub. Choosing which approach to use depends on whether you’re collaborating, if you are an experienced programmer, and how complex your work in Git will be; the advantages of each of these methods are described below. fredhutch.io’s recommended methods for beginners are described on their software page. In general, a good place to start is the GitHub Desktop client.   GitHub Desktop Client  Developers at Git have developed a desktop application with a graphical user interface that will allow you to accomplish most of the common tasks associated with Git version control and collaboration with GitHub. This is a great place for new Git users to learn about version control; you can download the desktop client for your system for free here. You can find some help getting started with GitHub Desktop here.   This desktop client will allow you to perform the basic Git workflow of making changes that are documented with Git version control. Additionally, the desktop client will allow you to clone repositories from web-based GitHub so that you can work on the project on your own computer.  After you have modified the file or files in your project, you can document and commit those changes back to the remote repository.    Command Line Git  Git was originally written, and continues to be developed, as a command-line tool accessible through Terminal (Mac) and Command Prompt or PowerShell (Windows). This version of Git is the only way you can run all available Git commands, and is appropriate if you are already familiar with the command line and consistently perform work using it. Additionally, you will likely need to use the command line version if you are connecting to remote computational resources like a cluster or cloud. You can learn more about command line Git here and download the command line software here.   GitHub on the Web  GitHub provides a web-based graphical interface for interacting with projects that are published through GitHub. While GitHub’s interface limits your ability to perform more advanced version control operations, there are additional tools available for collaborating, publishing wikis, and managing projects (see section below on “Additional options available in GitHub”).   Git Kraken (Another Desktop Client)  Git Kraken is a desktop client (Mac, PC, and Linux) available for free use to academics. It allows greater control and granularity than GitHub Desktop, especially when dealing with collaborative projects involving lots of branching and forking.  It is good for advanced beginners and beyond.   Git Integration with RStudio  If you are working with R statistical programming, the RStudio interface possesses robust integration with version control.   GitHub API  An API (Application Programming Interface) is a tool that allows you to communicate, or pass information between, two pieces of software. GitHub has an API that you can read more about here, which is useful for developing software that needs to reference information embedded in GitHub repositories.   GitHub and Information Security  With open, agile, collaborative research rapidly becoming the preferred model for scientific inquiry, many researchers at Fred Hutch and elsewhere are adopting Git and GitHub as an essential part of their analytic and publishing workflow. Combining code, data, documentation, and visualizations together in a GitHub repository along with an interactive notebook application provides an enhanced platform for reproducible research and dynamic, computational narratives. However, the same flexibility, ease-of-use, and openness that are major benefits in research collaboration can introduce serious risks when Git is used by design or unintentionally to maintain sensitive data such as protected health information (PHI) or data storage credentials. Git does not provide the same granular access controls, audit logging capabilities, and data encryption available with a database or filesystem certified for HIPAA compliance. A user with read access to a Git repository can readily clone the entire repository, including all current and previous versions of datasets and freely distribute the data through another repository or other communication channels with no record of such activity. While a GitHub repository containing sensitive data can be made “private,” simple misconfiguration or oversight can expose it to the public.   Considering both the large potential benefits and risks of using GitHub and Git, it is important for researchers at Fred Hutch to use these tools effectively while protecting the privacy of patients and Fred Hutch data resources. It is important to understand the types of sensitive information related to security which should never be put on GitHub.  Examples include individually identifiable health or financial information, server names, database or data storage credentials (i.e., usernames, passwords, tokens, access keys, etc), IP address, name of networks, etc.   Given the potential risk associated with even a private repository, however, you should carefully consider alternatives to maintaining any level of sensitive data in public or private GitHub repositories. The preferred method for ensuring privacy and security is keeping all potentially sensitive data out of GitHub repositories. Options for helping accomplish this while realizing the benefits of Git include restructuring workflows to preprocess and sanitize sensitive data in secure backend systems prior to it being posted to Git and storing secrets such as access keys and database credentials in the Vault secrets management system, accessing them when needed from Git using a secure API. For more guidance about how to structure your code or content you are putting in GitHub to ensure security, email scicomp.   Managing Credentials in Your Code  One issue to note when using GitHub to do version control in your code is that it can be very straightforward to inadvertently push content to GitHub that includes things such as API tokens, usernames and passwords, or even your AWS credentials themselves.  Please take care to structure your code in such a way that these “secrets” or anything you perceive to be private information are loaded from an external file or environment variables that themselves are not sent to GitHub.  See our Security page for more information on what is considered a secret or private information and see our Computing Credentials page for more specific guidance about how to structure your code to avoid a problem.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/compute_github/",
        "teaser":null},{
        "title": "Managing Code and Computing",
        "excerpt":"This section contains a variety of emerging topics that are becoming more critical for researchers to implement in their work.   Overview of Compute Resources on Campus  Researchers are more and more likely to need to analyze raw data sets using some sort of analysis process before they can be interpreted in the context of the scientific question. Raw data, whether from an array or sequencing for example, are not typically directly interpretable results, thus require some degree of processing. The nature of the processing depends on the data type, the platform with which the data were generated, and the biological question being asked of the data set. How this process is performed depends on the specific project, but support and resources for this process are available at the Fred Hutch for a variety of needs.  This section contains a basic overview of computing resource options available to Fred Hutch researchers, all the way from a single laptop to the Fred Hutch on-premise high performance computing cluster (gizmo) to various cloud-based options.   Github, Versioning and Sharing Code  While version control software has evolved over time, a new evolution that is happening more and more is the need for a wider group of researchers to actively use version control resources to manage their code and documentation of processes that are ongoing in their research. From the perspective of reproducibility, shareability and interoperability, the need for a sharing platform that integrates version control and collaboration is becoming more and more a critical part of a researcher’s toolkit. Thus, regardless of the degree to which code plays a direct role in a research project, more and more often at least a cursory understanding of what GitHub is and how it can be utilized in scientific research is important.   Notebooks and User Interfaces  While programming in R, Python or other languages is an important skill to learn, another layer of how to actually implement and manage processes you have developed in the code you’ve written can be useful. To manage the interface between code and environments, versioning and more integrated features (e.g. git or multi-language processes), various Notebook or User Interfaces exist. This document is an overview of common user interfaces commonly used to manage or interact with code.   Docker  Docker is an important tool for reproducible analysis in research, as it can be valuable in ensuring the computing environment an analysis is performed in can be completely reproduced.  At its core, Docker is a tool that lets you run code inside a totally fixed and reproducible computational environment. If you have used virtual machines, then you can think of a Docker container as being a lightweight, portable, intuitive system for building and running virtual machines. The major difference between a Docker container and virtual machine is that Docker doesn’t come with operating system, which makes it more lightweight and portable. However, Docker containers do require the Docker engine to run them, which is a client-side application. More about using Docker at Fred Hutch is on this Introduction to Docker page, and we have provided a detailed set of instructions in the Computing Resource Library.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/compute_overview/",
        "teaser":null},{
        "title": "Fred Hutch Computing Platforms for Bioinformatics",
        "excerpt":"Researchers are more and more likely to need to analyze raw data sets using some sort of analysis process before they can be interpreted in the context of the scientific question. Raw data, whether from an array or sequencing for example, are not typically directly interpretable results, thus require some degree of processing. The nature of the processing depends on the data type, the platform with which the data were generated, and the biological question being asked of the data set. How this process is performed depends on the specific project, but support and resources for this process are available at the Fred Hutch for a variety of needs.   The first step in doing this work is often as simple as asking “what computing resource do I need to use for this task?”  This section describes a range of computing resource options available to Fred Hutch researchers, all the way from a single laptop to the Fred Hutch on-premise high performance computing cluster (gizmo) to various cloud-based options, as well as how to get started using each platform.   Desktop computing platforms   While most bioinformatic analysis of genomics data, for example, will likely require more compute resources than are typically available in a desktop computer, some smaller analysis jobs or training can be done on a desktop or laptop.  For these smaller compute jobs, scientific computing languages R or Python may be useful (see the Bioinformatics page for more information about these computing languages). You can obtain assistance in installing a copy of R and RStudio on PCs through the Center IT desktop software resource. The installation of a commonly used Python and R package management system called Conda can be done on either PCs or Macs and more information about Anaconda and Miniconda (two versions of the Python oriented version of Conda package manager) can be found here.      Interactive Computing:  Easy Access, Moderate Capability  These systems are provided by the Fred Hutch to serve the community with the intention of:     being immediately accessible by everyone, regardless of skill set,   allowing for interactive development of code and/or approaches to analyses, and   being supported by compute resources larger than a typical desktop computer.   These systems are best used for situations such as training of new users in development of R or python code or curation of Galaxy-style workflows that need connectivity to our local filesystem in a way not accessible via the public instance of Galaxy. These systems also provide compute resources that are a step above a typical desktop computer, thus are good systems for slightly more compute-intensive jobs that still require interactivity.  Each is a managed resource with actively managed packages/modules/tools depending on the system, and thus work well for fairly standard platforms for interactive computing that does not require highly specialized software components.  Additionally, these systems may not be appropriate for analyses that require a higher level of data security, please contact FH username scicomp to inquire.                  Compute Resource       Access Interface       Resource Admin       Connection to FH Data Storage                       RStudio Server       web, FH credentials, no setup required       Center IT       Native to python, default access to /fh/fast                 Fred Hutch Galaxy Instance       web, platform-specific credentials       Matsen Group       Limited to staged data                 JupyterHub       web, FH credentials, no setup required       Center IT       Native to python, default access to /fh/fast           Interactive Computing: Command Line Interface (CLI), Moderate to High Capability  These systems are provided by the Fred Hutch to serve needs that rise above those that can be met using the above listed platforms.  Often reasons to move to these HPC resources include the need for version controlled, specialized package/module/tool configurations, more compute resources, or rapid access to large data sets in data storage locations not accessible with the required security for the data type by the above systems. In the table below, gizmo is actually the compute resource that can be accessed via multiple tools, which are also listed below.                  Compute Resource       Access Interface       Resource Admin       Connection to FH Data Storage                       Gizmo       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                                                           Rhino       CLI, FH credentials on campus/VPN off campus       Scientific Computing       Direct to all local storage types                 NoMachine       NX Client, FH credentials on campus/VPN off campus       Scientific Computing       Direct to all local storage types                 Python/Jupyter Notebooks       Via Rhino (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                 R/R Studio       Via Rhino (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types           Meet Rhino  Gizmo is actually not a stand alone system; instead, access to the resource is based on the Rhino platform supported by Center IT.  Rhino, or more specifically the Rhinos, are three locally managed HPC servers all accessed via the name rhino. Together, they function as a data and compute hub for a variety of data storage resources and high performance computing (HPC). The specific guidance for the use of each of the approaches to HPC access are slightly different, but will all require the user to learn how to access and interact with rhino.   Any user interacting with the following systems will be dependent on being proficient with the care and keeping of the Rhinos, and more information about on premise computing resources can be found in the Computing section of this site here.   More information on the topic of ssh configurations for access to rhino can be found here.   The NoMachine Cluster   NoMachine is a software suite that allows you to run a Linux desktop session remotely.  The session runs on the NoMachine server but is displayed on your desktop or laptop using the NoMachine client.  NoMachine (also abbreviated NX) is installed on CIT supported PC desktops and laptops.   NX has the particular advantage of maintaining your session even when you disconnect or lose connectivity.  All that is required is to restart the client and your session will be as you’d last left it.   There are three systems you can use for NX sessions: lynx, manx, and sphinx.  These are not computational systems but rather these hosts are used solely as launch-points for sessions on gizmo or rhino. Running computational tools on this system will get you a warning from SciComp.   Batch Computing and the Cloud   Batch computing allows you to queue up jobs and have them executed by the batch system, rather than you having to start an interactive session on a high-performance system.  Using the batch system allows you to queue up thousands of jobs- something impractical to impossible when using an interactive session.  There are benefits when you have a smaller volume of jobs as well- interactive jobs are dependent on the shell from which they are launched- if your laptop disconnected, the job would be terminated.   The batch system used at the Hutch is Slurm.  Slurm provides a set of commands for submitting and managing jobs on the gizmo and beagle clusters as well as providing information on the state (success or failure) and metrics (memory and compute usage) of completed jobs.  More information about on batch computing and cloud based computing resources can be found in the Computing section of this site here.                  Compute Resource       Access Interface       Resource Admin       Connection to FH Data Storage                       Gizmo       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                 Beagle       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Center IT       home, fast, economy, and S3 access. Beagle has its own scratch                 AWS Batch       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types          ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/compute_resoverview/",
        "teaser":null},{
        "title": "Data Archiving Best Practices",
        "excerpt":"As research datasets and digital research assets become larger and larger such as for imaging or genomics data, a strategy for managing data archiving becomes important.  Also, as data management plans and public repository deposition of data increasingly require researchers to steward larger and larger data sets in a more active manner.  Thus prioritizing active data management during the research process can be important in order to avoid massive loss of time and resources due to the need to slog through old data to comply with current funding or publication requirements.   Once large scale data sets are generated, annotating those datasets, using a working copy for analysis and simultaneously retaining the archived versions until they are no longer required is a unique challenge.  The file types involved, the required degrees of processing of the raw data and the overall size of the datasets can require a different approach to annotation and archiving than for smaller scale research data.  The need to address the best practices for research data of any size has been a focus of a national working group aiming to design and jointly endorse a concise and measurable set of principles, referred to as the FAIR Data Principles. The intent is for these principles to act as a guideline for those wishing to enhance the reusability of their data by themselves or others.  Specifically, the FAIR Principles are the ideas that data should be:  Findable, Accessible, Interoperable, and Reusable.   What to Archive:  Raw versus Processed Data  Depending on the type of large scale molecular data generated, the particular set of data files that are of most importance to archive may be different.  For the study of interest, consider questions such as:      Can we re-create this file using a known process we have documented using readily available resources?            If so, archive the original file(s) and the documentation of the process used to generate the file of interest, and keep the file(s) produced by the process in a working data location.       If not, archive the file(s) and any documentation that describes aspects of the data generation process that are critical to identify the types of downstream processes a researcher may want to choose to process/analyze the data.  Examples of this include genome, array, or sequencing panel version and release date or other process related variables such as the bioinformatic tools used in the workflow.           What is the relative size of this intermediate file versus the amount of compute resources needed to re-generate the intermediate file?            In some cases, such as whole genome sequencing data, the compute resource requirements needed to generate a list of genetic variants detected in the data may be relatively large compared to the amount of storage space required to store the intermediate vcf files that contain those variant lists.       In some cases the bioinformatic processing involved in generating a dataset is proprietary or under development, etc such that the researcher may not be easily able to re-create the intermediate files from the raw data and thus would create a significant hurdle to that researcher’s work.           Thus the data type, the unique conditions in which the data were generated, and the relative ease of access of the researcher to the processes involved in generating analyzable intermediate files from the raw data all have impacts on what data is reasonable to archive versus keep in work spaces.   Where/How to Archive it:  Data preservation  The term ‘preservation’ means ensuring something can still be seen or used over time. In the context of digital data, long-term preservation is the process of maintaining data over time so that they can still be found, understood, accessed, and used in the future. Genomic data producing technology continues to develop and thus data are at risk from one or more of the following problems:      file formats might not be compatible with future software, and therefore unreadable;   even if a file can still be opened with new software, it may be altered to a degree as to no longer be understandable or reliable for continued research;   storage media may have been degraded, scratched or broken, especially if they are portable, such as external hard drives, so information might be lost; and   the files or data will not be understood because there is no supporting documentation or metadata, or this has not been preserved correctly.   What can I do to ensure my data are usable in the future?  When creating, organizing and storing your data you can take a few initial steps to try and ensure your data remain useable and understandable for the future:      effectively document the critical metadata describing how and why the data were generated so that it can be understood and interpreted in the future.   keep more than one copy of data, and on a variety of storage media or spaces.   migrate data to new software versions, or use a format that can easily be imported to various software programs.   Ideally, this should be covered in a data management plan at the start of a project to protect the research work and capture any associated time and resources into your budget.   Data Storage Considerations for Archiving Data at Fred Hutch  One issue to consider is that it may be desired to have two copies of irreplaceable (either physically irreplaceable or infeasible to replace datasets), one in the local filesystem (location determined in part by the data size, frequency of access needed, type of access needed and cost considerations), and one in a managed cloud-based system (such as the PI Amazon Web Services s3 buckets managed by Fred Hutch).  Resources to obtain permissions, assistance in managing copying and data quality assessment, and information on accessing data in various locations can be obtained through Scientific Computing and Center IT Helpdesk below.  The Data Storage resource page discusses the various Fred Hutch managed storage locations available to researchers and the associated ease of access and costs associated (if applicable) with them.   Available Resources     Email helpdesk for questions or assistance with managing local filesystem based data, and email scicomp for questions or assistance withe managing cloud-based data and services.   Common Data Elements (NCI)   The Digital Curation Centre provides examples of disciplinary-specific metadata.   Project Open Data has a listed of standard data oriented Metadata Terms   BioSharing is a manually curated, searchable portal of three linked registries. These resources cover standards (terminologies, formats and models, and reporting guidelines), databases, and data policies in the life sciences, broadly encompassing the biological, environmental and biomedical sciences.   ISA Tools - metadata tracking tools for life sciences. The open source ISA metadata tracking tools help to manage an increasingly diverse set of life science, environmental and biomedical experiments that employing one or a combination of technologies. Built around the ‘Investigation’ (the project context), ‘Study’ (a unit of research) and ‘Assay’ (analytical measurement) general-purpose Tabular format, the ISA tools helps you to provide rich description of the experimental metadata (i.e. sample characteristics, technology and measurement types, sample-to-data relationships) so that the resulting data and discoveries are reproducible and reusable.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/dm_archive/",
        "teaser":null},{
        "title": "Data Ingestion and Public Datasets",
        "excerpt":"Large-scale research data can come from multiple sources, like one of the Fred Hutch Shared Resources, external vendors, external collaborators or public repositories. Regardless, if processing or analysis is required, your data will need to be accessible via a Fred Hutch managed data storage, compute resource, or workstation. Being selective about what degree public datasets need to be copied to a local storage space can lower your project costs.   Below, we outline some different approaches to ingest and store only the most relevant portions of the data.   Publicly Available Datasets  There are multiple sources and tiers of data available publicly.  In order to avoid, for example, a researcher having to pay to host large, raw datasets that are publicly available, there are approaches to accessing and documenting only the minimum required data.  Knowing how best to approach publicly available large scale data sets can both make a study far more productive in a shorter period of time and save resources from being unnecessarily spent on generating new data or storing copies of existing datasets when not required for the particular research they are being used for.  This section will have more to come on this topic.      cBioPortal is an excellent web-accessible resource to query various publicly available study data from projects such as TCGA or other more specific studies.   Sage Bionetwork’s Synapse platform hosts and organizes several open research projects that involve large scale molecular data sets and researchers can follow their documentation to download data through the web or python clients.   ImmuneSpace has publicly-accessible RNASeq, HAI, and flow-cytometry data from the Human Immunology Project Consortium   CAVD DataSpace has publicly-accessible data from HIV vaccine studies   Ingestion of Externally Generated Data  For data from non-Fred Hutch entities that you would like to transfer to Fred Hutch-managed storage for further analysis, there is a multi-step process that Scientific Computing can assist you with. Large scale biomedical datasets have higher risks of data corruption and transfer interruption. Also, “intermediate” data may also need to be generated during analyses by Fred Hutch investigators. Thus, it is important for you to work with Scientific Computing (email scicomp) to ensure that the external data are transferred completely, and you have secure and affordable storage on FredHutch systems.   Before ingesting any external data sets, please refer to the Overview of Data Storage at Fred Hutch page to begin to identify the final home of the data you are ingesting.  Choosing a location will depend on how large the data are, how often you need to access them, and what type of security concerns might apply to the dataset.  Once you have an idea where the data will need to live at Fred Hutch, you can begin the process of transferring them to Fred Hutch supported data storage systems.   The process is generally:     Fred Hutch user downloads data from an external source to the Scratch service using ftp, scp, ascp, etc or via collaborative data storage tools.   OR      Provide the sequencing center or data source the information needed to copy the data into one of the Fred Hutch Managed Amazon S3 transfer buckets.   THEN          Then validate the md5 checksums of the data against the checksum info (usually a text file containing md5sums) provided by the sequencing center or data source. (This checks for data corruption or incomplete transfers)            Transfer the validated data to the PI’s Economy storage.       Ideally, one wouldn’t transfer large amounts of data intended to be temporary into systems like Fast storage as that data will be included in the frequent snapshots for backup purposes in that storage system.  This means these data will have a large footprint in the overall system, requiring long term backup of what are actually temporary data (ironic in a way). A better way to ingest and store large data sets is to first ingest the data into Scratch storage and then move them into secure, stable object storage systems like Economy (either Local or Cloud).   FTP to Scratch  First, make a directory in Scratch for the data you want to ingest.  Then use wget to copy, in this case recursively, the data from the ftp url (e.g., ftp.broadinstitute.org/bundle/) using the username (user) and password (pass) required.   cd /fh/scratch/delete10/lastname_f/ mkdir ingestedDataDir cd ingestedDataDir wget --recursive ftp://user@ftp.broadinstitute.org/bundle/   While wget supports sending a password in the command line (either by using user:pass@ or via the --password option) this will make your credentials visible to anyone on the system.   Helpful wget Options   #Skipping Existing Files   If you need to repeat the ftp if not all files come down, use no-clobber when resending the command and this will skip any files that already exist.   --no-clobber   Managing Credentials   As mentioned above, there are different options for providing credentials (username and password) to the remote site.  The different options expose, to varying degrees, those credentials to others on the system.  Thus it is up to you to determine which of these methods should be used for storage of the credential.      IMPORTANT: Passwords are classified as level III data by the ISO.  Thus, none of these mechanisms can be used to store your HutchNet ID.    The first option is to send the credential on the command line, either in the URI or as an option to wget.  Providing the credential in the URI means adding the password after the username:   wget ftp://basil:VeryStrongPassword@site.example.org   You can also put the username and password in options for the wget command:   wget --user=basil --password=VeryStrongPassword ftp://site.example.org   This will expose the username and password to any other users on the system. For anonymous or commonly known credentials this may not be a concern but should not be used for any other credentials.   The next methods allow you to hide the credentials in a file- these files are unencrypted which makes storage of very sensitive credentials inadvisable (or in the case of your HutchNet ID, disallowed). It is essential that this file’s permissions allow read and write by you alone.   You can store these credentials in a file in your home directory.  On startup, wget will read the file $HOME/.wgetrc- this file can be used to store credentials:   user=basil password=VeryStrongPassword   Another way to store credentials is to create a file with the URIs you wish to download.  These URIs would contain the credentials using the user:pass syntax.  For example, if we create the file $HOME/data_download.txt with the line:   ftp://basil:VeryStrongPassword@site.example.org   You then provide this file to wget with the following command:   cat $HOME/data_download | wget -i -   As indicated above, setting this file to be readable by only you is essential:   chmod u=rw,go-rwx $HOME/.wgetrc chmod u=rw,go-rwx $HOME/data_download.txt   Sync to AWS S3 Economy Cloud Storage  Once you’ve confirmed that the intended data has been fully transferred to Scratch then transfer these data to the intended final location in Economy, in this case AWS S3 (Economy Cloud).   aws s3 sync /ingestedDataDir/ s3://your-pi-bucket-name/theseData/   Clean out Scratch  While the data in Scratch are not backed up and regularly deleted, the system does have a finite amount of space available to users.  Especially when your temporary data is 100s of GB or into the TB range, it’s preferred that after the data are confirmed to have been transferred to the final location that you remove your own data in Scratch.  This eliminates the possibility of someone else having to delete your data if another investigator needs a large amount of space.      Note:  Only ever use rm -rf when you are CERTAIN that a complete, permanent delete that cannot be retrieved is what you want.    rm -rf /fh/scratch/delete10/lastname_f/ingestedDataDir  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/dm_ingest/",
        "teaser":null},{
        "title": "Overview of Data Management for Researchers",
        "excerpt":"While the primary focus of research is the processes more clearly involved in the scientific endeavor, often the importance of knowing some degree of best practices for various data oriented issues is understated. The Generation section has an important section on data privacy and security when data relates to humans or human specimens.  The Computing section of this site contains a wide array of detailed information about the resources and processes provided by Center IT and Scientific Computing specifically.  However, here are aim to summarize the essential points of the best practices for the following topics here at the Fred Hutch.   Data Storage Best Practices  Depending on the type of data used by a research group, different combinations of data storage options may be appropriate.  Assessing the strengths, mode of access, and interactivity with computing resources of each of these types of data storage options alongside an assessment of the types of data a research group use and the type of interactions with those data is becoming more important for researchers. This assessment also is becoming more important to repeat over time as data storage and connectivity infrastructure changes and as the particular data types and interactions of research groups evolve.   Data Ingestion and Public Datasets Best Practices  Large scale datasets for a study can come from multiple sources, such as those generated by an outside sequencing center or the Fred Hutch Genomics Shared Resource or other large databases. Additionally, a study might rely on publicly available datasets in a repository with some degree of managed access. Regardless of the source, if bioinformatic or analytic processing of the data is required, data will need to be accessible via a Fred Hutch managed data storage, compute resource, or workstation. Below, we will outline some of the different approaches that exist to ingest and store only the most relevant portions of the data. Additionally being selective about what aspects and to what degree public datasets need to be copied to a local storage space can ensure that data storage and cost issues do not arise unnecessarily. Being knowledgeable about the various repositories and modes of access that exist for data of interest will ensure that research progress is neither hindered by data access challenges nor by incurred costs from unnecessary data storage.   Using Scratch  Scratch storage space is temporary storage that can be leveraged during workflows to store intermediate files or working copies of raw data while an analysis is being performed.  The benefits and limitations of using Scratch storage are discussed here as well as some guidance for how to structure your workflows to best employ Scratch.   Data Archiving Best Practices  Typically the need to actively archive data is fairly rare, but as research datasets and digital research assets become larger and larger such as for imaging or genomics data, a strategy for managing data archiving becomes important. Also, as data management plans and public repository deposition of data increasingly require researchers to steward larger and larger data sets in a more active manner. Thus prioritizing active data management during the research process can be important in order to avoid massive loss of time and resources due to the need to slog through old data to comply with current funding or publication requirements.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/dm_overview/",
        "teaser":null},{
        "title": "Using *Scratch* Storage",
        "excerpt":"Scratch storage serves as a temporary location for large data sets that ideally reside in an archive space like Economy storage, to be transferred to when compute processes are applied to them.  Data in Scratch are typically then deleted automatically after certain timeframes when they are no longer needed. Intermediate data that is generated can be saved in Scratch as well, and then the final data resulting from the compute process can be written to Fast storage for the researcher.  This allows large data to be archived in Economy storage, accessed by HPC when it is temporarily housed in Scratch and only the (typically smaller) resulting data are written to the more accessible, but more costly Fast storage.   Read more details about Scratch over in the Computing domain here.   Why is Scratch Different?  The scratch file system is maintained by SciComp for temporary storage of research data during active analysis.  This is a large, high-performance storage system.  It is not designed to be as available or as robust as the Home or Fast storage meant for long term data storage (these features were traded for lower cost and greater volume).  Data here is purged when unused for some amount of time (10, 30, and 90 days depending on the location).   Data on this platform is not backed up.  This storage is not appropriate for storing the primary or only copy of any data.  Most importantly, there is no charge to the investigator for data stored here.   Why use Scratch Storage for temporary data?   In bioinformatics workflows we are often using pipelines with many execution steps. Each of these steps can create a large amount of temporary data, for example extracting information from genomics data (BAM files). This data often needs to be kept for a short period of time to allow for quality assurance.   Informaticians often do not delete this data after this step because they are already off to the next task. Even worse, if temporary data is created in a standard file system such as Fast storage it will be picked up by the backup system and copied to the cloud the next night. If data is frequently created and deleted the backup data can grow to 5 or even 10 times the size of the primary data which is an enormous waste. To prevent this waste every informatician or Data Scientist working with large datasets should use Scratch storage as part of their routine.   For this purpose we have a scratch file systems attached to the Gizmo, Beagle and Koshu clusters. There is a different scratch file system mounted on each cluster.  Using a scratch resource has several advantages:      The scratch file system is free of charge   It is the most performant storage system connected to Rhino/Gizmo   You do not have to clean up your temporary data because the system does it for you   It reduces Fred Hutch storage expenses because the data is not backed up to the cloud.      Note: Even if you delete data from Fast storage one day after creation it will be kept in the backup system for a long time.    Using Scratch Temporary Storage with AWS S3 buckets and rhino   Using scratch storage for intermediate files during a process can be a useful way to keep storage costs low, while also using our local HPC resources (rhino and gizmo).  This adjusted way to work can be useful in reducing the tendency to store large and/or temporary data sets in Fast storage.  Due to the backup frequencies and speed configuration of fast, any files saved here even temporarily, will be retained in the backup for quite some time.   Using Economy Cloud storage (AWS S3) can be a more affordable way to store large data sets, and when combined with scratch can allow all of your current processes to work without a substantial shift in your approach.   Connect to rhino  See our pages on Access Methods and an overview of the Technologies Supported to learn more about what rhino is and how to use it.   Find Scratch space  Head over to some Scratch space for your PI.   Learn more about Scratch storage here.   cd /fh/scratch/delete10/lastname_f/ mkdir workingdir cd workingdir   Copy files from S3 to Scratch   Sync the prefix(es) of the file(s) in S3 containing the data you want, down to Scratch.     See the AWS CLI help for using S3 commands.    aws s3 sync s3://yourbucket/yourDirectory/ .   Perform your process  Process the data however you typically do.  In this example we are concatenating fastq’s from paired sequencing and then converting those into uBAMS for downstream analysis with GATK tools.   Here you can see we use the sbatch command to send the task to gizmo.  See more about HPC Job Management and commands here.  sbatch --wrap=\"zcat *_R1_*.fastq.gz &gt; forwardReads_R1.fastq\" sbatch --wrap=\"zcat *_R2_*.fastq.gz &gt; reverseReads_R2.fastq\"   Or, you can write a quick shell script to do a process.  Then you’re ready to convert from FASTQ to uBAM via a teeny shell script (miniScript.sh).  You can find more about how to use Slurm and see some curated examples of approaching job management in our slurm-examples GitHub repository.   #!/bin/bash  set -e  module load picard/2.18.1-Java-1.8.0_121  java -Xmx8G -jar picard.jar FastqToSam \\     FASTQ=forwardReads_R1.fastq \\ #first read file of pair     FASTQ2=reverseReads_R2.fastq \\ #second read file of pair     OUTPUT=forprocessing.bam \\     READ_GROUP_NAME=H0164.2 \\ #required; changed from default of A     SAMPLE_NAME=NA12878 \\ #required     LIBRARY_NAME=Solexa-272222 \\ #required     PLATFORM=illumina \\ #recommended   Execute the process.   chmod +x /fh/path/miniScript.sh sbatch /fh/path/miniScript.sh   Copy Results Back to S3  After you have performed your intended process and removed any intermediate files that are not the end result you are after and do not need to save, you can then sync the directory back up to S3.  By using sync, all of your output files and logs will go to S3 in a mirrored structure to that in scratch.  Another option is to directly sync only the files you want to save by name.   aws s3 sync workingdir/ s3://yourbucket/yourDirectory/   Be Polite: Clean up after yourself   Scratch, by definition isn’t backed up, and has a finite total size for all the users at the Hutch (the sum of all data stored in individual PI’s scratch space is fixed).  Thus, it is best when you finish work and have some output files you want to save, to sync them with S3 and then delete all of what is in scratch.  Yes, you don’t HAVE to do this as eventually your files will be deleted, but it is good practice to minimize your impact on others, AND make sure that you get in the habit of taking what you need right away.   rm -rf workingdir  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/dm_scratch/",
        "teaser":null},{
        "title": "Data Storage Guidance",
        "excerpt":"Most Fred Hutch based researchers using large scale biomedical data sets initially store their data using Fast storage alongside their smaller scale laboratory data.  This provides direct, rapid access to files both manually (e.g., via mapping a drive to a workstation) and to local computing resources (via our HPC cluster, see below).  However, a strategy for where, when and for how long to store what size data is important to create to ensure that data access by researcher or compute resource, transfer and archiving are not unnecessarily complicated thus hindering the research process.   Depending on the type of data used by a research group, different combinations of data storage options may be appropriate. Assessing the strengths, mode of access, and interactivity with computing resources of each of these types of data storage options alongside an assessment of the types of data a research group use and the type of interactions with those data is becoming more important for researchers.  This assessment also is becoming more important to repeat over time as data storage and connectivity infrastructure changes and as the particular data types and interactions of research groups evolve.   Overview of Storage Resources  More detailed documentation regarding data storage at Fred Hutch can be found in the Computing domain here which includes additional information about data storage in databases.                  Storage Resource       Costs (per TB/month)*       Backup Location/Duration       Best Use                       Home       Free to 100GB limit       7 days of Snapshots, Daily backups, Off Site copy       Data specific to a user, not shared to others, relatively small data sets                 Fast       $$$ beyond 5TB per PI       7 days of Snapshots, Daily backups, Off Site copy       Large data sets that need high performance access to computing resources, Unix file permissioning, but neither PHI nor temporary data (such as intermediate files)                 Secure        beyond 1TB per PI       7 days of Snapshots, Daily backups, Off Site copy       PHI containing datasets or those that require auditing, relatively small datasets                 Economy Local and Cloud       $ beyond 5TB per PI       Multi-datacenter replication, 60 day undelete with request to helpdesk       Best for archiving large data sets, or primary storage of large files.  Good for PHI or other data that requires encryption and auditing. Requires Desktop Client to access, see Object Storage page.                 Scratch       Free       Not applicable       Temporary files, such as those intermediate to a final result that only need to persist during a job.  Appropriate use  can significantly reduce data storage costs, see Scratch Storage and Using Scratch pages.              Note:  All admin contact for assistance with dat storage can be initiated by emailing helpdesk, but different system administrators are primary for different platforms.  Admin assistance can be requested for data transfers and validation, data import, as well as restoring data from backups for a given resource if available.       Contact Scientific Computing staff by emailing scicomp for help with:            identifying current storage space usage       assistance with identification of possible duplicated data sets       guidance with implementing active data management practices           Data Locations for Fred Hutch Shared Resource-Generated Data  For data made by Fred Hutch researchers via the Genomics Shared Resource, the default data deposition is currently managed directly by Genomics, and will result in the data being made available to the researchers via their Fast storage ( e.g., at path /fh/fast/lastname_f/SR/ngs for sequencing data).  Other types of datasets are transferred to researchers in either a dnaarray directory or via other forms of transfer specific to the platform type or data source.  This allows for rapid access to recently generated datasets.  However, once data generated via the Genomics Core becomes of primary interest to archive for occasional use, it is a good idea to visit the Data Storage section and consider implementing the active data management scheme described above with the assistance of Scientific Computing.   For example, depending on the intended use of the datasets, it may be desirable once data is generated by the Genomics Shared Resource to archive the data to the researcher’s Economy Local storage space, with a copy put in Scratch or Economy Cloud for immediate processing.  The specific organization of archive and working copies of data will depend on the particular project involved.      For consulting about how to handle large amounts of externally or internally generated data email scicomp.   For additional assistance regarding data generated via the Fred Hutch Genomics Shared Resource, email bioinformatics.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/dm_storage/",
        "teaser":null},{
        "title": "Overview of How-To and Training",
        "excerpt":"There are a variety of resources for training on various aspects of Bioinformatics and data skills available at the Fred Hutch, in the Seattle area as well as online resources.   Local Training and Resources     fredhutch.io offers frequent, on campus courses on a topics such as R, python or GitHub, and information about previous and upcoming courses is on their site.  In order to sign up for courses, go to MyHutch and click on Hutch Learning to see currently available courses and register for them.   Bioinformatics Shared Resources   Scientific Computing training materials (requires login), related to introduction to Unix and Gizmo, though additional, more recent, information can be see in the Computing section of this site.   FHBig (Fred Hutch Bioinformatics Interest Group), is a Fred Hutch based community group that has created the FHBig website and maintains a slack workspace to support communication between staff at the Hutch.   UW Biostatistics Summer Institutes offer yearly intensive courses over the summers on a wide variety of topics.   Online Learning     Biostar Handbook: A Beginner’s Guide to Bioinformatics   CalTech Learning from Data   The Carpentries, with lessons from Data Carpentry and Software Carpentry   Cognitive Class: Data science and cognitive computing courses   Coursera, including courses on Data Science and Bioinforamtics   CodeAcademy   DataCamp   DataQuest   edX, including courses in Data Analysis and Statistics and Bioinformatics   The Open Source Data Science Masters   ROSALIND interactive bioinformatics problem set   Udacity has courses on Data Science  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/howto_training/",
        "teaser":null},{
        "title": "Bioinformatics Overview",
        "excerpt":"Bioinformatics refers to the computational analysis of raw biological data, typically those generated within the context of a scientific experiment or research project. One of the most rapidly expanding and heavily used application of bioinformatics is genomic sequence analysis, in which some form of high-throughput, large scale data collection instrument (such as a genome sequencer or a DNA microarray) is used to collect a large amount of information about such biological entities as genes or transcripts, etc.  Large scale data generation approaches like this can be referred to as molecular profiling, and can be applied to a variety of biological and biomedical research contexts from model organism studies (e.g., yeast strains) to human specimens in the context of a clinical assay.   Best Practices for Research Data Management  While the primary focus of research is the processes more clearly involved in the scientific endeavor, often the importance of knowing some degree of best practices for various data oriented issues is understated. Here we aim to summarize the essential points of the best practices for the following topics here at the Fred Hutch.      Data Storage   Data Ingestion and Public Datasets   Using Scratch   Data Archiving   Programming  While there are many types of programming languages various software used in scientific research are written in, there are a handful of specific languages that are commonly used in the process of doing a wide range of research tasks. We will introduce the most common here.      R, RStudio   Python   Unix/Bash and Other Common Languages   Managing Code and Computing  Researchers are more and more likely to need to analyze raw data sets using some sort of analysis process before they can be interpreted in the context of the scientific question. Doing this work requires knowledge of what compute resources are available, how one might interact with the code they write, attention to the need for version control and workflow creation.     Fred Hutch Compute Resources   GitHub, Versioning and Sharing   Notebooks and UIs   Docker   Training and References  These sections include a resource list of good opportunities for training from on-campus options to web-based learning options.  Additionally, the Research Topic Articles are a work in progress, where we can host short demo’s or articles about common questions or approaches as written by Fred Hutch based researchers and staff.     Training Opportunities   Research Topic Articles  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/inf_index/",
        "teaser":null},{
        "title": "R and RStudio",
        "excerpt":"R is a common statistical and computing language used in a variety of biomedical data analyses, visualizations and computing settings.  R itself can be downloaded to install it on your local computer from the Comprehensive R Archive Network project, or CRAN, or via the FH Center IT’s Self Service Tools (on Macs or on PCs).  Call the IT Helpdesk if you do not have permissions to install or update R on your local computer.   RStudio  The RStudio IDE is a free and open source, popular interface to R, which supports a number of added features including a GUI that can be customized to view aspects of your work of most importance (such as plots, files, environment variables, scripts, workspaces, etc). RStudio can be downloaded here and requires an installation of R itself first.  Keep in mind that updates to R/RStudio subsequently will require a two step process, one to update R itself, and the other to update the interface to R, RStudio.   RStudio has a few particularly useful features:     Support for R Markdowns/Notebooks   Integration with git or SVN   Concurrent views of code, plots, files, and environment variables with customized panels.   Direct deployment of Shiny apps via Shinyapps.io   R package management and direct installation capabilities   R Packages and Extensions  There are a number of available resources built on R that are free and open source that can greatly expand the utility of R and RStudio for research purposes.  There are currently three main sources of R packages that are of interest to a majority of the research community.   Bioconductor  Bioconductor is a public repository of R bioinformatics packages. Bioconductor packages are curated for intercompatibility and grouped into workflows (eg. CyTOF, ChIP-seq, eQTL, etc…). New Bioinformatic tools often result in a submission of the corresponding packages to Bioconductor.  These are reliable, well vetted packages that undergo a rigorous process for submission.   CRAN  CRAN, (Comprehensive R Archive Network) is a public repository of numerous R packages along with R itself. Numerous packages are available, though packages are not vetted as heavily as Bioconductor and generally are required to successfully be built, but may not always perform reliably, or be fully documented.   GitHub  GitHub hosts many open source R packages. As they are not vetted or peer-reviewed, these packages can be more experimental than those on CRAN or Bioconductor and thus you will want to proceed with caution. Some basic instructions on how to install packages into your local R/RStudio are included in this vignette.   Local (Desktop) Use  When using R/RStudio locally, you have the option to install a number of different packages from multiple sources.  Depending on the source of the package, you may approach downloading and installing them slightly differently but you manage the various packages installed, the versions of them as well as the version of R you are using them with.   Remote (Rhino and Gizmo) Use  If computing resources beyond what is available via your desktop are required, you may consider running R scripts or RStudio from the rhinos or gizmo.  When using R/RStudio on shared computing resources, different options for builds and modules are available that you can take advantage of.  SciComp makes pre-built modules available for researcher use in order to facilitate more reproducible and reliable use of software on the local cluster.   Current R Builds on Rhino/Gizmo  SciComp maintains a current list of the various builds of R available on Gizmo for use by researchers on the EasyBuild site.  Each build has different packages installed and versions of R itself, thus identifying if an existing R build matches your needs is a first step to using R on Gizmo.  If you do not see a build that meets your needs, then contact scicomp with the specific needs of your project.   Rhino  Depending on what OS is on your local computer, the steps are:     If your local computer runs Windows, you should connect to the Rhino’s with NoMachine, then launch RStudio from within NoMachine.   If your local computer is a Mac, you should install XQuartz and connect to Rhino, then launch RStudio from the terminal   If your local computer runs Linux, you simply need to connect to Rhino with X11 forwarding using the -X flag as detailed above for Mac computers.   Gizmo  From Rhino, execute the grabnode command and a node will be allocated on Gizmo after you selected the CPUs and number of days you need the node.   rstudio.fredhutch.org  Lastly, a Hutch supported RStudio server can be accessed at rstudio.fhcrc.org from computers connected to the Hutch network. For more information about using it, please contact scicomp.   The Tidyverse  The Tidyverse is a group of R packages that coordinate together and are commonly used for manipulating and  visualizing data in data science applications.  There are a number of useful packages for research based users that are part of the Tidyverse, and it’s worth the time to learn about them and see how one might employ them to clean, analyze and convey data and results.  DataCamp has an online Introduction to the Tidyverse that can be useful when first evaluating whether these packages might be useful.   Shiny  Shiny is an R package bundled with RStudio that enables the creation of interactive applications powered by R code. These apps can be viewed on any computer running RStudio, or they can be hosted on a server. Scicomp provides instructions for hosting Shiny apps here   Local resources     Seattle useR group   Cascadia RConf, a local yearly conference      NOTE: This article is a work in progress. If you have suggestions or would like to contribute email sciwiki.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/prog_R/",
        "teaser":null},{
        "title": "Overview of Common Programming Languages in Bioinformatics",
        "excerpt":"Software used in scientific research are written in a wide range of programming languages, we highlight the most commonly used ones below.   While programming in R, Python or other languages you may need access to various User Interfaces or “Notebooks”.  This document provides an overview of these tools.   Compute Resources Needed  For R and Python, you can run the code you have written locally on your computer, or remotely on the Linux clusters. For running remotely, you can either run on a cluster node shared with other users, or reserve a node for your exclusive use for a limited time.   Running on your computer     Pro: immediate access, familiar environment   Con: limited CPU, memory and disk resources for large tasks (eg. aligning RNASeq reads, variant calling, etc.)   Running remotely on shared cluster node (Rhino)     Pro: higher CPU, memory and disk resources   Con: need to transfer files to Hutch servers, requires Internet connection, can be temporarily slow if Rhino has many concurrent users   Running remotely on reserved cluster node (Gizmo)     Pro: higher CPU, memory and disk resources, and you’re the exclusive user   Con: need to transfer files to Hutch servers, requires Internet connection, if you request a very powerful computer, you may have to wait a while for one to become available   When using the Fred Hutch computing clusters, users should access these programming languages via the environmental modules (eg. ml R rather than simply R in Rhino). Doing this will ensure reproducibility of your code and that you’re using the latest software available. More information about environmental modules are available here.   R and RStudio  R is a programming language and also a software development environment. It is widely used among statisticians and has strong capabilities for statistical modeling and data analysis. While R’s core functions are fairly small, there is a robust community of user-contributed R ‘packages’ (eg. see “Bioconductor” below). You can download R for your computer, install it from Center IT’s Self Service (on Macs or on PCs), or run R on SciComp’s computing clusters (see “Current R Builds on Gizmo” and in the Scientific Computing domain of this site).   RStudio is a graphical frontend to R that also improves upon a basic R installation, providing syntax-highlighting and code-completion, static or dynamic reports (via RMarkdown documents), and easing the creation of R packages, among other functionalities. It is considered an IDE which functions much like a wrapper around R itself, to create a graphic user interface, and easy access to various tools and functions that enhance the user’s experience of using R.   For more information about RStudio, see the Notebooks and UIs page.   Python   Python is another language used extensively within the bioinformatic community. A very high-level comparison of Python to other commonly used languages is that it’s generally on the easier side for being able to learn and understand, but it doesn’t give you as much detailed control over the details of computation compared to C. In other words, it’s easier to use, but not quite as performant. One of the nice things about Python in recent years has been that there is a large community of software developers contributing highly efficient code as installable modules, which makes the entire codebase more valuable for your average user.   Unix/Bash   Unix is the foundation for both Linux and macOS, and is the operating system that is most commonly used for developing and executing bioinformatic software tools. In order to navigate a Unix-based operating system and execute commands, it is extremely useful to use the command line interface, which is generally referred to as BASH (Bourne-Again SHell).   Other languages  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/prog_overview/",
        "teaser":null},{
        "title": "Python and the Condas",
        "excerpt":"Educational Resources  There are some publicly accessible resources available to assist in getting started learning and using Python for scientific research computing.  Note the How-To’s and Training section in the sidebar for additional details and in-person training opportunities.     Getting started with Python   Python scientific computing with Scipy   Python Data Analysis Library   Learn Python the Hard Way   Biopython tutorial   Jupyter Notebooks   Jupyter Notebooks are web interfaces to an interpreter shell such as Python. They are most used by data scientists who would like to experiment with their code and easily generate charts and graphs. At Fred Hutch there are at least 4 ways how you can use Jupyter Notebooks, including the latest incarnation called ‘Jupyter Lab’.  You can find more information about Jupyter and related technologies here at the Project Jupyter site.   Jupyter Notebook on your computer   Install the software on your own computer install Jupyter and run locally.   Jupyter on Rhino and Gizmo  Current Python Builds on Rhino/Gizmo  SciComp maintains a current list of the various builds of Python available on gizmo for use by researchers.  Each build has different modules installed and versions of Python itself, thus identifying if an existing Python build matches your needs is a first step to using Python on gizmo.  If you do not see a build that meets your needs here, then contact scicomp with the specific needs of your project.   Rhino  Just load a Python distribution maintained by SciComp and run Jupyter lab:       petersen@rhino1:~$ ml Python/3.6.7-foss-2016b-fh2     petersen@rhino1:~$ jupyter lab --ip=$(hostname) --port=$(fhfreeport) --no-browser      ... or simply use the 'jupyterlab' wrapper script:     petersen@rhino1:~$ jupyterlab   Then connect to the URL, copying the link given by the previous command, which looks as follows:         Copy/paste this URL into your browser when you connect for the first time,     to login with a token:         http://rhino1:11112/?token=0eee692be6c81c1061db  Gizmo   From Rhino execute the grabjupyter command and a node will be allocated on Gizmo after you selected the CPUs and number of days you need the node.   Jupyter on Jupyterhub   SciComp maintains an installation of Jupyterhub. Login with your Hutch Net Id.  (Jupyterhub does not have the latest Python packages)   Also only the first method allows you to install your own python packages as administrator of your machine.  The other 3 methods require you to either request a package from Scientific Computing or install the package in your home directory with the –user option (e.g. pip3 install --upgrade --user mypkg) or to create a virtual Python environment, for example:       petersen@rhino1:~$ ml Python/3.6.7-foss-2016b-fh2     petersen@rhino1:~$ python3 -m venv ~/mypython     petersen@rhino1:~$ source ~/mypython/bin/activate     (mypython) petersen@rhino1:~$ jupyter lab     (mypython) petersen@rhino1:~$ deactivate     petersen@rhino1:~$  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/prog_python/",
        "teaser":null},{
        "title": "Other Programming Languages Common in Bioinformatics",
        "excerpt":"Shell scripting  During the course of your work you may need to do a simple task on a large number of files, like renaming all the files from a sequencing run, or raising the contrast on microscopy images. Performing these tasks on individual files by hand is time-consuming and prone to errors. Unix, Mac OSX and Windows all have simple shell scripting programming languages built-in for these small repetitive tasks require simple logic   The benefits of shell scripting are:     not needing to install additional software on your computer and,   ease of use. Most Unix-based systems (eg. Ubuntu) come with the Bourn Again SHell (“Bash”), which are also standard on Mac OSX systems. Windows have the Command Prompt and PowerShell. You can enter shell scripting commands directly via a command line interface or save these commands in a shell script to be run immediately non-interactively.   Shell scripting resources     A gentle introduction to command line interface and related concepts are here.   Some basic Bash commands can be found here.   Slightly more advanced Bash scripting are found here.   Common Bash pitfalls goes into more subtle, advanced usage   Overview of Windows PowerShell and an example comparing Command Prompt and PowerShell.   Julia  The Julia language aims to combine the accessible syntax of R or Python with the speed of C/C++ programs. While not currently as functional as R or Python for bioinformatic tasks, there is a growing collection of resources for Julia for bioinformatics.   Go  Go, or GoLang also has some support for data science.   Perl  Perl is a computational language often found in bioinformatic analyses. The language was originally developed in 1987. perl.org has numerous tutorials and modules for learning the language.      NOTE: This article is a work in progress. If you have suggestions or would like to contribute email sciwiki.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/bioinformatics/prog_unixbash/",
        "teaser":null},{
        "title": "using Docker at Fred Hutch",
        "excerpt":"Accessing Docker  You can either install docker on your own machine or you install on the SciComp test server farm   On Your Local Computer   It’s recommended (but not required) that you install Docker on your workstation and develop your image on your own machine until it’s ready to be deployed.      Windows   Mac   Ubuntu Linux   On the SciComp Test Environment   You can deploy your own docker machine on the Proxmox virtual test environment in ca 60 sec using the prox command.  This environment uses multiple large memory machines (16 cores, 384GB memory each) which are re-purposed previous generation Rhino class machines.   Login to Rhino via ssh and follow these quick steps to run your own docker host.  Pick a new host name (in this case we pick sausage) and  make sure it does not already exist, ping should respond “unknown host”       petersen@rhino:~$ ping sausage     ping: unknown host sausage   Now bootstrap a new machine sausage using the “prox new” command  with the –docker option and your HutchNet password.       petersen@rhino3:~$ prox new --docker --no-bootstrap sausage     Password for 'petersen':      installing on node \"proxa3\"     creating host sausage with ID 121 in pool SciComp         ...UPID:proxa3:00001F6C:00F2DBDE:57EE629A:vzcreate:121:petersen@FHCRC.ORG:     ​Starting host 121 ..         ...UPID:proxa3:00001FB3:00F2E185:57EE62A8:vzstart:121:petersen@FHCRC.ORG:     Machine 121 : running, cpu: 0%       waiting for machine sausage to come up .. hit ctrl+c to stop ping     PING sausage.fhcrc.org (140.107.117.249) 56(84) bytes of data.     64 bytes from sausage.fhcrc.org (140.107.117.249): icmp_seq=1 ttl=63 time=3.17 ms   The prox command checks that the machine is up, now you can login as root with the prox ssh command or by using your Hutch net password.       petersen@rhino:~$ prox ssh root@sausage     Welcome to Ubuntu 16.04.1 LTS (GNU/Linux 4.4.19-1-pve x86_64)   First, change the network configuration and restart Docker. You only need to do this once, but if you don’t do it you will have network problems:   echo '{ \"bip\": \"10.99.99.1/24\" }'  &gt; /etc/docker/daemon.json service docker restart   Verify that docker works as expected by running the hello world container.       root@sausage:~# docker run hello-world     Unable to find image 'hello-world:latest' locally     latest: Pulling from library/hello-world     c04b14da8d14: Pull complete      Digest: sha256:0256e8a36e2070f7bf2d0b0763dbabdd67798512411de4cdcf9431a1feb60fd9     Status: Downloaded newer image for hello-world:latest      Hello from Docker!     This message shows that your installation appears to be working correctly.   Now let’s assume you would like to install multiple machines to build a small cluster. Each machine requires some ad hoc configuration and we want a little more disk and memory. warning: Each machine requires its own IP address and there are a limited number of IP addresses in the subnet of the development cluster. Try not to deploy not more than 10 machines at a time.       petersen@rhino3:~$ prox new --docker --mem 1G --disk 8 --no-bootstrap  sausage1 sausage2 sausage3     Password for 'petersen':      creating host sausage1 with ID 116 in pool SciComp         ...UPID:proxa3:000039A6:0111B96E:57EEB19E:vzcreate:116:petersen@FHCRC.ORG:     creating host sausage2 with ID 118 in pool SciComp         ...UPID:proxa3:000039B6:0111B980:57EEB19E:vzcreate:118:petersen@FHCRC.ORG:     creating host sausage3 with ID 121 in pool SciComp         ...UPID:proxa3:000039C4:0111B991:57EEB19E:vzcreate:121:petersen@FHCRC.ORG:     Starting host 116 ..     starting host 116, re-try 0         ...UPID:proxa3:00003A04:0111BCB7:57EEB1A6:vzstart:116:petersen@FHCRC.ORG:     Machine 116 : running, cpu: 0%      Starting host 118 ..         ...UPID:proxa3:00003AF7:0111BD3C:57EEB1A8:vzstart:118:petersen@FHCRC.ORG:     Machine 118 : running, cpu: 0%      Starting host 121 ..         ...UPID:proxa3:00003BE2:0111BDC2:57EEB1A9:vzstart:121:petersen@FHCRC.ORG:     Machine 121 : running, cpu: -1%    After you are done with your work you can stop and then destroy these machines.       petersen@rhino3:~$ prox stop sausage1 sausage2 sausage3     Password for 'petersen':      UPID:proxa2:000060FE:01121EA2:57EEB2A1:vzstop:116:petersen@FHCRC.ORG:     UPID:proxa3:00006110:01121EB3:57EEB2A1:vzstop:118:petersen@FHCRC.ORG:     UPID:proxa4:00006127:01121EC6:57EEB2A1:vzstop:121:petersen@FHCRC.ORG:      petersen@rhino:~$ prox destroy sausage1 sausage2 sausage3     Password for 'petersen':      UPID:proxa2:000061C7:01122C18:57EEB2C4:vzdestroy:116:petersen@FHCRC.ORG:     UPID:proxa3:000061CB:01122C2A:57EEB2C4:vzdestroy:118:petersen@FHCRC.ORG:     UPID:proxa4:000061CF:01122C3B:57EEB2C4:vzdestroy:121:petersen@FHCRC.ORG:​​      Important: If you stop a machine with the ‘prox stop’ command the host name will not be purged from DHCP/DNS. If you want to re-use that hostname you need to wait for a couple of days.  However, if you login to a machine and shut it down with  the ‘shutdown -h now’ command the hostname will be released immediately and you  can re-use right away after destroying the machine.  This trick does currently only work with the default machine which is based on Ubuntu 16.04. It does not work (as of December 2018) if you use Ubuntu 18.04, such as:        petersen@rhino3:~$ prox new --docker --ubuntu 18.04 --no-bootstrap sausage   Using pre-made Docker images with application stacks   You may not need to create your own Docker image, but that depends on what you  want to do. If you are using software that is readily available, there is probably already a Docker image containing that software.  Look around on Docker Hub to see if there’s already a Docker image available.   SciComp is also developing Docker images that contain much of the software you are used to finding in /app on the rhino machines and gizmo/beagle clusters (here’s the R image).   If you’ve found an existing Docker image that meets your needs, you don’t need to read the rest of this section.   Create your own Docker image and put your software inside   Create GitHub Account   You really need a GitHub account to properly build docker containers. Please see our Shiny app example on how to create your own docker image   Deploy your Docker image to production   Your own machine or the SciComp test farm are likely poor choices for running production level applications in containers. For production deployments you will need a GitHub account. (see above)   Using docker hub   Dockerhub is a good choice for fully public open source projects   Push your Dockerfile to a GitHub repository  More to come.  Create an Automated Build in Docker Hub  More to come.  Create a Job Definition   Using the Fred Hutch Container environment   SciComp supports 3 Rancher environments for hosting docker  based web applications:                  Env Name       Purpose                       SciComp       Internal only SciComp managed applications                 SciComp-ext       Publicly accessible SciComp managed applications                 HDC_Local       HDC application environment (currently hosting HICORIQ)           Please email SciComp to request assistance and discuss which environment is best for your needs.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/Docker/",
        "teaser":null},{
        "title": "How to migrate large data to Economy Local",
        "excerpt":"Learn how archive data with little effort using the Gimzo cluster as a Helper   Disclaimer   Disclaimer for users who decide to delete data or put it on USB drives, etc: Before you decide to delete data in order to reduce storage costs please be aware that there may be data retention requirements imposed by the research funding sponsor, other contractual obligations, NIH data sharing and retention requirements (if applicable),  and Center policies. Please contact the Office of the General Counsel if you are in doubt about the retention requirements for your data.   Overview   When we migrate large amounts of data we have different use cases. First (A) we assume that we have a folder mydata that contains medium to large size data. (Many files that are &gt; 1MB up to multiple  GB, scientific datasets, bam files, etc) After that we look for the best way (B) to store smaller files (e.g. source code and small text files, home directories of previous staff).  For further instructions please see How to use Economy File Storage  . PLEASE NOTE: copying thousands of small files to economy file will be slow and frustrating to manage. Please use method B) if you have many small files.   Still excited to do this yourself ? SciComp offers to move the data for you. All you have to do is send the Fast file source folder and the Economy File target folder and the PI account to use to SciComp @ and the data will be moved for you.   How do I find out if I have many small files ?  Please see File Data Management Tips and Tricks   How can I find my big files?  Please check the Scientific Storage Reporter Howto   Preparation   create and switch to a folder where you keep the logs for your archiving activity. We recommend to use a folder Archivelogs in your home directory or in the fast file folder of the PI, eg. /fh/fast/lastname_f/archive/Archivelogs: use sw2account to switch to the economy file account of your PI. use ‘swc ls’ to make sure that the destination does not already exist. data at the destination will be overwritten without asking.       &gt; mkdir ~/Archivelogs     &gt; cd ~/Archivelogs     &gt; sw2account lastname_f     &gt; swc ls /archive/my/target   make sure that ‘Archivelogs’ is your current directory during the following archiving activities   A) Migrating medium to large size files   if you have not done yet make sure you use the right Economy file account with sw2account submit a swc upload command to gizmo. Please check if the target folder has a trailing slash or not as this influences where you data is copied to. Also make sure that you request enough wall clock time on gizmo (in the example below 7 days) check the progress of your copy job using the tail command on the –output file use the swc compare command to make sure that source and target folder have the same size. If the size differs you may run another swc upload job ask your colleaques if the data made it over correctly before you delete the source or ask IT to delete it for you.       $ sbatch -J \"copy\" --mail-type=ALL -t 7-0 --output=myfolder-archive.txt --wrap=\"swc upload /myfolder /archive\"     $ tail -f myfolder-archive.txt     $ swc compare myfolder /archive      To make things even easier there is a wrapper script : fhUpload,  which will use one gizmof node per subfolder to move data:​​​​        $ fhUpload /fh/fast/....../human/hg19/2013 /human/hg19/2013     submitted batch job 27981126     Upload job submitted to Gizmo. If you would like to monitor progress please use one of these commands:     swc size \"/human/hg19/2013\"     tail -f Upload_.....slurm.log.txt   To make things a little easier if you have a folder that has multiple subfolders with large files in them you can use the fhUploadSubfolders2Economy command, which will use one gizmof node per subfolder to move data:​​​​       $ fhUploadSubfolders2Economy human/hg19/2013 /human/hg19/2013      Will now submit a cluster job for each of these commands to Gizmo:      swc upload human/hg19/2013/TCGA.HNSC /human/hg19/2013/TCGA.HNSC     swc upload human/hg19/2013/TCGA.THCA /human/hg19/2013/TCGA.THCA     swc upload human/hg19/2013/TCGA.READ /human/hg19/2013/TCGA.READ     Do you want to continue? [y/N] n   if you do not want to use a separate cluster node for each sub directory you can also use the simpler fhUpload which submits an upload command for a single directory to a single Gizmo node and works very simple: $ fhUpload /myfolder /archive/folder   B) Migrating many small size files   After you have gone through the ‘Preparation’ steps above you just need to use the fhArchive command to archive many small files to Economy.       $ fhArchive      usage: fhArchive /directory/to/archive /swift/folder /scratch/folder         -&gt;  compress and archive a directory to economy file   You do need to pass a scratch folder as final argument to be able to compare the size of the source with the archive. As the archive is compressed it will have a different size. fhArchive uncompresses the archived data to scratch and then compares the sizes of the original data with the scratch folder. For example if you would like to archive a sub folder ProjectA in fast file for PI Jane Doe it would look like this:       petersen@rhino1:/home…en/Archivelogs$ fhArchive /fh/fast/doe_j/ProjectA /archive/ProjectA /fh/scratch/delete30/doe_j/archive_verify     Submitted batch job 45975375     Archive job submitted to Gizmo. If you would like to monitor progress please use one of these commands:     swc size \"/archive/ProjectA\"     tail -f Archive__fh_fast_doe_j_ProjectA.slurm.log.txt   If the sizes of source and destination are not identical, use this command to search for errors:       grep -B5 -A5 -i error Archive__fh_fast_doe_j_ProjectA.slurm.log.txt   It is strongly recommended to not archive more than 1 TB per fhArchive command because each fhArchive command uses only a single compute node with &lt;= 100 MB/s throughput and 4 cores. It will be faster if you execute fhArchive multiple times, one for each sub directory e.g ProjectA/subproject1, ProjectA/subproject2, etc )   Checking results   when the job finishes you should get an email from slurm with a subject line similar to this one: subject: SLURM Job_id=45975375 Name=Arc__fh_fast_doe_j_ProjectA Ended, Run time 00:29:56, COMPLETED, ExitCode 0 then check the output file with the tail command, you want to see something like this:       &gt; tail -f Archive__fh_fast_doe_j_ProjectA.slurm.log.txt          checking posix folder /fh/fast/doe_j/ProjectA ...         81,024,500,435 bytes (75.460 GB) in /fh/fast/doe_j/ProjectA         checking 2nd posix folder /fh/scratch/delete30/doe_j/archive_verify ...         81,024,500,435 bytes (75.460 GB) in /fh/scratch/delete30/doe_j/archive_verify     OK! The size of /fh/fast/doe_j/ProjectA and /fh/scratch/delete30/doe_j/archive_verify is identical!     The data was uploaded and then uncompressed to scratch.     The sizes of the source directory and the scratch directory should be identical.      now you can delete everything under archive_verify. Note that fhArchive does not delete the orginal source data (e.g. under /fh/fast). You need to delete this data after you have confirmed that your data was archived successfully.     &gt; rm -rf /fh/scratch/delete30/doe_j/archive_verify     Getting your data back is easy. Note that you do not need to restore the entire archive, you can also just restore a subdirectory. This is possible because \"swc archive\" creates one tar.gz archive per directory level.     swc unarch /archive/ProjectA/subproject1 /fh/scratch/delete30/doe_j/ProjectA/subproject1  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/Economy-local-large/",
        "teaser":null},{
        "title": "Using the command line/API for Accessing Economy Local Storage",
        "excerpt":"Access to data stored in Fred Hutch resources that are object stores can be achieved via command line tools or the API.   Accessing Economy File directly via Object API   This is an overview available tools starting with the ones you will most likely use and that are best supported.   swc (swift commander - simple access)  Using the swc command is the simplest way of accessing the swift object store. The tool includes easy to use sub commands such as swc upload and swc download as well as simplified versions of standard unix tools such as ls, cd, rm, etc. By using swc you can ignore most of the peculiarities of the swift object storage system and (almost) work with it like a traditional file system. It is the best option for HPC scripting and automation.   First, let’s invoke the swc command and see what it has to offer:   gizmod16:~&gt; swc *** Swift credentials not set. *** Please execute 'sw2account &lt;accountname&gt;' to get credentials. Use 'sw2account --save &lt;accountname&gt;' to set them as default.   Swift requires certain environment variables to be set for each PI.  You can easily set them by using the ‘sw2account’ command which asks for your HutchNet password:   rhino04:~&gt; sw2account groudine_m   now let’s try running ‘swc’ again, still without any command options, you are prompted for your hutchnet password:  petersen@rhino:~$ swc  Swift Commander (swc) allows you to easily work with a swift object store. swc supports sub commands that attempt to mimic standard unix file system tools. These sub commands are currently implemented: (Arguments in square brackets are optional).  swc upload &lt;src&gt; &lt;targ&gt;   -  copy file / dirs from a file system to swift swc download &lt;src&gt; &lt;targ&gt; -  copy files and dirs from swift to a file system swc cd &lt;folder&gt;           -  change current folder to &lt;folder&gt; in swift swc ls [folder]           -  list contents of a folder - or the current one swc mkdir &lt;folder&gt;        -  create a folder (works only at the root) swc rm &lt;path&gt;             -  delete all file paths that start with &lt;path&gt; swc pwd                   -  display the current swift folder name swc cat|more|less &lt;file&gt;  -  download a file to TMPDIR and view with cat, more or less swc vi|emacs|nano &lt;file&gt;  -  download a file to TMPDIR and edit it with vi|emacs or nano swc chgrp &lt;group&gt; &lt;fld.&gt;  -  grant/remove rw access to current swift account or container swc rw &lt;group&gt; &lt;folder&gt;   -  add rw access to current swift account or container swc ro &lt;group&gt; &lt;folder&gt;   -  add ro access to current swift account or container swc list &lt;folder&gt; [filt]  -  list folder content (incl. subfolders) and filter swc search &lt;str&gt; &lt;folder&gt; -  search for a string in text files under /folder swc openwith &lt;cmd&gt; &lt;file&gt; -  download a file to TMPDIR and open it with &lt;cmd&gt; swc header &lt;file&gt;         -  display the header of a file in swift swc meta &lt;file&gt;           -  display custom meta data of a file in swift swc mtime &lt;file&gt;          -  show the original mtime of a file before uploaded swc size &lt;folder&gt;         -  show the size of a swift or a local folder swc compare &lt;l.fld&gt; &lt;fld&gt; -  compare size of a local folder with a swift folder swc hash &lt;locfile&gt; &lt;file&gt; -  compare the md5sum of a local file with a swift file swc arch &lt;src&gt; &lt;targ&gt;     -  create one tar archive for each folder level swc unarch &lt;src&gt; &lt;targ&gt;   -  restore folders that have been archived swc auth                  -  show current storage url and auth token swc env                   -  show authentication env vars (ST_ and OS_) swc clean                 -  remove current authtoken credential cache  Examples: swc upload /local/folder /swift/folder swc upload --symlinks /local/folder /swift/folder (save symlinks) swc compare /local/folder /swift/folder swc download /swift/folder /scratch/folder swc download /swift/folder $TMPDIR swc rm /archive/some_prefix swc more /folder/some_file.txt swc openwith emacs /folder/some_file.txt   OK, we see that there are some options, the swc command has nearly 30 sub commands. Let’s upload a folder called ‘testing’ that is stored somewhere on fast file. The target folder on economy file is called /test:   petersen@rhino04:~/sc$ swc upload ./testing /test *** uploading ./test *** *** to Swift__ADM_SciComp:/test/ *** executing:swift upload --changed --segment-size=2147483648 --use-slo --segment-container=\".segments_test\" --header=\"X-Object-Meta-Uploaded-by:petersen\" --object-name=\"\" \"test\" \"./test\" *** please wait... *** /fld11/file12 /fld11/file11 /fld11/fld2/fld3/fld4/file43 /fld11/fld2/fld3/fld4/file42 . . . /test   let’s make sure that the data is really there by comparing the size of the local folder with the swift folder:   petersen@rhino02:~/sc$ swc compare ./testing /test     checking swift folder test ...     3,180,498,696 bytes (2.962 GB) in test (swift)     checking posix folder ./testing/ ...     3,180,498,696 bytes (2.962 GB) in ./testing OK! The size of ./testing and /test is identical!   now let’s download a subfolder of that folder /test to our scratch file system for further processing:   petersen@rhino04:~/sc$ swc download /test/fld1 /fh/scratch/delete30/lastname_f/tmp ...downloading /test/fld1, please wait... executing:swift download --prefix=\"fld1\" test fld1/file11 [auth 0.123s, headers 0.162s, total 0.170s, 0.000 MB/s] fld1/file12 [auth 0.118s, headers 0.164s, total 0.171s, 0.000 MB/s] fld1/fld2/file21 [auth 0.123s, headers 0.158s, total 0.169s, 0.000 MB/s] . . /fh/scratch/delete30/lastname_f/tmp/   please check our HPC and data migration tutorials in the scicomp wiki at http://scicomp.fhcrc.org and the swc page on github for additional details:      How to migrate large data to Economy File   swift commander on github   Check out this video how to the swc command   R swift client   The R swift package allows to upload and download files to swift directly from R. This greatly improves usability from interactive R sessions if you do not work on a SciComp supported Linux computer you need to install the R swift package first:   &gt; install.packages('devtools',repos='http://cran.fhcrc.org') &gt; devtools::install_github(\"mtmorgan/swift\") interacting with the object stop is pretty easy. If your credentials are set (see above) you can start right away. In this case we (1) list the content of the SciComp swift container, (2) download a file and (3) upload a results file. &gt; library('swift') &gt; &gt; swlist('SciComp')         size              last_modified                                 name 1    20.8 Mb 2014-12-08T07:01:27.141470                annotationhub.sqlite3 2     725 Kb 2014-12-08T06:54:18.677440                       tmp/squeue.txt 3     7.9 Kb 2014-12-08T06:54:18.974290                         tmp/test.csv  &gt; myfile &lt;- swdownload('SciComp','tmp/squeue.txt') &gt; myobject &lt;- read.table(myfile) &gt; &gt; ....processing &gt; &gt; newfile &lt;- paste0(myfile,'.csv') &gt; write.csv(myobject, newfile, row.names=FALSE) &gt; swupload('SciComp',newfile) &gt; ##### and even easier: use swread to load a file directly from swift into memory ###### &gt; myobj &lt;- swread('SciComp','tmp/squeue.txt') &gt; mydf &lt;- swread('SciComp','tmp/test.csv')  # a csv file always loads into a dataframe Limitations and Notes: only supports files with sizes less than 5GB   Swift standard client (python)   The swift client allows you to read and write files in your containers directly without mounting the container in the file system.  This is particularly handy for scripting and automation. The swift client is used by swc ‘under the hood’ and offers more options than swc but it is also slighly more difficult to use.  Do not use this tool if swc meets your needs.   The swift client operates similar to ftp or scp and can be used in batch mode.  These operations are:      delete: delete a container or objects within a container   download: download objects from containers   list: lists the containers for the account or the objects for a container   post: updates meta information for the account, container, or object   stat: displays information for the account, container, or object   upload: uploads files or directories to the given container   capabilities: lists features/capabilities of the swift cluster   tempurl: creates a temporary url with expiration date that can be published within the Fred Hutch network   You can supply your credentials on the command line, but this is discouraged as it can expose your password to any other user on the system. The easiest way to get credentials to Swift Economy File storage on SciComp system is using the command sw2account with the id of the PI as an argument (sw2account lastname_f) . It is recommend that you use the sw2account command to save the credentials of the PI account you mostly work with.   $ sw2account --save groudine_m   With those environment variables set, the connection can be tested with the “stat” subcommand:   $ swift stat Account: AUTH_username Containers: 0 Objects: 0     Bytes: 0 Accept-Ranges: bytes X-Timestamp: 1386202905.14525 X-Trans-Id: tx93f905a47d044277b276c-0052a0ca26 Content-Type: text/plain; charset=utf-8   The “post” subcommand creates a new container into which we can “upload” a file.  The “list” subcommand rather obviously displays the contents of the indicated object:   $ ls -l samplefile.tgz -rw-rw-r-- 1 user group 8134841 Nov 12 09:12 samplefile.tgz $ swift post newcontainer $ swift list newcontainer $ swift list newcontainer $ swift upload -S 2G newcontainer samplefile.tgz samplefile.tgz $ swift list newcontainer samplefile.tgz  you can upload an entire folder structure using the swift upload command. swift will recreate the entire folder structure you send on the command line. If you run “swift upload -S 2g newcontainer /fh/fast/lastname_f/myfolder” it will create the entire folder structure /fh/fast… under newcontainer. if you just want myfolder to show up under mycontainer you need to cd to  /fh/fast/lastname_f  and then run  “swift upload newcontainer myfolder”   “download” and “delete” work as you might expect:   “download” and untar a compressed file directly into Gizmo’s scratch file system (–output -  sends the downloaded file directly to STDOUT )   $ swift download newcontainer samplefile.tar.gz --output - | tar xzv -C $SCRATCH/$SLURM_JOB_ID   Limitations and Notes      Files larger than 5GB require segmentation.  The value indicated with the “-S” argument is the size that files will be split into on the object store.  A segment size may be any value up to 5GB.  A reasonable default value is -S 2G which sets the maximum segment size to 2 GB, which means a file of 10GB size will be split into 5 segments.   using multiple segments will increase upload speeds (and downlaod speeds in the future), setting the segment size to 1GB on a 10GB file can increase the theoretical upload performance 10 fold   Swift commands that fail don’t set a return code, which could complicate scripting.   You can’t delete a directory in a container. When you delete the last file from the directory, swift deletes the directory automatically.  However, swift will delete a container with files still in it, so be careful what you delete.   Amazon S3 (boto)   Swift has an Amazon S3 compatiblity layer that supports most of the S3 API. boto is a very popular python library that allows you to interact with S3 compatible object stores. It is currently installed on most scientific computing systems such as the rhinos and gizmo nodes. The current version this boto3. You can also install it on your own system:   On SciComp systems boto3 is already in the latest Python, just load the module   $ ml Python   or you install boto within virtualenv, please check the scicomp wiki for “Manage Python environments using virtualenv”   Please see this detailed and current example for boto3 in the scicomp wiki:   Boto3 access to Economy File   The client method below is a little  older and exposes more low level access to s3. It should not be required for most access.   To use this method, put your credentials into a text file (.e.g.  ~/.aws/s3.fhcrc.org) separated by colon:   echo \"accountname:secretkey\" &gt; ~/.aws/s3.fhcrc.org   then you run your first example: using Boto to list your existing storage containers (buckets):   #! /usr/bin/env python3  import boto3, os  with open(os.path.expanduser('~/.aws/s3.fhcrc.org'), 'r') as f:     access_key_id, secret_access_key = f.readline().strip().split(':')  s3client = boto3.client('s3',         endpoint_url='https://s3.fhcrc.org',         aws_access_key_id = access_key_id,         aws_secret_access_key = secret_access_key)  l = s3client.list_buckets() print(l)  boto3 is the current and recommeded version of boto, however you will still find many examples for boto, the legacy version of the library import boto.s3.connection  connection = boto.s3.connection.S3Connection(     aws_access_key_id='EC2_ACCESS_KEY or USERNAME',     aws_secret_access_key='S3 API KEY',     port=443,     host='s3.fhcrc.org',     is_secure=True,     validate_certs=False,     calling_format=boto.s3.connection.OrdinaryCallingFormat() )  b=connection.create_bucket('mybucket')  buckets = connection.get_all_buckets() print buckets   There is more detailed documentation about boto available here   Python (swiftclient)   Instead of using the swift command from the unix shell one can also access swift directly from python using the swiftclient python module which is also used by the swift command line client. This is a quick example that writes 100 objects / files to a swift container called prj1. create this container by using the command swift post prj1   #! /usr/bin/env python # -*- encoding: utf-8 -*- import os, sys, random from swiftclient import client as swiftclient  arg=0 if len(sys.argv) &gt; 1:     arg=sys.argv[1]  object_base_name = \"%s-array\" % arg content_base = \"asdasdfasdfafaf\" container_name = \"prj1\"  AUTH = os.environ.get('ST_AUTH') USER = os.environ.get('ST_USER') PASSWORD = os.environ.get('ST_KEY')  post_url, post_token = swiftclient.Connection(     '%s' % (AUTH),     USER,     PASSWORD, auth_version=1).get_auth()  #headers = get_headers #headers['x-auth-token'] = post_token headers = {'x-auth-token': post_token} sync_to = post_url + '/' + container_name  for i in range(100):     x = random.random()     object_name = \"%s-%s\" % (object_base_name,i)     content = \"%s-%s\" % (content_base,x)     swiftclient.put_object(sync_to, name=object_name, headers=headers,                     contents=content)   Access From the Command Line or REST API   Economy File supports several access methods:   Direct Access / Linux / Rhino   Please see How to Access the Swiftstack Object store to learn about direct Access via API or command line tools. If you want to use data from Economy File on the Gizmo cluster or from the Rhinos this is your preferred access method.   Other Access Methods   There are many other tools out there that support object storage systems. Swiftstack supports the Swift API and the more common S3 API. To get your credentials for these access methods please use this link https://toolbox.fhcrc.org/sw2srv/swift/account and enter the name of the PI or department account account (e.g. lastname_f).  You may be prompted for your hutchnet ID password.   If you have permissions to see the credentials you will get 3 entries:   {  \"account\": \"Swift_lastname_f\",  \"key\": \"abf47sfj48sfrjsrg8usrgj\",  \"password\": \"Huew4jv&amp;jfwvjsdg\" }   For tools that use S3 protocol you need account and key. Use the entry in account for “access key id” and the entry in ‘key’ for “secret key id”. Connect these tools to https://s3.fhcrc.org   For tools that use the Swift protocol you need the account, a password, and an authentication endpoint (a URL used for authenticating your credentials). Use https://tin.fhcrc.org/auth/v1.0 for this.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/EconomyCommand-API/",
        "teaser":null},{
        "title": "Using Mountain Duck or Cyberduck for Accessing Economy Storage",
        "excerpt":"Access to data stored in Fred Hutch resources that are object stores can be achieved using clients.  As of March 2016 Center IT officially supports Mountain Duck and Cyberduck clients for this purpose. You can use these Windows or Mac clients to move small amounts of data (Gigabytes rather than Terabytes) and occasionally open a file for editing.   Mountain Duck can map Economy File as a simple drive in Windows Explorer or in Mac Finder. Mountain Duck is the default choice for most users and you should try it first before you move to other options. The performance of Mountain Duck is limited.   Mountain Duck is a commercial tool based on Cyberduck. If you don’t need to mount Economy File as a drive on your computer, Cyberduck may be sufficient for you. Cyberduck copies files 5-10 times faster than Mountain Duck and it uses the same bookmarks as Mountain Duck so you need to configure each connection only once.  The two tools work great in combination.   Mountain Duck   Mountain Duck is best if you want to work with economy file just like a normal share or drive. You can navigate through the file system, double click on an excel spreadsheet, modify and save it. This convenience come with a significant downside: Mountain duck is really quite slow. It will only copy data at about 5MB / second for uploads and no more than 20MB/s for downloads. This is ok for most excel sheets but too slow for imaging or genomic data.   Ask the Helpdesk to install Mountain Duck: Contact the HelpDesk (helpdesk@fhcrc.org or x5700) or your divisional IT support and ask for an installation Mountain Duck on Windows or Mac If required ask the Helpdesk or your divisional IT support to be added to the security group lastname_f_grp of your PI (e.g. gilliland_g_grp). Make sure you CC your PI or Manager to let them know that you are requesting access.   Installing Mountain Duck yourself   ​Windows 10   Open Software Center and search for Mountain Duck and install the package using the Install button Restart your Computer if the installer asks you to do so. After install you should see an orange icon in your system tray (if you do not see it in the tray start Mountain Duck from the start menu.) Continue with “Configuring Mountain Duck” below   ​Windows 7:   Open folder X:\\fast_ADM\\SciComp\\setup\\packages\\MountainDuck\\ on the Center Drive (X) Start (double click) file INSTALL.bat. The install script will install a tested version of mountain duck, a profile and a license file​​ Continue with “Configuring Mountain Duck” below      Mac OSX:   Download and install the Software in trial mode from https://mountainduck.io/  Download the Swiftstack profile from this link and open / doubleclick it from your downloaded files: http://files.swiftstack.com/OpenStack Swift Auth v2.0 (SwiftStack HTTPS).cyberduckprofile To get a license file download fredhutch50.mountainducklicense  and double click the file or drag it to the Mountain Duck application icon to register.   Configuring Mountain Duck      Start Mountain Duck (if not already started) and click the Tray icon (see above) ​- Select “New Bookmark” and then profile “OpenStack Swift Auth v2.0 (SwiftStack HTTPS)” from the pull down menu   Enter the following settings: (lastname_f is the hutch investigator ID, eg. gilliland_g or groudine_m, username is the hutchnet id)      Nickname: Eco_lastname_f (or whatever you like, should be unique per PI) Server: tin.fhcrc.org Tenant Name:User Name: AUTH_Swift_lastname_f:username  (case sensitive: this is the Tenant name of the PI and your user name separated by a colon, e.g. AUTH_Swift_gilliland_g:markg or AUTH_Swift_groudine_m:gary etc. ) Path: /auth/v​2.0 (optional, not required in Swiftstack profile)       After you are done, click “Connect” to save the settings, you will be prompted for your password. After entering your password wait a few seconds. Your drive should open or you should see it in Windows Explorer. If you are having problems please check How to troubleshoot MountainDuck and CyberDuck   Cyberduck (Mac, Windows)   Installing Cyberduck   If required ask the Helpdesk or your divisional IT support to be added to the security group lastname_f_grp of your PI (e.g. gilliland_g_grp). Make sure you CC your PI or Manager to let them know that you are requesting access. If you are installing Cyberduck yourself (e.g on a Mac) you need to download the Swiftstack profile from this link and open it from your downloaded files: http://files.swiftstack.com/OpenStack%20Swift%20Auth%20v2.0%20(SwiftStack%20HTTPS).cyberduckprofile If you have permission to install software yourself install it from https://cyberduck.io/ . Please use the download links below the yellow duck and install the software. You do not need to pay for the software through the Mac App store.   Configuring Cyberduck  ​At the top select OpenStack Swift Auth 2.0 (SwiftStack HTTPS) from the bottom of the list. Add the content below to the following fields:   Nickname: EcoFile (or whatever you like) Server: tin.fhcrc.org Tenant Name:User Name: AUTH_Swift_lastname_f:username (case sensitive: this is the Tenant name of the PI and your user name separated by a colon, e.g. AUTH_Swift_gilliland_g:markg or AUTH_Swift_groudine_m:gary etc.   Path: /auth/v​2.0   Confirm by simply closing this bookmark Window and then click on the bookmark. You are prompted for your hutch net ID password.      To get a general feel how the software works please see this video on youtube Note: In this video Cyberduck is connecting to Amazon S3 which is very similar to the Economy File storage system. Another video shows a user who is working with Cyberduck on a Mac.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/Mountain-CyberDuck/",
        "teaser":null},{
        "title": "Using Singularity Containers",
        "excerpt":"Docker can only run containers as root so we cannot use them in shared multi-user environments such as Rhino/Gizmo with shared storage such as /home, /fh/fast or /fh/scratch. Singularity can import Docker containers and run them.   Understanding Singularity   Singularity containers can run under any user account once created. This is possible:       petersen@gizmof13:~$ ml Singularity     petersen@rhinof13:~$ singularity exec ~/ubuntu-python.simg python3 /fh/fast/_HDC/team/SciComp/script.py   We are loading the singularity lmod environment on Gizmo and are then running python3 with a script residing in Fast. The python3 binary is not executed on Gizmo but Singularity is using python3 from inside the ubuntu-python.img container which is stored in my home directory. This process allows us to package entire pipelines in containers and integrate them seamlessly in HPC workflows as /tmp, /home and /fh are always read from the Gizmo environment outside the container.   Preparing Singularity and a workaround (DO NOT USE IT ON RHINO)   Before you use Singularity please let SciComp know the users in your group and the PI folder you will be working with, otherwise you will get this error which also affects others.       $ singularity run  container.simg     ERROR : There was an error binding the path /app: Too many levels of symbolic links     ABORT : Retval = 255   You should only use Singularity on a Gizmo node and never on Rhino. SciComp will only allow users to run Singularity who are able to follow this guidance consistently.   Using Docker Containers with Singularity   Docker containers are the predominant storage format for containerized workflows and it is important to know that Singularity can easily import Docker containers. To create a new container from a Docker image on DockerHub you just need to run the singularity pull command (make sure you run ml Singularity before):       petersen@gizmof13:~$ ml Singularity     petersen@gizmof13:~$ singularity pull docker://ubuntu:latest      WARNING: pull for Docker Hub is not guaranteed to produce the     WARNING: same image on repeated pull. Use Singularity Registry     WARNING: (shub://) to pull exactly equivalent images.     Docker image path: index.docker.io/library/ubuntu:latest     Cache folder set to /home/petersen/.singularity/docker     Importing: base Singularity environment     Exploding layer: sha256:84ed7d2f608f8a65d944b40132a0333069302d24e9e51a6d6b338888e8fd0a6b.tar.gz     .     .     Exploding layer: sha256:e9055237d68d011bb90d49096b637b3b6c5c7251f52e0f2a2a44148aec1181dc.tar.gz     Exploding layer: sha256:c6a9ef4b9995d615851d7786fbc2fe72f72321bee1a87d66919b881a0336525a.tar.gz     WARNING: Building container as an unprivileged user. If you run this container as root     WARNING: it may be missing some functionality.     Building Singularity image...     Singularity container built: ./ubuntu-latest.simg     Cleaning up...     Done. Container is at: ./ubuntu-latest.simg   Now you can just use any tool inside the singularity container using the exec command shown above or you shell into the container to see what’s in there:       petersen@gizmof13:~$​ singularity run ./ubuntu-latest.simg     WARNING: Failed to open directory '/fh/fast/xyz'     WARNING: Failed to open directory '/fh/fast/abc'     WARNING: Failed to open directory '/fh/fast/klm'     WARNING: Failed to open directory '/home/xyz'     WARNING: Failed to open directory '/home/abc'     WARNING: Non existent bind point (directory) in container: '/app'     WARNING: Non existent bind point (directory) in container: '/fh'     petersen@ubuntu-latest.simg&gt;    You will get an error message which means that empty folders inside the container for mount points /app and /fh are not created yet. You can address this by creating these folders in the docker container you pull from DockerHub. There are also some warnings like WARNING: Failed to open directory You can ignore those warnings.   Tuning your Environment   Singularity uses settings from the home directory of the invoking user on the host system, for example .bashrc. The recommended settings for ~/.bashrc: check for a Singularity symlink at the root or a SINGULARITY_NAME env var. In some cases you want bash to behave differently if you are inside a container. You can put this into ~/.bashrc.       if [ -L '/singularity' ]; then         PS1='\\u@$SINGULARITY_CONTAINER&gt; '         export PROMPT_COMMAND=''     else         PS1=''     fi   Creating custom Singularity Containers for Docker Images   Sometimes you would like a larger Singularity container than your docker container and/or customize your singularity container after the fact. (This should be the exceptions as we want to use docker containers in most cases.  If you have root access (SciComp staff) you can use the esudo wrapper (yes, use esudo wrapper instead of sudo!) to open the image as root in write mode to create the 2 empty folders to mount file systems.       petersen@gizmof13:~$​ singularity create --size 2048 ubuntu-1404.img     petersen@gizmof13:~$​ singularity import ubuntu-1404.img docker://ubuntu:trusty     petersen@gizmof13:~$​ ​esudo singularity shell -w ubuntu-1404.img      WARNING: Non existant bind point (directory) in container: '/app'     WARNING: Non existant bind point (directory) in container: '/fh'     Singularity: Invoking an interactive shell within container...      Singularity ubuntu-1404.img:~&gt; mkdir /app /fh   SciComp’s Singularity install is configured to allow access to mounted file systems at /app, /fh and /mnt, this is set in singularity.conf, for example these 3 lines were added to /app/easybuild/software/Singularity/2.x.x-GCC-5.4.0-2.26/etc/singularity/singularity.conf:       bind path = /app     bind path = /fh     bind path = /mnt   Troubleshooting   You might see this intermittent Error with Singularity:   ERROR  : Could not open image /fh/fast/.......: Too many levels of symbolic links   the gory details are reported here:  https://github.com/sylabs/singularity/issues/2556   Workaround: You have not notified SciComp that you want to use this folder with Singularity. Please do so ASAP ​ Please email SciComp to request assistance and discuss which environment is best for your needs.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/Singularity/",
        "teaser":null},{
        "title": "Using R to Access AWS S3",
        "excerpt":"This demo provides specific examples of how to access AWS S3 object storage via the AWS CLI.  You can use Amazon Web Services’ S3 (Simple Storage Service) directly from R.  The R package which facilitates this, aws.s3, is included in recent builds of R available on the rhino systems and the gizmo and beagle clusters.   Getting Started   The first step is to load a recent R module:   ml R/3.5.0-foss-2016b-fh1   Then start R:   R   Load the aws.s3 R package:  library(aws.s3)   NOTE: The example fragments from this point on assume you are in an R session with aws.s3 loaded.   List all buckets   blist &lt;- bucketlist()   List all objects in a bucket   The bucket name you supply must be one you have access to.   b &lt;- 'fh-pi-doe-j' objects &lt;- get_bucket(b)   Get bucket contents as a data frame   df &lt;- get_bucket_df(b)   Saving objects to S3   Create a data frame of random numbers and save it to S3:   df &lt;- data.frame(replicate(10,sample(0:1,1000,rep=TRUE))) s3save(df, bucket=b, object=\"foo/bar/baz/df\")   Loading objects from S3   # first remove the object from memory if it's there: if (\"df\" %in% ls()) rm(\"df\") # now load it: s3load(object=\"foo/bar/baz/df\", bucket=b) # demonstrate that it exists again: head(df)   Upload a file to S3   First, write the existing df data frame to a csv file on your local disk:   write.csv(df, file=\"df.csv\") # copy the csv to s3: put_object(\"df.csv\", \"foo/bar/baz/df.csv\", b)   Read a CSV in S3 into a data frame   # first remove the object from memory if it's there: if (\"df\" %in% ls()) rm(\"df\") df &lt;- s3read_using(read.csv, object=\"foo/bar/baz/df.csv\", bucket=b) # demonstrate that it exists again: head(df)   Download a file from S3   This will create the file df.csv in the current directory:   save_object(\"foo/bar/baz/df.csv\", b)   Work with object names matching a pattern   Assume your S3 bucket has three objects whose keys start with foo/bar/baz/ and end with one of d, e, or f. You want to read each object into memory and end up with d, e, and f objects in your R session.   bdf &lt;- get_bucket_df(b) matches &lt;- bdf$Key[grep(\"^foo/bar/baz/\", bdf$Key)] for (match in matches) {   s3load(object=match, bucket=b) }   Write data frame to S3 as a file  When you have a data frame in R that you’d like to save as an object in S3, you’ll do the following:   # using write.table s3write_using(df,                FUN = write.table, quote = F, row.names = F, sep = \"\\t\",                object = \"foo/bar/baz/df.txt\",               bucket = b)   # write write.csv s3write_using(df,                FUN = write.csv, quote = F, row.names = F,                object = \"foo/bar/baz/df.csv\",               bucket = b)   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/aws-R/",
        "teaser":null},{
        "title": "Using the AWS Command Line Interface (CLI) to Access AWS S3",
        "excerpt":"This demo provides specific examples of how to access AWS S3 object storage via the AWS CLI.   Copy a file to an S3 bucket  This command will copy the file hello.txt from your current directory to the top-level folder of an S3 bucket:   aws s3 cp hello.txt s3://fh-pi-doe-j/   You can also copy files to folders within your bucket. Folders do not have to be created beforehand. This examples copies the file hello.txt to the folder path a/b/c:   aws s3 cp s3://fh-pi-doe-j/hello.txt s3://fh-pi-doe-j/a/b/c/   Copying files from an S3 bucket to the machine you are logged into This example copies the file hello.txt from the top level of your lab’s S3 bucket, to the current directory on the (rhino or gizmo) system you are logged into. The current directory is represented by the dot (.) character.   aws s3 cp s3://fh-pi-doe-j/hello.txt .   After running this command, you’ll have the file hello.txt in your current directory.   This example copies hello.txt from the a/b/c folder in your bucket to the current directory:   aws s3 cp s3://fh-pi-doe-j/a/b/c/hello.txt .   Creating S3 prefixes   You can also copy files directly into an S3 prefix (denoted by a “PRE” before the name on S3). The prefix does not have to already exist - this copying step can generate one. To copy a file into a prefix, use the local file path in your cp command as before, but make sure that the destination path for S3 is followed by a / character (the / is essential).   For example:   aws s3 cp s3://fh-pi-doe-j/hello.txt s3://fh-pi-doe-j/test_prefix/   will copy hello.txt into the PRE named test_prefix. Without the trailing /, the file hello.txt will be copied into the S3 bucket under the filename test_prefix, rather than into the desired prefix itself. If the prefix test_prefix does not already exist, this step will create it and place hello.txt within it.   Listing bucket contents   This example will list the contents of your lab’s bucket:   aws s3 ls s3://fh-pi-doe-j/   To list t​he contents of a specific folder, just add the folder path to the end of the previous example:   aws s3 ls s3://fh-pi-doe-j/a/b/c/   More S3 Commands   The complete set of AWS S3 commands is documented here, and you can also type:   aws s3 help   for more information. To see documentation for a specific s3 subcommand, such as cp, do this:   aws s3 help cp   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/aws-cli/",
        "teaser":null},{
        "title": "Using Python to Access AWS S3",
        "excerpt":"From any of the rhino systems you can see which Python builds are available by typing ml Python/3. and pressing the TAB key twice. Choose the most recent version (at the time of writing it is Python/3.6.5-foss-2016b-fh3). Once you have loaded a python module with ml, the Python libraries you will need (boto3, pandas, etc.) will be available.   You can then get to an interactive Python prompt with the python command, but many prefer to use ipython to work with Python interactively.   Getting Started   The first step is to load a recent Python module:   ml Python/3.6.5-foss-2016b-fh3   Then start Python:   python   Import Libraries  From within python (or ipython) do the following to get started:   import boto3 import numpy as np import pandas as pd import dask.dataframe as dd from io import StringIO, BytesIO  s3 = boto3.client(\"s3\") s3_resource = boto3.resource('s3')  bucket_name = \"fh-pi-doe-j\" # substitute your actual bucket name   The following fragments all assume that these lines above have been run.   List all buckets in our account   response = s3.list_buckets()   The command above returns a lot of metadata about the buckets. If you just want to see the bucket names, do this as well:   for bucket in response['Buckets']:     print(bucket['Name'])   List all objects in a bucket   response = s3.list_objects_v2(Bucket=bucket_name)   Again, this response contains a lot of metadata. To view just the object names (keys), do this as well:   for item in response['Contents']:     print(item['Key'])   Note that this method only returns the first 1000 items in the bucket. If there are more items to be shown, response['IsTruncated'] will be True. If this is the case, you can retrieve the full object listing as follows:   paginator = s3.get_paginator('list_objects_v2') page_iterator = paginator.paginate(Bucket=bucket_name) for page in page_iterator:     for item in page['Contents']:         print(item['Key'])   Read object listing into Pandas data frame   response = s3.list_objects_v2(Bucket=bucket_name) df = pd.DataFrame.from_dict(response['Contents'])   About pandas and dask   There are two implementations of data frames in python: pandas and dask). Use pandas when the data you are working with is small and will fit in memory. If it’s too big to fit in memory, use dask (it’s easy to convert between the two, and dask uses the pandas API, so it’s easy to work with both kinds of data frame). We’ll show examples of reading and writing both kinds of data frames to and from S3.   NOTE: Pandas dataframes are usually written out (and read in) as CSV files. Dask dataframes are written out in parts, and the parts can only be read back in with dask.   Saving objects to S3   # generate a pandas data frame of random numbers: df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))  # save it in s3: csv_buffer = StringIO() df.to_csv(csv_buffer) s3_resource.Object(bucket_name, 'df.csv').put(Body=csv_buffer.getvalue())  # convert data frame to dask: dask_df = dd.from_pandas(df, 3)  # save dask data frame to s3 in parts: dask_df.to_csv(\"s3://{}/dask_data_parts\".format(bucket_name))    Reading objects from S3   To read the csv file from the previous example into a pandas data frame:   obj = s3.get_object(Bucket=bucket_name, Key=\"df.csv\") df2 = pd.read_csv(BytesIO(obj['Body'].read()))   To read the parts written out in the previous example back into a dask data frame:   dask_df2 = dd.read_csv(\"s3://{}/dask_data_parts/*\".format(bucket_name))   Upload a file to S3   # write the example data frame to a local file df.to_csv(\"df.csv\")  # upload file: s3.upload_file(\"df.csv\", Bucket=bucket_name, \"df.csv\")   Download a file from S3   # second argument is the remote name/key, third argument is local name s3.download_file(bucket_name, \"df.csv\", \"df.csv\")  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/aws-python/",
        "teaser":null},{
        "title": "Instructions for Using Koshu (beta)",
        "excerpt":"About Koshu   The cluster Koshu is a cloud-based Slurm cluster based in Google’s Cloud Platform.      Koshu is currently under beta as it is currently not sized for large-scale work and may experience more maintenance time than other clusters or be missing some functionalities.  We’re using this to “kick the tires,” make sure everything functions before bringing it into production.  Check back frequently for updates or, as always, contact SciComp with any questions or to note any problems you experience when using Koshu.    At this time there are three partitions: the “campus” partition with smaller nodes, a “largenode” partition with nodes that have greater capacities (processors and memory), and a partition named “gpu” that has GPU capabilities.                  Partition       Node Class       Count       Processors       Memory       GPU                       campus       koshuf       70       4       32,768 MB       none                 largenode       koshug       10       8       262,144 MB       none                 gpu       koshuk       10       4       131,072 MB       1 NVIDIA Tesla v100           Note that the processors are hyperthreaded, which means that each physical CPU could be assigned two threads.  Whether this is an improvement for your tools will affect how you run them.   Koshu has access to most Hutch storage systems in the same paths as on gizmo.  The exception to this is the scratch directories: /fh/scratch/delete10 and /fh/scratch/delete30.  These directories are on different file systems and are not available outside the koshu environment.   Using Koshu   Much like Beagle, jobs are routed to Koshu by use of the -M option when running Slurm commands on campus resources (e.g. rhinos):   sbatch -M koshu ...   Note that salloc and srun do not support this.  To run interactive sessions you need to log into the host koshu-login.   Managing CPUS   As indicated, there are 4 CPUs indicated in Slurm while the host has 8 available CPUs.  If your workload benefits or at least does not suffer from hyperthreading, then you can use all 8 of the processors.  However, if your workload won’t benefit from this, you need to adjust the way you launch your tools as many will default to using all configured processors on the node.   For example, bowtie2 has the option -p to control the number of threads it will use:   bowtie2 -p 4 ...   would use four threads.  The best-practice for this would be to use the Slurm environment variable SLURM_CPUS_ON_NODE which will always contain the number of processors you have been assigned on the node:   bowtie2 -p ${SLURM_CPUS_ON_NODE} ...   Managing GPUs   Two options are required for using the GPUs in this cluster.  First you must select the gpu partition and then you must select the number and type of GPU using Slurm’s gres options:   sbatch -M koshu -p gpu --gres=gpu:V100-SXM2-16GB:1 ...   It is a bit cumbersome, but this will (as the cluster gains capabilities) allow greater flexibilty in selecting resources for your job as well as ensuring that GPU tools run correctly.   Technically, omitting the gres option won’t prevent the job from running.  However, the job won’t have some necessary environment variables set and it would be possible that a node’s GPU would get oversubscribed or that other failures will occur.   There are several environment variables set in a job:                  Variable       Purpose                       CUDA_VISIBLE_DEVICES       Tell CUDA capable tools which GPUs to use                 GPU_DEVICE_ORDINAL       Tell OpenLC capable tools which GPUs to use                 SLURM_JOB_GPUS       Environment variable set by Slurm with allocated GPUs           These should automatically constrain most tools to the appropriate GPUS.  At this time, there’s only a single GPU per host, but we will likely adjust that depending on needs and input from the community, so ensuring that scripts and tools use these variables to control which GPUs are used is important.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/cluster_koshuBeta/",
        "teaser":null},{
        "title": "Parallel Computing on Slurm Clusters",
        "excerpt":"Parallel computing is an approach to computing where many tasks are done simultaneously- either on a single device or on multiple independent devices. These tasks can be dependent or independent of each other requiring varying degreees of ordering and orchestration.  Parallel computing can be quite complicated to set up but can improve job throughput when done correctly.   Parallel computing starts with breaking a larger task into smaller steps- the “size” and relationship of those steps is highly dependent on the task at hand but determines much about how the job can be “parallelized”. Because of the variety of approaches to large tasks, often there can be multiple strategies to consider using to identify the most effective approach to use for the particular task at hand.   When steps are highly dependent on each other (e.g. the output of one step is used for input into the next) that job is said to be “serial” and it won’t benefit greatly from parallel processing.  At the other end, “embarassingly” or “pleasantly” parallel work has individual steps that do not depend on each other and can occur in at the same time, often in great numbers.   Once you’ve determined how your work can be parallelized, there are two ways to distribute those steps.  The first uses the capabilities of multi-core CPUs- modern CPUs have multiple cores, each of which are capable of processing independent steps.  This technique is typically referred to as “threading.”   Another approach to using multiple processors is to spread the work over multiple different computers.  This approach has the advantage of being able to scale up the amount of computation being done concurrently.  This approach is often described as “distributed”      Note: It is also possible to combine those techniques- using multiple cores on multiple computers.  This can add a little complexity, but many tools will handle this neatly.    Choosing an Approach   The primary drivers for choosing between the two approaches is how much communication between individual steps is necessary and how many steps there are. Communication between steps is computationally expensive, and if that communication needs to cross a network (as in a distributed solution) there can be a degredation in performance compared to keeping all of the steps on the same system (as in the threaded solution).  However, if there are many steps the resources on a single system will be a bottleneck, which makes a distributed solution more appealing.   An Atlas of Computational Workloads   Pleasantly Parallel   “Pleasantly parallel” work (AKA “embarassingly parallel”) is typically made up of many completely independent steps.  By independent we mean that:      any one step does not depend on the output or completion of any other step   steps do not need to exchange information with other steps   Examples: simulations, GWAS, chromosome by chromosome analyses   Sequential   This is one opposite of pleasantly parallel.  In sequential workloads each step is dependent on another step- step “B” cannot proceed until step “A” is complete. This kind of workload is nearly impossible to speed up with additional processors.   Highly Connected   Another opposite of the pleasantly parallel workload is workload where steps communicate information between other steps.  Weather and climate forecasts are notorious for this kind of workload- each step represents a block of atmosphere which is affected by its neighbors, thus step needs to look at the state of another step and vice-versa.   These problems require very low-latency, high-speed communication between steps and are typically better served when run on a single system (or one of the exotic supercomputers).  That said, modern networks are fairly good and can provide usable service for this communication if the number of steps greatly exceeds the number of cores available on a single system.   Parallel Operations in Slurm   Slurm has two concepts important when looking at implementing a parallel workload: jobs and steps.  The step is the atomic unit of a job- a job can be made up of multiple steps.  In general, the steps are independent and don’t directly communicate with each other. Steps can execute across many CPUs and hosts and many steps can run simultaneously within the job. For example, suppose I need to do three things to a data set:      divide it into pieces   run a calculation on each of those pieces   summarize those calculations   In Slurm, I can create a job made up of three steps.  The first step is sequential- it will use one CPU on one host.  The second step can be run on multiple CPUs as these calculations are independent.  The last step is, again, a sequential task that can only use a single CPU.  The job would need to request as many CPUs as could be used (or is practical) by the second step- the first and last step would only use a single CPU.   Examples  We have begun consolidating examples of parallel computing approaches in the FredHutch/slurm-examples repository.  Please refer to that repository for more community curated exmaple approaches and associated documentation to see if someone has approached a problem similar to yours so you don’t have to start from scratch.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/cluster_parallel/",
        "teaser":null},{
        "title": "Matlab",
        "excerpt":"Desktop Usage  The majority of Matlab users at the Hutch have Matlab installed on their desktop system. Most users have dedicated licenses for their desktop installs Matlab. Dedicated licenses offer the flexibility to use Matlab with a laptop on or off campus. The center has a limited number of shared network licenses. Users are allowed to use a maximum of 4 concurrent Matlab licenses from the license manager. The shared network licenses can be used with the centers Linux cluster. This document describes how to use Matlab on the Gizmo cluster.   Interactive Matlab on Linux  If you have a Windows or Mac desktop and want to run Matlab interactively on a Linux server, you will need to map the display from Linux to your Windows system.  We recommend using noMachine for both display mapping and session management. Do not run Matlab directly from the noMachine servers or from a Rhino.  Use grabnode to allocate a cluster node for starting a Matlab session.      From your Linux session manager, start a terminal.   ssh rhino   Type grabnode Answer the questions about how many CPUs and Memory you require. When your grabnode is successful you will have a Linux command prompt on a gizmo cluster node.   module load matlab/R2017a   Type matlab and press enter.  The Matlab window should open in your Linux session. Success!   Running Matlab batch jobs on Linux  Matlab programs can be run on the gizmo cluster. The cluster supports additional resources that are not available to a desktop install.      Start a terminal session on a Rhino   Your Matlab program and data need to be available to the Gizmo/Rhino systems.   Create a batch script to run your Matlab program   #!/bin/bash #SBATCH --partition=campus #SBATCH --time=1-0 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=31GB  # No comments above this line # this job is requesting 4 CPUs and 31GB of memory for 1 task myworkdir=/fh/fast/PI_b/user/UserName/project_dir/  # These two lines need to be in your script to define the Matlab environment source /app/Lmod/lmod/lmod/init/bash module laod matlab/2017a  cd $myworkdir matlab -nodisplay -nosplash -nodesktop -r  \"run('myAnalysisJob.m $1 $2'); exit;\"   MATLAB Distributed Compute Engine  The distributed compute engine allows you to run parallelized Matlab programs across multiple cores and/or nodes simultaneously.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/matlab/",
        "teaser":null},{
        "title": "Shiny Applications",
        "excerpt":"Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions. For more information about Shiny, go to Rstudio’s Shiny page.   There are currently two approaches available for deploying Shiny apps, either manually via Shinyapps.io, or via a SciComp supported pipeline.  Which approach is best for a given project depends on what the needs of the app are:      Using Shinyapps.io to deploy Shiny Apps            It streamlines with RStudio allowing auto-deployment in the interface.       There are multiple plans, such as the free plan which currently includes 25 hours activity per month, or the ‘standard’ and ‘professional’ plans which include password protection (both) and custom domain/urls (professional only), for a monthly subscription fee.       The free plan currently limits users to 5 apps per account!       Deploying shiny apps on Shinyapps.io does not require the use of GitHub or allows you to include multiple applications within a single GitHub repository if you have many apps to manage.       Provides a web analytics feature showing app activity.           Using the application deployment pipeline developed by Fred Hutch Scientific Computing            SciComp’s pipeline has the capability to host applications written in other languages as long as the application itself can be containerized.       Integrates with data sources within Hutch campus. For example, the application can be connected to a campus based database system (Postgres, mysql, etc…) without exposing it to the entire internet.       There is a template repository (https://github.com/FredHutch/shiny-app-template) to help users to assemble your own shiny app.       Once the application is deployed, updates/re-deploying is done by committing changes to GitHub, as the pipeline is based on a continuous integration/continuous delivery (CI/CD) feature.       Users can pick their own custom site URL in the fredhutch.org domain (example: ‘myshinyapp.fredhutch.org’).       User can specify if their app is only facing campus within the firewall or being exposed to the entire internet. Authorization feature can also be included upon request.       This service is completely free to campus users regardless of usage hours or number of apps.           To deploy a Shiny app via the Fred Hutch system, you must first have access to the Fred Hutch institution GitHub.  You can find more information about getting set up with GitHub at Fred Hutch here.  As an overview, you must first create a GitHub username, and then have scicomp connect it to the Fred Hutch institution.   Set up your GitHub Repository  There are two ways to set up your repository, and either is suitable.  Note:  Documentation for using GitHub at the Fred Hutch is being generated here, and it is a good place to start to find the necessary help you may need to use GitHub to set up your Shiny app.   To do a basic set up via the web, go to GitHub and create a new repository in the Fred Hutch institution for your app.  Then clone your new repository to your local computer. In the local folder created, create a folder called app and put your shiny app files (either app.R, or ui.r/server.R) and any other associated data files needed for the app in the app directory.  (Once you are finished and have tested your app locally, either via R or RStudio, push your edits to GitHub. )   Cloning the Template Set Up   The template for your app can be found in this GitHub Repo (accessible after login): FredHutch/Shiny-app-template  Clone this repository, remove the unnecessary files listed and add your Shiny app files to the app directory, before pushing your changes to a new repo on GitHub.   Using Command Line Git          git clone https://github.com/FredHutch/shiny-app-template.git &lt;your_app_folder&gt;     cd &lt;your_app_folder&gt;     ls -al     rm -rf .git     ls -al     git init     echo \"# &lt;your_app_folder&gt;\" &gt; README.md     git add .     git commit -m 'initial commit'   After the steps above from a terminal, you have achieved these steps:     Cloned a template   Created your own repo   Add a new git control to this repo   Now it’s time to inject your wonderful shiny app to this template. The goal is to put all your app code base to template’s subfolder ‘app’.   Using the GitHub Desktop Application   To keep track of file changes within a local repo, first add this folder to GitHub Desktop Application:         You can create a remote repo by clicking on the ‘Publish repository’ and select the correct branch (in this case ‘master’ is the branch name). You can specify if you want this repo to be private or not. Also, please make sure this repo is under FredHutch as the organization.      Test Your Application Locally  Open the app in an R console.         Change directory to subfolder ‘app’ under your app root:       cd &lt;your_app_folder&gt;   cd app   R           Via R console,       source('start.r')           Then go to a browser to check the url: http://localhost:7777   Insert Your App into the Template  Use your favorite code editor to add your own shiny app content to this template.  Here are a few reminders for the shiny apps with a single R script:      Please split your app.R or ui.R/server.R in the app directory in the local cloned repository   If you are using the ui.R/server.R structure, make sure you include ‘library(shiny)’ to both ui.R and server.R files, and in server.R, please remove ‘shinyApp(ui, server)’   Make sure all files (data, etc) needed by the app are also in the app directory or a subdirectory thereof   Push Edits to GitHub  After you added your own content to this repo, you are ready to commit the changes and push the changes to the remote repo.      Now your GitHub Desktop Application console looks like this:         Two files with green crosses on the right have been added as new files.  Two existing files have been modified with yellow dots on the right. You can further examine and adjust those changes by click the yellow dot or right click:         Write a commit message and click on ‘commit to master’ if you want to commit change to your master branch:         Push committed change from local files to remote repo      Viewing Your App  You should be able to find your app within the repo you created.        Deployment Service from SciComp  Hopefully, you can see your app working as expected in the browser. Please keep a good record of all your dependencies. You may have all the necessary R packages installed on your local machine but SciComp deployment platform needs to know those dependencies.  You also want to be ready to answer all the questions below before you contact Scientific Computing team.  If you are ready with your codebase  and have the answers for the below information required by deployment, then go to https://getshiny.fredhutch.org to submit the application deployment request. This request website is only accessible from Campus computers through wired connection or the Marconi wireless network.           Is your application codebase version-controlled by Git and in a GitHub repository (repo) ?  If so, is it under FredHutch account?            How do you want to name (DNS name) your application? So the it will look something like .fredhutch.org            Is your application facing Fred Hutch internally (open to the Fred Hutch campus) or externally (open to the entire internet)?            Does your application require authorization (password protection), that only your collaborators have access?            Does your application contain PHI? Please remove PHI from your application?            Please list out all the dependencies (R packages you used).       More Information about developing and using Shiny Apps     Introduction to Shiny with a cheatsheet and some example apps   How to get started with Shiny apps   Which GitHub protocol should I use?   Tutorial   Shiny cheatsheet   Shiny articles   Show me shiny   Shinyapps.io   Gallery   Shiny Google User Group  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/shiny/",
        "teaser":null},{
        "title": "Snapshots",
        "excerpt":"How to use snapshots for self-service file recovery or undelete  The file system underlying home directories and most shared folders on Linux systems has a feature called snapshots.   Snapshots maintain a point-in-time copy of your files. These are not actually copies, but perserved blocks and are automatically aged.   It can be confusing to use, so let’s see an example:   $ pwd /home/username $ ls deleted_dir ls: cannot access deleted_dir: No such file or directory $ cp -avr .snapshot/daily.2018_12_09_0010/deleted_dir .   From every directory (/home/username in this example) there is a virtual subdirectory called .snapshot. This directory holds a series of timestamped directories that show the contents of the parent directory at the stamped time.    /home/username/job_outputs/2018_12_09/.snapshot/timestamp      has the contents of      /home/username/job_outputs/2018_12_09      at whatever timestamp you use   In this example, we are recursively (-r), verbosely (-v) copying the deleted_dir folder and all contents back to /home/username in archive (-a) mode that preserves the metadata like ownership, timestamps, and permissions.   To see all the available timestamps, run ls .snapshot from any directory.   When to use snapshots  If you accidentally delete files or directories in your home directory, use snapshots to recover your deleted files at any time.   In the case of shared folders, it is possible you will lack the permissions to restore some files or folders. Typically the owner of the file or directory needs to do the restore copy.   In all cases the .snapshot virtual folder is read-only so you cannot alter the files there, even accidentally.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/snapshots/",
        "teaser":null},{
        "title": "Advanced SSH config including X11, ssh gateway (snail), and MacOS keychain",
        "excerpt":"This page contains several configs for advanced (and more convenient) SSH use.   Background  We use 2 common protocols to access remote compute resources. The first is the Hypertext Transfer Protocol (HTTP). This is the way your browser communciates with remote web servers. The second common protocol is Secure SHell (SSH). This is the method used to run a program on a remote server. Typically the program run is a shell that gives you an interactive command line on the remote host.   Many programs you will want to run on a remote compute servers are command line programs. These are easily executed from your shell remote process. You use this method when opening a terminal (shell) on a compute server (like rhino) and typeing the name of the program or script.   If the remote program has a Graphical User Interface (GUI) then you use a Windows-like system called X (or X11 or XOrg or officially The X Window System). The X Window System is backward from the way you may think it works - what you run on your laptop or desktop is actually the X server, and the remote program you want to display on your device is the client. All SSH clients support tunnelling the X traffic, so usually you will use SSH to start the remote GUI program, which is then displayed on your device.   This is a list of clients and servers by OS:                  OS       SSH client and command       X server       Notes                       MacOS       OpenSSH ssh       XQuartz       OpenSSH is already installed on MacOS                 Linux       OpenSSH ssh       Xorg       Both likely already installed on most systems                 Windows       puTTY puTTY       Windows Subsystem for Linux       Must be installed - ask admin           SSH Connections  SSH uses the TCP network protocol to make connections between systems. TCP is a persistent, stateful connection that is resilient to network interruptions. When you use SSH to run a program on a remote system (even a shell) the connection you make is tied to an SSH process on the remote machine, and all programs are children of that SSH process. If that process stops, all children stop as well. If the network connection does drop between your client and the server, your SSH process stops, and all your running programs stop.   There are several ways around this:      NoMachine - you can use the NoMachine service   GNU Screen or tmux - these utilities run a separate process on the remote host that is not tied to your SSH connection   Cluster jobs - you can run your programs using our slurm cluster, which queues and executes programs independently of your SSH connection   GNU Screen and tmux are both terminal multiplexers. They run one or more shell processes on a remote system and enable you to disconnect and reconnect while leaving the process(es) running. They are both full-featured utilities that take a long time to learn. Using the command below, you can get working with tmux quickly.      Use SSH to connect to a remote system: ssh rhino   On Fred Hutch systems, load the tmux environment module: module load tmux or just ml tmux   Attach to your running tmux session: tmux attach or just tmux a   If you do not have a running session, start one: tmux   When in a session, typing exit or ctrl-d will exit the tmux session   Closing the lid of your laptop and/or exiting your local ssh client will leave the tmux session running   Explicitly disconnect with ctrl-b d (ctrl-b puts tmux into command mode, and the disconnect command is d)   A note about tmux iterm2 integration. If you are using iterm2 on a Mac, add -CC to your tmux command. This will run a tmux console as well as tmux in that terminal. From there, you can use iterm2 commands to create a new tmux sessions in a new window (command-n) or a new tab (command-t). When you disconnect, simply reattach to that first tmux session, and iterm2 will re-open all your windows and/or tabs.   A note about the X Window System and terminal multiplexers. As stated above, on your laptop you are running an X server, and the remote program is a client. Terminal multiplexing allow your laptop to disconnect because the multiplexing server is running on the remote system and your device is the client. When you close your laptop, the X server stops. Clients exit at that point as a client cannot do anything without a server. See NoMachine for a workaround for X programs.   The SSH config file  Located in your home directory in the .ssh folder is a file called config (create it if it doesn’t exist). Your SSH client will read configuration options from this file when you use it. Any of the command line options can be specified in this config file to avoid overly complex SSH commands. This is what this page is all about.   Forward X11  You must start from a client that is running an X11 server. These include any Linux system including NoMachine, XQuartz on MacOS, and Xming or Cygwin on Windows.                  Config       Command Line       Value       Notes                       ForwardX11       -X       yes/no       This will instruct your ssh client to set up a tunnel between the X11 server on your client device and a port on the remote machine. This port is specified by the DISPLAY environment variable.                 ForwardTrustedX11       -Y       yes/no       Trusted X11 connections bypass X11 security features. The traffic is all still encrypted, but root on the remote machine may be able to access your client device. Some SSH implementations do not support X11 security extensions, so you must specify -Y or ForwardTrustedX11 to bypass them.           Check on the remote system that you have a DISPLAY environment variable set using echo $DISPLAY and test X11 itself by running a simple X11 program like xeyes.   Identity  Often the local user on your client device is not the same as your username on the remote system. You can always ssh joeuser@remotehost.com but you can also specify that in the config file.                  Config       Command Line       Value       Notes                       User       @       Your remote system username       Make ssh commands shorter and easier.                 Identity       -i              SSH supports multiple identities and multiple keys. Use this option to specify a different SSH key from the default ~/.ssh/id_rsa.           Keys  If you use your password to ssh, you can certainly continue to do so. However, with a key you will have to type your password less frequently, or sometimes never. Many remote systems do not allow password authentication and require keys.   Create an SSH key - on MacOS or Linux, run ssh-keygen and follow the prompts (change the file location if you are creating a new key but want to keep the old one). On Windows you will need puttygen if using puTTY or copy the Linux way if using Cygwin.   You will be prompted to enter and then confirm a passphrase. This is the best protection of your ssh key, and should be a longer complex passphrase. See below to automate the use of it, and try to resist creating passphrase-less keys.   This command will create two files in ~/.ssh - id_rsa and id_rsa.pub. The first is your private key, and the second is the public key.   MacOS Keychain  No one wants to type a long passphrase every time they use SSH. Creating an SSH key with no passphrase is unsafe. MacOS keychain to the rescue.   The following steps will get your MacOS device into a state where your SSH key passphrase is unlocked for you when you log into the Mac, and is automatically supplied to SSH. You will need to be logged in to your Mac, have a terminal open, and have an SSH key to start.      Add your ‘key’ to the keychain - run ssh-add -K &lt;path to keyfile&gt; the default is ~/.ssh/id_rsa. This will prompt for your passphrase, and then store it in your login keychain.   Tell ssh to use it - add the following to your ssh config:                  Config       Command Line       Value       Notes                       UseKeychain       -o UseKeychain       yes/no       So not all config options have easy command line options. This tells your ssh client to use your keychain for the passphrase.                 AddKeysToAgent       -o AddKeysToAgent       yes/no       Starts and ssh-agent process that will hold your unlocked key safely in memory for subsequent ssh session. Shutdown or reboot and this disappears.           Good key management  Your SSH key is actually in two parts - the public key, and the private key. Public/Private key pairs are fascinating if you are in to encryption. Be sure to search and read all about them.   Public Key  Your SSH public key is not secret. It can and should be distributed. Typically your public key is added to your ~/.ssh/authorized_keys file (just add it to the end). Also, if there is a public key file in ~/.ssh on the remote system, that will work. You can email this, paste it, whatever. The trust is on the other end - any remote system that accepts your public key in agreeing that it identifies you.   Private Key  Protect this. This is how you prove you own the public key. It is best to not put this anywhere other than your client device, and perhaps a backup if you like, though no lasting encryption will be done with it, so losing it will not destroy any data.   Best Practices  The best practices involving keys include:      limit copies of your private key (there should be only one)   do not enter a blank passphrase   add your public key (cut-and-paste is fine) to ~/.ssh/authroized_keys file   you do not need to put both of your keys on every machine you want to use, only your client device   Agents  SSH supports an in-memory agent process that will hold your unlocked private key safely. This alleviates the need to type your passphrase for the duration of time the agent is running. See above for MacOS options. On Linux, you can run eval $(ssh-agent) to start an agent, and ssh-add [path to keyfile if not default] to add your key to the running agent process.   You can forward key requests from remote machine back to your running agent:                  Config       Command Line       Value       Notes                       ForwardAgent       -o ForwardAgent       yes/no       Forward agent requests back to the agent running on your client device.           Proxies  Fred Hutch supports use of a VPN to remotely connect with our network. It also supports an SSH gateway. This is allowed as all SSH communication is encrypted, and the gateway system is audited. The gateway is called snail.fhcrc.org.   SSH supports using a proxy or “jump host” and can be configured to do so automatically.                  Config       Command Line       Value       Notes                       ProxyJump       -J       hostname       SSH will use the specified host as a middle-point before completing the connection to your desired host.           All together now  Your SSH config supports multiple ‘stanzas’ to help you as your config may not be the same for remote hosts. The ‘stanza’ keyword is Host. In the example below, everything from one Host line to the next Host line is applied the specified host.  Host *.fhcrc.org !snail.fhcrc.org \tProxyJump snail.fhcrc.org  Host *.fhcrc.org \tUseKeychain yes \tAddKeysToAgent yes \tIdentityFile ~/.ssh/id_rsa \tForwardX11 yes \tForwardX11Trusted yes \tForwardAgent yes    \tUser bmcgough  This config contains all of the features mentioned above and will apply them to all fhcrc.org hosts. It will use snail (the SSH gateway host) for all hosts except snail itself, as this would create a loop. Obviously you would replace your identity file path and username if needed.   One final note - SSH does not do DNS resolution before consulting the config file, so typing ssh rhino will not trigger the *fhcrc.org Host entries. You can add rhino to the Host line to have this trigger the config as well.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/ssh/",
        "teaser":null},{
        "title": "Synology",
        "excerpt":"Synology storage appliances (cloud sync)   Some labs use Synology Storage appliances (e.g. to store data from instruments). Synology can backed up to Economy File.   Installing Synology Cloud Sync      Open the Synology NAS web interface and login as an Administrator   Open Package Center   Install Cloud Sync   Configuring Synology Cloud Sync  ​      Open “Cloud Sync” and click the + to add a new connection   Choose OpenStack Swift as the Provider   Enter in the values for your connection to Economy  File: Identity Service Endpoint:  https://tin.fhcrc.org/auth/1.0 Identity Service Version: 1.0 Username:  API Key:  (not the key) Create a new container named \"synology_backup\" (or similar desciptive name --- this is the equivalent of a folder)   Setup a syncing task:      Name the connection   Choose the local path (on the Synology Appliance)   Choose the remote path in Economy file (do not select the root folder as it will sync the entire economy file account preventing you from adding new tasks)   Select the sync direction: Choose “Upload local changes only”, this is a backup   Make sure encryption is unchecked. You don’t need encryption because the data is already encrypted in the storage system which is local in a hutch data center and not in the cloud. If you decide to use encryption, make sure you do not lose the key or your data will be lost forever as IT does not have any backdoor to get your data back if you use this type of encryption. ​- Confirm your settings, click Apply and OK on congratulations message   Syncing will start  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/synology/",
        "teaser":null},{
        "title": "Running Tensorflow (and Keras) on GPUs",
        "excerpt":"In order to run tensorflow on GPUs you need to use a special version of Tensorflow. GPUs are currently installed on the Koshu cluster (Google cloud). In early 2019 GPUs will also be available on the new GizmoJ class nodes. For now please follow the instructions for getting a GPU based machines on Koshu:   GPU Tensorflow in a Python Environment   GPU Tensorflow with the standard Python   load a current version of Python 3.6 on Rhino using the ml shortcut for module load and then use pip3 to install the Tensorflow wheel made for Koshu and Gizmo       ~$ ml Python/3.6.7-foss-2016b-fh2     ~$ pip3 install --user --upgrade /app/src/tensorflow/14.04/python3.6/cuda10/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl   then create a small python test script:       echo \"#! /usr/bin/env python3\" &gt; ~/tf-test.py     echo \"import tensorflow\" &gt;&gt; ~/tf-test.py     echo \"from tensorflow.python.client import device_lib\" &gt;&gt; ~/tf-test.py     echo \"print(tensorflow.__version__)\" &gt;&gt; ~/tf-test.py     echo \"print(tensorflow.__path__)\" &gt;&gt; ~/tf-test.py     echo \"print(device_lib.list_local_devices())\" &gt;&gt; ~/tf-test.py     chmod +x ~/tf-test.py   and run it on Koshu (for example)       ~$ sbatch -M koshu -p gpu --gres=gpu:V100-SXM2-16GB:1 -o out.txt ~/tf-test.py     Submitted batch job 27074536 on cluster koshu     ~$ tail -f out.txt     1.12.0     ['/home/petersen/.local/lib/python3.6/site-packages/tensorflow', '...']     ...     physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"   if you want to switch back to the non-GPU version of Tensorflow just uninstall the GPU version you installed under .local       ~$ pip3 uninstall tensorflow     Uninstalling tensorflow-1.12.0:     Would remove:         /home/petersen/.local/bin/freeze_graph         /home/petersen/.local/bin/saved_model_cli         /home/petersen/.local/bin/tensorboard         /home/petersen/.local/bin/tflite_convert         /home/petersen/.local/bin/toco         /home/petersen/.local/bin/toco_from_protos         /home/petersen/.local/lib/python3.6/site-packages/tensorflow-1.12.0.dist-info/*         /home/petersen/.local/lib/python3.6/site-packages/tensorflow/*     Proceed (y/n)? y         Successfully uninstalled tensorflow-1.12.0   GPU Tensorflow in a virtual environment   Python virtual environments are useful for advanced users who would like to work with multiple versions of python packages. It is important to understand that the virtual env is tied to the Python environment you have previously loaded using the ml command. Let’s load a recent Python and create a virtual environment called mypy       ~$ ml Python/3.6.7-foss-2016b-fh2     ~$ python3 -m venv mypy     ~$ source ./mypy/bin/activate     (mypy) petersen@rhino3:~$ which pip3     /home/petersen/mypy/bin/pip3   Now that you have our own environment you can install packages with pip3. Leave out the –user option in this case because you want to install the package under the virtual environment and not under ~/.local       (mypy) petersen@rhino3:~$ pip3 install --upgrade /app/src/tensorflow/14.04/python3.6/cuda10/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl   Now you can just continue with the example from GPU Tensorflow with the standard Python. After you are done with your virtual environment you can just run the deactivate script. No need to uninstall the tensorflow package:       (mypy) petersen@rhino3:~$ deactivate      ~$    GPU Tensorflow from a singularity container   To run in a Singularity container, you need to start with a Docker image containing a modern Python and the tensorflow-gpu package installed.  The Tensorflow Docker images are all set up and ready.   After that load Singularity:   ml Singularity      Note that there is a singularity module (note the case) that is out-of-date.  Loading Singularity (with caps) will give you the most modern release which is typically what you want.    After that, the only change is to enable NVIDIA support by adding the --nv flag to singularity exec:   singularity exec -nv docker://tensorflow/tensorflow:latest-gpu python ...   Sample code is available in the slurm-examples repository.   Tensorflow from R   Scientific Computing maintains custom builds of R and Python. Python modules with fh suffixes have Tensorflow since version 3.6.1. Only Python3 releases have the Tensorflow package. To use Tensorflow from R, use the FredHutch Modules for R and Python. Example Setup       ml R     ml Python/3.6.7-foss-2016b-fh2      # Start R     R     # R commands     pyroot = Sys.getenv(\"EBROOTPYTHON\")     pypath &lt;- paste(pyroot, sep = \"/\", \"bin/python\")     Sys.setenv(TENSORFLOW_PYTHON=pypath)     library(tensorflow)     tf_version()     sess = tf$Session()     hello &lt;- tf$constant('Hello, TensorFlow!')     sess$run(hello)     # Sample output: b'Hello, TensorFlow!'   Troubleshooting   First verify that you have a GPU (Tesla V100 or Geforce) active as well as CUDA V 10.0 or newer       petersen@koshuk0:~$ nvidia-smi      Thu Dec 27 14:44:19 2018            +-----------------------------------------------------------------------------+     | NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |     |-------------------------------+----------------------+----------------------+     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |     | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |     |===============================+======================+======================|     |   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |     | N/A   39C    P0    34W / 300W |      0MiB / 16130MiB |      0%      Default |     +-------------------------------+----------------------+----------------------+   If you see any issues here you need to contact SciComp to have the error corrected   Also please email SciComp to request further assistance   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/tensorflow-gpu/",
        "teaser":null},{
        "title": "using Toolbox to get Hutch master data",
        "excerpt":"What is toolbox.fhcrc.org ?   Did you ever want to know to which PI or division a user rolls up to or check if someone is Hutch staff or affiliated or get all users in the same department? Then Toolbox may be for you. Toolbox is a system that provides Hutch master data to developers (currently focusing on user centric master data). Toolbox can only be accessed from inside the Hutch network or VPN and you may have already used it to get access to credentials for Cloud Computing / Economy File. However Toolbox offers much more. If you develop applications or need any kind of automation that requires information about the almost 5000 Hutch employees and non-employees (e.g affiliates) this is the tool for you. Toolbox is polling the central HR database daily so your scripts will always use current information.   getting data from Toolbox   Toolbox offers its data as simple json ( https://toolbox.fhcrc.org/json/ ) or csv files ( https://toolbox.fhcrc.org/csv/ ) and you can easily parse them using your favorite coding tool such as Python, R or the Linux shell. You can browse the json files using Chrome or Firefox. (The Microsoft browsers IE and Edge want to download json files instead).   what datasets are available ?   The exact file collection available is in flux but here is an extract:      faculty.json/csv - a list of all member track Hutch faculty   pi.json/csv - a combined list of all member track Hutch faculty and Hutch staff who have been contact PI on a NIH grant in the last 5 years. Members of this list are getting certain allocations of data storage and high performance computing covered by center funds. IDs for Faculty / PI ID’s are built from the PI’s last name and the first initial of the preferred first name (or nick name), e.g. doe_j for Jane Doe   users.json/csv - all active hutch employees and non-employees (affiliates). If a person has a “preferred name” (aka nick name) set in the HR system, the givenName column will always be set to this entry.   employees.json/csv - all Hutch employees (Faculty and Staff)   sc_users.json/csv - all users in departments that have had at least one scientific computing user in the past. Users in this list automatically get a posix home directory (formerly known as ‘Fred account’) and other scientific computing accounts (such as access to the Gizmo, Beagle and Koshu compute clusters)   pi2user.json/csv - get the user name (hutchnet id) of a PI when passing in the PI id (lastname_f aka doe_j)   user2pi.json/csv - get the PI id by passing in the user id of a person in the reporting line of the PI   user2sup.json.csv - get the immediate supervisor by passing in the user id   pi_groups.json/csv - get users who are member of a security group that governs access to a PIs data in center storage systems (e.g. Fast &amp; Economy File)   pi_groupmember.json/csv - get the security groups a specific user is member of. The list of security groups is limited to the ones that govern access to Fast File and Economy local and cloud (S3) storage.   Writing code that uses Toolbox (examples)   There are 2 types of JSON files. Files that have the number “2” in their names are simple mappings or key / value pairs. For example user2sup.json allows you to get the the username of the direct supervisor of a user. Other files are larger tables. For example  users.json contains all users (faculty, staff and affiliates) with details such as department, job titles and division. (Note: Human Biology has the ID ‘CB’ which is an artifact of the past and Shared Resources has its own division ID although it technically rolls up under Research Administration). These json databases are presented as lists of dictionaries in Python. The field or column naming convention are mostly based on LDAP RFC 2798, e.g. givenName, displayName, sn (for surname)   Python Examples   To parse JSON files from a web server we need to import the requests and json packages. To process csv files we use the pandas package which is the standard toolset for python based data scientists. Files with “2” in their name are converted to simple Python dictionaries, e.g. {'key1': 'valueA', 'key2': 'valueB'}. The other JSON files are converted to a list of dictionaries were each dictionary represents a row. [{'key1':'A','key2':'B'},{'key1':'C','key2':'D'}]. In the csv files this would be a row in the table. To start with a simple example let’s say we want to find the supervisor of Fred Appelbaum, the Center’s deputy Director. We pull the file user2sup.json into a json object and then a Python dictionary. The correct answer should be Gary Gilliland’s user id and voila:       &gt;&gt;&gt; import requests, json     &gt;&gt;&gt; URL = 'https://toolbox.fhcrc.org/json/user2sup.json'     &gt;&gt;&gt; d = requests.get(URL).json()     &gt;&gt;&gt; print(d['fappelba'])     gary   then we would like to know to which PI user markg (Mark Groudine) rolls up to       &gt;&gt;&gt; URL = 'https://toolbox.fhcrc.org/json/user2pi.json'     &gt;&gt;&gt; d = requests.get(URL).json()     &gt;&gt;&gt; print(d['markg'])     groudine_m   and the answer is of course groudine_m. When we query other staff who reports indirectly or directly up to Mark Groudine the result would also be ‘groudine_m’. If we query the username of non-scientific staff there is no PI to roll up to and user2pi will return an empty string.       &gt;&gt;&gt; URL = 'https://toolbox.fhcrc.org/json/user2pi.json'     &gt;&gt;&gt; d = requests.get(URL).json()     &gt;&gt;&gt; print(d['petersen'])   Now let’s look for a slightly more complicated query. We are looking for staff in the clinical research division who have the word ‘Engineer’ in their job title and in the search result we would like to display 2 columns: department and title.       &gt;&gt;&gt; URL = 'https://toolbox.fhcrc.org/json/users.json'     &gt;&gt;&gt; users = requests.get(URL).json()     &gt;&gt;&gt; for user in users:     &gt;&gt;&gt;     if user['division'] == 'CR' and 'Engineer' in user['title']:     &gt;&gt;&gt;         print(user['department'], user['title'])      Clinical Research Data Systems Application Suppt Engineer     ...     Clinical Research Data Systems Software Dev Engineer   If you prefer csv over json you can do the same thing using the csv package, you just need to write 2 lines more code       &gt;&gt;&gt; import requests, csv     &gt;&gt;&gt; URL = 'https://toolbox.fhcrc.org/csv/users.csv'     &gt;&gt;&gt; r = requests.get(URL)     &gt;&gt;&gt; reader = csv.DictReader(r.iter_lines(decode_unicode=True))     &gt;&gt;&gt; for row in reader:     &gt;&gt;&gt;     if row['division'] == 'CR' and 'Engineer' in row['title']:     &gt;&gt;&gt;         print(row['department'], row['title'])      Clinical Research Data Systems Application Suppt Engineer     ...     Clinical Research Data Systems Software Dev Engineer   Most data scientists will use the power of pandas to query csv files. If you think that iterating over a list is not too elegant this may be the right option for you.       &gt;&gt;&gt; import pandas     &gt;&gt;&gt; URL = 'https://toolbox.fhcrc.org/csv/users.csv'     &gt;&gt;&gt; df = pandas.read_csv(URL)     &gt;&gt;&gt; crdeng = df.loc[(df['division'] == 'CR') &amp; (df['title'].str.contains('Engineer')), ['department', 'title']]     &gt;&gt;&gt; print(crdeng)                             department                       title     237   Clinical Research Data Systems  Application Suppt Engineer     ...     1882  Clinical Research Data Systems       Software Dev Engineer   filtering json files using the sci package   Querying json files can be a bit unwieldy for more complex queries. The sci python package offers a few wrapper and shortcut functions for scientists and programmers. The sci package is already installed on Rhino if you load the latest Python through the ml command. Otherwise you can just install the sci package in your home directory using the command pip3 install --user --upgrade sci. The sci package only supports Python3 and the next examples require a sci package &gt;= 0.1.0)       import requests, json, sci     URL = 'https://toolbox.fhcrc.org/json/users.json'     users = requests.get(URL).json()      # get a list of all employeeIDs     uids = sci.json.jget(users, 'employeeID'))      # get a list of employeeIDs of all Administrative Assistants     uids = sci.json.jsearch(users,'title','Administrative Assistant','employeeID')      # get the job title of the 10th user in the list      mytitle = sci.json.jsearchone(users,\"employeeID\",uids[9],\"title\")      # Search and return an entire row of the first match     user = sci.json.jgetonerow(users,'title','President &amp; Director'):   In another example we would like to send an email notification to users outside the Admin division who regularly write code to inform them of new features. We also want to CC their supervisors to increase awareness. Coders may have a range a job titles we need to consider but there is no chance this will be comprehensive list. The output list of email addresses is separated by semicolon so you can paste it directly into the to, cc or bcc fields of Outlook. (Please note: This example requires sci package &gt;= 0.1.3 which has the sci.fh.getToolbox function)       #! /usr/bin/env python3     import sci     titles = [             'Data Engineer',             'Software Dev Eng',             'Bioinformatics Analyst',             'Programmer',             'Developer'             ]     users = sci.fh.getToolbox('users.json')     coders = []     supervisors = []     for u in users:         for t in titles:             if t in u['title'] and u['division'] != 'AD':                 if u['mail'] not in coders:                     coders.append(u['mail'])                     email = sci.json.jsearchone(users,'mgrID',u['mgrID'],'mail')                     if email and email not in supervisors:                         supervisors.append(email)      print('\\n### Coders ####')     print('; '.join(coders))     print('\\n### Supervisors ####')     print('; '.join(supervisors))   don’t forget velocity checks   If you are adding and deleting users based on active users in Toolbox you will sooner or later run into the situation that something fails and toolbox  returns a user list that is empty. Now your script may think that all users have left the center and will try to delete all existing users in the system you manage. To prevent this we are building a velocity check. A velocity check is a routine that checks if a certain action makes sense. For example it will be extremely unlikely that 100 users or more will leave the Hutch at any given day so we will just cancel all actions that involve more than 100 users.      CODE EXAMPLE TBD   R examples   R also offers a few nice options: use library jsonlite to process JSON files. For importing a csv file into a tibble (a dataframe like object with a few convenience features that was inspired by the pandas dataframe) use readr to import and then dplyr to filter it with a sql like syntax.   First we would like to know to which PI user markg (Mark Groudine) rolls up to       &gt; library(jsonlite)     &gt; URL = 'https://toolbox.fhcrc.org/json/user2pi.json'     &gt; user2pi = fromJSON(URL)     &gt; print(user2pi['markg'])     $markg     [1] \"groudine_m\"   and no surprise: he rolls up to PI id groudine_m   Then we try processing a csv file and show the filtering capabilities of dplyr. We would like to find the first name of all employees who are among the first 50 employees of the Hutch since it was founded. Hutch employee IDs start with 10000       &gt; library(readr, dplyr)     &gt; URL = 'https://toolbox.fhcrc.org/csv/employees.csv'     &gt; tib = read_csv(URL)     &gt; newtib = tib %&gt;% select(employeeID, givenName) %&gt;% filter(employeeID &lt; 10050)     &gt; print(newtib)     # A tibble: 4 x 2     employeeID givenName         &lt;int&gt; &lt;chr&gt;         1      10002 Brenda        2      10018 Stan          3      10035 Patricia      4      10041 Molly      Linux Shell (bash and jq)   Through the linux Shell (e.g. on Rhino) you have access to many tools such as such as curl and jq, the command-line JSON processor, that can help processing information quickly.   Consider the unlikely scenario that you need to collaborate with every PI and would like to create a folder for each PI just as you see it in Fast File (/fh/fast):       URL=https://toolbox.fhcrc.org/json/pi.json     rootdir=/fh/fast/doe_j/collab     for PI in $(curl -s ${URL} | jq -r '.[] | .pi_dept'); do         if ! [[ -d ${rootdir}/${PI} ]]; then             mkdir -p ${rootdir}/${PI}         fi     done   or check to which PI user markg (Mark Groudine) rolls up to       curl -s https://toolbox.fhcrc.org/json/user2pi.json | jq -r .markg   or you would like to retrieve the email addresses of all center employees:       curl -s https://toolbox.fhcrc.org/json/employees.json | jq -r '.[] | .mail'  or how many unique job titles are there?       curl -s https://toolbox.fhcrc.org/json/employees.json | jq -r '.[] | .title' | sort | uniq | wc -l   Information for System Administrators   As of early 2019 Toolbox lives on VM fox.fhcrc.org. Most scripts are symlinks under /root/bin that point to a git clone of the FredHutch/IT repository       root@fox:~# readlink ~/bin/sc_dump_groups_json.py /root/github/IT/general/newusernotify/sc_dump_groups_json.py   several cron jobs are triggered, groups are updated hourly       root@fox:~# cat /etc/cron.d/sc_dump_groups_json      10 * * * * root /root/bin/sc_dump_groups_json.py --users &gt; /var/www/toolbox/json/pi_groupmember.json      11 * * * * root /root/bin/sc_dump_groups_json.py  &gt; /var/www/toolbox/json/pi_groups.json     12 * * * * root /root/bin/sc_dump_groups_json.py --gid   Other data - what’s missing ?   If you have a proposal for improvements either in the text or in the code examples please send a pull request. If you have examples in other Programming languages that are used at Fred Hutch (Java, C#, Powershell) please also send a pull request. If you would like see other Fred Hutch master data via Toolbox or if you believe that any of the data is incorrect please contact Dirk Petersen.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/toolbox/",
        "teaser":null},{
        "title": "Using Markdown with Visual Studio Code",
        "excerpt":"Markdown is a formatting language used when the resulting document will be rendered or converted into html documents for web-viewable material.  The format is commonly used to generate documents like readme’s in GitHub or blog posts and even this website.  Markdown documents are written in plain text, and thus are often simpler to create, edit and manage using text editors intended for writing code.  This page includes a how-to discussion of using VSCode, a software package (and associated customizing plugins) intended for writing code that lends itself well to markdowns as well.   Markdown References  There are multiple “flavors” of markdown, for example, this site is rendered using GitHub-flavored markdown. More detailed instructions for the various types of formatting possible using GitHub flavored markdown can be found here.  A handy Markdown Cheatsheet is also available here, and can serve as a useful refresher for the formatting required in your markdown in order to use automatic formatting features such as headings, table of contents, tables, links, image embedding and comments.   Visual Studio Code for Working with Markdowns     VSCode is freely available here and is supported on all major operating systems (Windows, macOS &amp; Linux). Download the correct version for your platform and install it.  Note, on a Mac you may need to drag the application icon from wherever you unpacked the zip to your Applications folder.  This editor has a variety of features that are useful for editing markdowns including an option to toggle a preview pane to view the rendered markdown as you edit.  There are also user-developed plugins that can be installed when needed to tailor the editor to your specific needs.   Creating and Editing Markdown documents   To get started creating or editing markdown files, you should already have a folder set up where you will be working with your markdown files (e.g., a cloned GitHub repo like the one this site is generated from, a folder containing your documentation) on your workstation. From the VSCode “File” menu, select “Open Folder” or “Open” (on macOS) and navigate to the folder containing the files you wish to edit, or to the empty folder you will be creating new documents in. The image below shows and open folder with some markdown files to edit. If you open a folder then all available documents in the folder will show up in the left sidebar, allowing you to switch between documents by simply selecting the files.      Select an existing file or create a new file (File –&gt; New File) and save it with the .md extension. When you click on the file name it will open a tab and display the contents of the file.   To see a live side-by-side preview of the rendered markdown document to check the formatting and see any images, you can click on the split window with magnifying glass icon in the upper right of the open files tab. After clicking on this tab, a new pane will appear on the right and display the rendered markdown.      Note: While it’s possible to edit markdown files from random locations in your file system in a one-off manner without opening the folder in VSCode, opening the folder is required for the features of some plugins and linters to work correctly.   Git Integration   VSCode has built-in support for Git repositories (you still need Git installed on your workstation if it’s not already present).   If the folder you have open to edit files is a Git repo, any changes made to files that you’ve saved, the “Git” icon (1) in the left toolbar will have a blue circle indicating how many un-staged/un-committed files there are. To stage (git add) a file that you wish to commit, click on the Git icon, then the file that you want to stage. Click on the “+” icon (2) to stage the file. To commit the changes, enter a commit message in the message box (3) and then click the check mark icon (4).      To push your changes to a remote repository, pull in any new changes from the remote repository or other git functions, click on the “…” icon (5) to reveal the full Git menu.      Installing Plugins   After you have VSCode installed, you can start creating and editing markdown files right away, but there are a few VSCode plugins that will be very helpful. These plugins will allow you to easily paste images into your documents and check the spelling and find formatting errors.   To install a plugin, in VSCode, click on the square extensions icon in the upper left tool bar (1), then in the search box (2) type the name of the plugin you wish to install, find the plugin you are looking for in the results, click on it and then click on the green “Install” button. After it’s installed you’ll need to reload VSCode to use it.      Below are the recommended plugins for authoring Markdown:   Plugin: Paste Image   The “Paste Image” plugin is a huge time saver if your documentation will contain lots of images. Install the Paste Image plugin from “mushan”.   By default, Paste Image will create (if it doesn’t already exist) a single “images” folder at the root of your open directory structure for all images pasted. If you would like to keep your images folder at the same level of the directory structure as your markdown document (to keep them together), you can make the following configuration change. Go to “File” –&gt; “Preferences” –&gt; “Settings” (or “Code” –&gt; “Preferences” –&gt; “Settings” on macOS) and in the “User Settings” configuration section, add the following pasteImage entries between the curly braces like the following, then close the user settings tab and click save json.   {     \"pasteImage.path\": \"${projectRoot}/assets/${currentFileNameWithoutExt}/\",     \"pasteImage.basePath\": \"${projectRoot}/assets\",     \"pasteImage.forceUnixStyleSeparator\": true,     \"pasteImage.prefix\": \"/assets/\" }  In this example the sub-folder that will be created in the assets folder named after your file name (this just lets us avoid a single huge assets folder in the long run).  Note: if you are editing markdown that is destined for the Fred Hutch Wiki, you must use the above configuration. It is set in the workspace settings for vscode at /.vscode/settings.json which will override the user settings. However in other contexts, you may need to use another folder name like images in order for certain processes to function.   After the Paste Image plugin is installed and configured, you can simply copy your prepared image or screenshot, insert the cursor where you want to insert the image in the document and use Ctr+Alt+V on Windows or Linux and Command+Alt+V on macOS. This will automatically create the assets (or whatever you’ve named it) folder if doesn’t already exist and place your image there naming it with a date/timestamp. The inserted text in the editor will look like the following:   ![](assets/2018-06-01-12-34-51.png)   Between the empty square brackets, you can place “alt-text”, or a string/name describing the image (it won’t be displayed in your browser or the preview). Without this “alt-text” the Markdown linter (plugin) will provide a warning, but it’s O.K. to leave it empty if you like for most downstream rendering processes.   More information about this plugin is available here   NOTE: To get the Paste Image working on Linux, it may be required to install the “xclip” package  (sudo apt install xclip).   Plugin: Spellchecker   VSCode doesn’t include a built-in spell checker. To add a spell checker to VSCode, install the “Code Spell Checker” plugin from “Street Side Software.” After the plugin is installed (and VSCode reloaded) it should automatically start finding typos. There will be a green squiggly line under the suspect word. If the word is misspelled you can click on the word and a yellow light bulb icon will appear. Click on the light bulb icon and a menu should appear offering you suggestions. If the suspected work is in fact spelled correctly you can use the same context menu to add it to your local dictionary.      More information about this plugin is available here.   Plugin: MarkdownLint   A markdown “linter,” a program that tests for formatting problems in a structured plain text file (there are also yaml linters, json linters, etc), may also be useful to help you write correctly formatted markdown. Install the “markdownlint” plugin from “David Anson.” It works exactly like the Spellchecker plugin. Hovering your cursor over the green squiggly underline will reveal the warning from the linter; clicking on the light bulb will reveal a click you can click to learn more about what you are doing wrong.   More information about this plugin is available here.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/compdemos/vscode_markdown_howto/",
        "teaser":null},{
        "title": "Computing Credentials",
        "excerpt":"This section describes how to get access and credentials to computing systems on campus and in the cloud.   HutchNet ID   A HutchNet ID is the standard login name and password you receive when you start working at the Hutch or are an official affiliate. It is also called Network login or Active Directory credentials. You can use it to login to most resources at the Center (Desktop Computer, Employee Self Service, VPN, Webmail) as well to Scientific Computing systems such as Rhino (ssh rhino), which is the login system to large scale cluster computing resources like Gizmo, Beagle and Koshu.   If one of your collaborators requires access to the Fred Hutch network you can submit a non-employee action form. Non-employees is a generic administrative term for affiliates, students, contractors, etc.   Please see the Service Desk site on CenterNet for more information about HutchNet ID including password rotation, etc.   GitHub.com   The Fred Hutch GitHub organization offers free access to public and private git repositories to all Fred Hutch staff and collaborators. If you are a Fred Hutch employee working with source code and don’t have a github.com account yet, please create one and email scicomp: “Please add my GitHub user id xyz to organization github.com/FredHutch”. Once you are a member of the organization you can create repositories, teams and invite external collaborators to share and edit code.      Note: github.com/FredHutch is the only officially approved cloud based source code system at Fred Hutch. It has security features that are otherwise not available via other systems.    A GitHub account is different from other accounts. If you leave the Hutch you keep your GitHub account, however you will just be removed from the Fred Hutch organization on GitHub and your former colleagues can still add you as an external collaborator to their GitHub repositories.   We have an entry in the Bioinformatics Resource library that provides more information about git and GitHub in general and specifically here at the Fred Hutch.   Guidance for Managing Credentials and Passwords  One issue to note when using GitHub to do version control in your code is that it can be very straightforward to inadvertently push content to GitHub that includes things such as API tokens, usernames and passwords, or even your AWS credentials themselves.  Please take care to structure your code in such a way that these “secrets” or anything you perceive to be private information (see our Security page for more information about what this might be) are loaded from an external file or environment variables that themselves are not sent to GitHub!!   Amazon Web Services (AWS)   You can obtain Amazon Web Services (AWS) credentials to make use of the Center’s AWS account. By default this will give you access to your lab’s S3 bucket, but you can request permission to use other services such as AWS Batch.  AWS credentials are designated per user, so any Fred Hutch employee should obtain their own credentials.      Note: Beyond precautions taken to protect any other credentials listed here, take care to ensure AWS credentials are never shared with or disclosed to any other user, directly (e.g., by email) or indirectly (e.g., by including them in code and sharing the code/committing to GitHub).  If you need credentials for an external collaborator, or if you are having a permissions issue, please email scicomp to request support from Scientific Computing.    There are two ways to get AWS credentials. Which one to use depends on how you will use AWS, either via the command line on rhino/gizmo or via graphical programs on your local computer such as via Cyberduck or Mountain Duck.   Command Line (Rhino/Gizmo) Instructions   A working HutchNet ID is all you need to login to rhino (ssh rhino). Some users will see an error message that their home directory was not found. This can happen if you are in a newly created department or in one that is typically not working with SciComp resources. Please email scicomp to have your home directory created.   First, ssh to one of the rhino machines (or use NoMachine):   ssh rhino   Then run the awscreds command.   awscreds   This will prompt you for your HutchNet password, which will not echo to the screen when you type it in.  It will then write out your credentials to files, which programs that use AWS will look for. awscreds will report exactly what it’s doing and where it has written your credentials. awscreds includes some options that allow you to customize its behavior. You can see this options by typing the command   awscreds --help   One important option is the --force flag, which tells awscreds that it can overwrite your existing credentials. This may be needed if your credentials are changed, and can be invoked as follows:   awscreds --force   See more about accessing AWS S3 via the command line here for Python, here for R, and here for the AWS Command Line Interface.   GUI Instructions   Open a web browser and navigate to https://toolbox.fhcrc.org/sw2srv/aws/account. This page is only accessible within the Hutch network. When prompted, enter your HutchNet ID and password. Your browser will display your access key and secret key. You can use these with graphical applications such as Cyberduck or Mountain Duck. See the more about how to use Cyberduck or Mountain Duck to connect to AWS S3 here.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/access_credentials/",
        "teaser":null},{
        "title": "Computing Resource Access Methods",
        "excerpt":"   Note:  This page is still a work in progress.  If you have specific input on topics related to access methods, please email them to sciwiki.    Client Devices   Client devices are supported by Center IT (ADM, CRD, VIDD) as well as Division IT groups (BSD, HB, PHS).   Laptops and Desktops   Center IT supports 3 different operating systems:      Laptops:            Windows       MacOS           Desktops:            Windows       MacOS       Ubuntu Linux LTS           Please see Center IT’s page in CenterNet about Laptops and Desktops for more information about support.      Note: If you request a Linux Desktop, Center IT will recommend that you use NoMachine from a Windows or Mac system instead as this is the most appropriate choice for most users but not all. If it is required for your work, it can be supported.    Mobile Devices   Please see Center IT pages for more information on Mobile Device Services or contact Help Desk with questions or problems.   Access Methods (Terminals, X11, ssh)   There are multiple ways you can get access Scientific Computing resources which are all running on a supported version of Ubuntu Linux. The most simple form of access is using a secure shell terminal software such as ssh or putty. You may also need graphical output (GUI), for example to use tools like R Studio or advanced text editors. For this you will need X11 tools which can either be installed as add-ons to your ssh program or a dedicated tool such as NoMachine.  See this page for more information about ssh configurations.   Putty (Windows)   Putty is the most widely used secure shell software for Windows. You can install putty through “Software Center” and then simply connect to host name rhino   iTerm2 (Mac OSX)   The default Mac terminal is a good choice for connecting to and using SciComp Linux-based resources.  We also recommend using iTerm2 for your local terminal program on your Mac- it has a richer feature set that comes in handy as your computing needs grow.   xquartz X11 (Mac OSX)   The only X11 client for Mac is XQuartz. Install XQuartz before running any Linux X11 apps such as RStudio or Matlab. Go here, and download and install the latest version of XQuartz.   After installing XQuartz just start the app. The XQuartz icon will appear in the toolbar showing that it is running. XQuartz is now running an X11 client and that is the only interaction that you need to do with XQuartz. After XQuartz is running, open the Mac terminal. The latest version of XQuartz sets the DISPLAY environment with the default Mac terminal window. We do not recommend using the terminal that comes with XQuartz.   When connecting to a rhino with ssh always use the -X flag to forward your Xsession to your Mac.   ssh -X jfdey@rhino2   After connecting to a Rhino verify that your X11 client is working by typing xeyes.   Terminator (Linux)   Terminator is a convenient terminal emulator that can have multiple terminals in a single window. You can use Terminator on your Linux desktop. Windows and Mac users will need a X11 software to use SciComp resources as graphical desktop. We recommend NoMachine NX to connect to a Linux machine with Terminator.   After starting Terminator you can right click on the terminal window and either split it horizontally or vertically. This 1 min video shows you how it works:      NoMachine NX access (Multi-OS)  NoMachine NX is a remote desktop software for Linux servers that gives you full remote access to a graphical user interface from Windows or Mac clients. It is installed on the SciComp session Servers lynx, sphinx or manx which you use to access the rhino systems. To connect through the local Fred Hutch network or through a VPN connection please download and install the NoMachine Enterprise Client from NoMachine’s Site.  Windows users can also install an older version from “Software Center”.   Configuration is very quick and the process is described in this video:      One of the nicest features of NoMachine is that you can disconnect from the server’s desktop environment and later resume the connection and continue right where you left off with all terminal windows in the same place. NoMachine from Windows or Mac is often a better choice than installing a Linux desktop.   How can I get access from remote locations?   The Fred Hutch network is protected by a firewall and there are currently 2 options to get access to resources inside the network, using VPN or snail.fhcrc.org.   VPN   The Fred Hutch desktop VPN service is the default choice for all remote connections. Please see the VPN page on CenterNet for more details.   ssh to snail.fhcrc.org   Snail is a SSH gateway (also called bastion host or jump host) you can use to get remote access if you do not require the features that VPN provides. Using SSH can be easier for some users, for example if you have a network printer at home you cannot use it while connected to VPN. By default you login to snail.fhcrc.org first and then to rhino. However, if you add these 2 lines your ~/.ssh/config file you only have to type ssh once      Host rhino*.fhcrc.org     ProxyCommand ssh yourusername@snail.fhcrc.org exec nc %h %p 2&gt; /dev/nul  If you are outside the Fred Hutch network type ssh rhino.fhcrc.org to use the snail gateway and if you are inside type ssh rhino to bypass the gateway.   Please see this page to learn more about ProxyCommand.  See this page for more information about ssh configurations.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/access_methods/",
        "teaser":null},{
        "title": "Computing Access Overview",
        "excerpt":"   NOTE: This section is a work in progress. If you have suggestions for content you’d like to see or would like to contribute email sciwiki.    This section focuses on information describing what types of credentials are needed for various Center IT supported computing resources, as well as instructions for accessing those resources.   Credentials  An oveverview of what credentials you need for which resource, and how to get them.   Methods  Overviews of methods for accessing computing resources here at the Hutch.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/access_overview/",
        "teaser":null},{
        "title": "Cloud Computing at Fred Hutch",
        "excerpt":"The Hutch is just getting started with cloud computing.  Options like the beagle and koshu clusters, while built in the cloud, are very much a simple extension of existing infrastructure into cloud providers but does not fully or particularly efficiently utilize the real capabilities and advantages provided by cloud services.   If you are at all interested or curious about cloud solutions and would like to talk over options, Scientific Computing hosts a cloud-specific office hours every week.  Dates and details for SciComp office hours can be found in CenterNet.   AWS Batch Overview  The Amazon Web Service (AWS) is a “cloud” computing provider which sells access to computational resources on a minute-by-minute basis. The “cloud” is actually just a simple idea that you can buy access to computers, instead of buying the computers themselves. Anytime you have a bunch of computers sitting in a warehouse running code for a bunch of different users around the world, that’s the “cloud.”   Among the different products offered by AWS, the three most relevant to bioinformatics are:     Amazon Elastic Compute Cloud, or EC2: a service that provides access to cloud-based computers of various sizes that allow temporary use by researchers to run computing jobs that require larger processors (CPU’s) or more memory than is typically available on a land-based computer.  The computing resources available for a task depend on the choices made about the CPU’s or memory allocation in the specific EC2 instance (a virtual computing environment).   Amazon Simple Storage Service, or S3: a service that provides cloud-based data storage in the form of “buckets”, or a pool of data that can be accessed anywhere, anytime via the web by users with credentials allowing the access to that specific bucket. The size and particular security and credentials associated with individual S3 buckets are particularly well suited to scaling and flexibility with respect to access.   AWS Batch: a service which wraps around AWS EC2 resources such that researchers can more easily do computing processes with EC2 instances on data stored in S3.   Essential terms      Docker image: lightweight operating system / virtual machine see Docker documentation   CPU: Central processing unit, basically just a unit of computation. Your laptop probably has 4 CPUs, while large servers have dozens.   How it works   The basic idea behind AWS Batch is that it allows you to run a job, which consists of (1) a command, inside of a (2) Docker image on a machine, with a (3) specified amount of compute resources. For example, a trivial job that you could run would be:      A command to print some text (echo Hello World), on   A Docker image that’s able to run the command (ubuntu:16.04), with   A specified number of CPUs (e.g. 1) and memory (e.g. 1Gb)   Based on those parameters, Batch will automatically:      Find a machine with the required number of CPUs and memory   Download the Docker image   Run the command within the Docker image   Shut down the Docker container   Shut down the machine if it is no longer needed   Batch will often combine multiple jobs onto a single machine, if that is the most cost effective approach, with all jobs effectively isolated from each other within their own Docker image. The big idea here is that you can save money by only paying for compute resources at the exact time you need it, without worrying about how to pick the most cost effective combination of EC2 instances.   Analyzing and storing data   Coming from Fred Hutch, one of the biggest things to transition is how you access and store data. While we are very used to the shared filesystem on gizmo, you have none of that available on Batch. Instead, you must download your data from S3 before analyzing it, and then upload the results back to S3 when you are done. All data within the Docker image when the command is complete will be deleted – we refer to this type of storage as “ephemeral.”   It’s very easy to download and upload from S3, but it just means that you have to get used to keeping your data there in order to use AWS Batch effectively. As a benefit, it’s much cheaper to store data there compared to /fh/fast, and just as stable.   Using Docker images   In order to run your code, you must have it packaged up in a Docker image. While this may be slightly difficult at first, it has the added benefit that your analysis is highly reproducible. You never have to worry that some dependency may have changed, and which would change the results. It is also very easy to publish your results and cite the Docker image as the definitive record of all of the dependencies and software needed to run your analysis and generate the published results.   When to Use AWS batch  AWS Batch is an AWS service that uses Docker containers to build a batch computing system.  Batch is made up of a queueing system where jobs are defined and queued, and a computational resource made up of Docker containers to process those jobs.  Resources are provisioned when there are jobs to be processed and destroyed when the work is complete.  This results in a very efficient and cost-effective solution for some work.   Batch is useful if you have a fairly standard processing workflow or at least a step which is fairly consistent.  The classic example for Batch is image processing: converting a raw image to some other format.  Batch is capable of much more complicated analyses and pipelines.   As Batch is very much a cloud service, some familiar resources aren’t available when using this.  Our ubiquitous file systems (home directories, fast-file, scratch) are not available- data used in Batch is typically stored in S3 or some other web-available source.  There have been some recent changes which expand options for data storage which may make some workloads more accessible to Batch.   How do I use AWS Batch?   SciComp provides access to AWS Batch in two ways:      Via the AWS Command Line Interface (CLI).   Via programmatic interfaces such as Python’s boto3. The earlier version of this library (boto) is deprecated and should not be used.   Access to the AWS Management Console (the web/GUI interface), is not available to end users at the Center. However, there is a customized, read-only dashboard available which displays information about compute environments, queues, job definitions, and jobs. Please report any issues you discover with this dashboard.   Get AWS Credentials   You will need AWS credentials in order to use AWS Batch. You can get the credentials here.   Initially, these credentials only allow you to access your PI’s S3 bucket. To use the credentials with AWS Batch, you must request access to Batch.   Request access by emailing scicomp with the subject line Request Access to AWS Batch.  In your email, include the name of your PI.   SciComp will contact you when your access has been granted.   Note that you will not be able to create compute environments or job queues. If you need a custom compute environment, please contact SciComp.   Create and Deploy a Docker Image  See our detailed information in the Computing Resource Library here about creating and deploying Docker images, as well as running your own Docker Host.   Create a Job Definition   Job Definitions specify how jobs are to be run. Some of the attributes specified in a job definition include:      Which Docker image to use with the container in your job   How many vCPUs and how much memory to use with the container †   The command the container should run when it is started †   What (if any) environment variables should be passed to the container when it starts †   Any data volumes that should be used with the container (the compute environments provided by SciComp include 1TB of scratch space available at /scratch). (Note: the process of providing scratch space is going to change soon, check back for updated information).   What (if any) IAM role your job should use for AWS permissions. This is important if your job requires permission to access your PI’s S3 bucket.   † = these items can be overridden in individual job submissions.   Using secrets in jobs  More to come.  Using scratch space   “Scratch space” refers to extra disk space that your job may need in order to run. By default, not much disk space is available (but you have infinite space for input and output files in S3.   The provisioning of scratch space in AWS Batch turns out to be a very complicated topic. There is no officially supported way to get scratch space (though Amazon hopes to provide one in the future), and there are a number of unsupported ways, each with its own pros and cons.   If you need scratch space, contact SciComp and we can discuss which approach will best meet your needs.   But first, determine if you really need scratch space. Many simple jobs, where a single command is run on an input file to produce an output file, can be streamed, meaning S3 can serve as both the standard input and output of the command. Here’s an example that streams a file from S3 to the command mycmd, which in turn streams it back to S3:   aws s3 cp s3://mybucket/myinputfile - | mycmd | aws s3 cp --sse AES256 - s3://mybucket/outputfile  In the first aws command, the - means “copy the file to standard output”, and in the second, it means “copy standard input to S3”. mycmd knows how to operate upon its standard input.   By using streams in this way, we don’t require any extra disk space. Not all commands can work with streaming, specifically those which open files in random-access mode, allowing seeking to random parts of the file.   If a program does not open files in random-access mode, but does not explicitly accept input from STDIN, or writes more than one output file, it can still work with streaming input/output via the use of named pipes.   More and more bioinformatics programs can read and write directly from/to S3 buckets, so this should reduce the need for scratch space.   Submit your job   There are currently two ways to submit jobs:      via the AWS Command Line Interface (CLI): aws batch submit-job. Recommended for launching one or two jobs.   Using Python’s boto3 library. Recommended for launching larger numbers of jobs.   AWS Batch also supports array jobs, which are collections of related jobs. Each job in an array job has the exact same command line and parameters, but has a different value for the environment variable AWS_BATCH_JOB_ARRAY_INDEX. So you could, for example, have a script which uses that environment variable as an index into a list of files, to determine which file to download and process. Array jobs can be submitted by using either of the methods listed above.   We are looking into additional tools to orchestrate workflows and pipelines.   Which queue to use?   No matter how you submit your job, you need to choose a queue to submit to. At the present time, there are two:      mixed - This queue uses a compute environment (also called mixed) which uses many instance types from the C and M families. Each of the instance types used is one that the Center has high limits for in our account.   optimal - This queue uses a compute environment (also called optimal) which uses the instance type optimal, meaning Batch will choose from among the C, M, and R instance types. While the Center’s account has high limits for most C and M types, its limits for the R types are lower. Batch has no awareness of per-account instance limits, so it may try to place jobs on R instances which could result in longer time-to-result.   Submitting your job via the AWS CLI   The easiest way to submit a job is to generate a JSON skeleton which can (after editing) be passed to  aws batch submit-job. Generate it with this command:   aws batch submit-job --generate-cli-skeleton &gt; job.json   Now edit job.json, being sure to fill in the following fields:      jobName - a unique name for your job, which should include your HutchNet ID. . The first character must be alphanumeric, and up to 128 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed.   jobQueue - the name of the job queue to submit to (which  has the same name as the compute environment that will be used).  In most cases, you can use the mixed queue.   jobDefinition The name and version of the job definition to use.  This will be a string followed by a colon and version number, for  example: myJobDef:7. You can see all job definitions with  aws batch describe-job-definitions, optionally passing a --job-definitions parameter with the name of one (or more) job definitions. This will show you each version of the specified definition(s). You can also view job definitions in the dashboard.   If you are using fetch-and-run, do NOT edit the command field. If you are not using fetch-and-run you may want to edit this field to override the default command.   Set the environment field to pass environment variables to your jobs. This is particularly important when using fetch-and-run jobs; these require that several environment variables be set. Environment variables take the form of a list of key-value pairs with the values name and value, see the following example.   \"environment\": [   {     \"name\": \"FAVORITE_COLOR\",     \"value\": \"blue\"   },   {     \"name\": \"FAVORITE_MONTH\",     \"value\": \"December\"   } ]   Now, delete the following sections of the file, as we want to use the default values for them:      dependsOn - this job does not depend on any other jobs.   parameters - we will not be passing parameters to this job.   vcpus in the containerOverrides section.   memory in the containerOverrides section.   retryStrategy section.   With all these changes made, your job.json file will look something like this:   {     \"jobName\": \"jdoe-test-job\",     \"jobQueue\": \"mixed\",     \"jobDefinition\": \"myJobDef:7\",     \"containerOverrides\": {         \"command\": [             \"echo\",             \"hello world\"         ],         \"environment\": [             {                 \"name\": \"FAVORITE_COLOR\",                 \"value\": \"blue\"             },             {                 \"name\": \"FAVORITE_MONTH\",                 \"value\": \"December\"             }         ]     } }   Once your job.json file has been properly edited, you can submit your job as follows:   aws batch submit-job --cli-input-json file://job.json   This will return some JSON that includes the job ID. Be sure and save that as you will need it to track the progress of your job.   Submitting your job via boto3   Notes on using Python      We strongly encourage the use of Python 3. It has been the current version of the language since 2008. Python 2 will eventually no longer be supported.   We recommend using Virtual Environments, particularly pipenv, to keep the dependencies of your various projects isolated from each other.   Assuming pipenv  and python3 are installed, create a virtual environment as follows:   pipenv --python $(which python3) install boto3   Activate the virtual environment with this command:   pipenv shell   You can now install more Python packages using pipenv install. See the pipenv documentation for more information.   Submitting your job   Paste the following code into a file called submit_job.py:   #!/usr/bin/env python3 \"Submit a job to AWS Batch.\"  import boto3  batch = boto3.client('batch')  response = batch.submit_job(jobName='jdoe-test-job', # use your HutchNet ID instead of 'jdoe'                             jobQueue='mixed', # sufficient for most jobs                             jobDefinition='myJobDef:7', # use a real job definition                             containerOverrides={                                 \"command\": ['echo', 'hello', 'world'], # optionally override command                                 \"environment\": [ # optionally set environment variables                                     {\"name\": \"FAVORITE_COLOR\", \"value\": \"blue\"},                                     {\"name\": \"FAVORITE_MONTH\", \"value\": \"December\"}                                 ]                             })  print(\"Job ID is {}.\".format(response['jobId']))    Run it with   python3 submit_job.py   If you had dozens of jobs to submit, you could do it with a for loop in python (but consider using array jobs).   Monitor job progress   Once your job has been submitted and you have a job ID, you can use it to retrieve the job status.   In the web dashboard   Go to the jobs table in the dashboard. Paste your job ID or job name into the Search box. This will show the current status of your job. Click the job ID to see more details.   From the command line   The following command will give comprehensive information about your job, given a job ID:   aws batch describe-jobs --jobs 2c0c87f2-ee7e-4845-9fcb-d747d5559370  If you are just interested in the status of the job, you can pipe that command through jq (which you may have to install first) as follows:   aws batch describe-jobs --jobs  2c0c87f2-ee7e-4845-9fcb-d747d5559370 \\ | jq -r '.jobs[0].status'   This will give you the status (one of SUBMITTED, PENDING, RUNNABLE,   STARTING, RUNNING, FAILED, SUCCEEDED).   View Job Logs   Note that you can only view job logs once a job has reached the RUNNING state, or has completed (with the SUCCEEDED or FAILED state).   In the web dashboard   Go to the job table in the web dashboard. Paste your job’s ID into the Search box. Click on the job ID. Under Attempts, click on the View logs link.   On the command line   On Rhino or Gizmo   On the rhino machines or the gizmo cluster, there’s a quick command to get the job output. Be sure and use your actual job ID instead of the example one below:   get_batch_job_log 2c0c87f2-ee7e-4845-9fcb-d747d5559370   You can also pass a log stream ID (see below) instead of a job ID.   On other systems   If you are on another system without the get_batch_job_log script (such as your laptop), you can still monitor job logs, but you need to get the log stream ID first.   To get the log stream for a job, run this command:   aws batch describe-jobs --jobs 2c0c87f2-ee7e-4845-9fcb-d747d5559370   (Note that you can add additional job IDs (separated by a space) to get the status of multiple jobs.)   Once a job has reached the RUNNING state, there will be a logStreamName field that you can use to view the job’s output. To extract only the logStreamName, pipe the command through jq:   aws batch describe-jobs --jobs 2c0c87f2-ee7e-4845-9fcb-d747d5559370 \\ jq -r '.jobs[0].container.logStreamName'   Once you have the log stream name, you can view the logs:   aws logs get-log-events --log-group-name /aws/batch/job \\ --log-stream-name jobdef-name/default/522d32fc-5280-406c-ac38-f6413e716c86   This outputs other information (in JSON format) along with your log messages and can be difficult to read. To read it like an ordinary log file, pipe the command through jq:   aws logs get-log-events --log-group-name /aws/batch/job \\  --log-stream-name jobdef-name/default/522d32fc-5280-406c-ac38-f6413e716c86 \\ | jq -r '.events[]| .message'   NOTE: aws logs get-log-events will only retrieve 1MB worth of log entries at a time (up to 10,000 entries). If your job has created more than 1MB of output, read the documentation of the aws batch get-log-events command to learn about retrieving multiple batches of log output. (The get_batch_job_log script on rhino/gizmo automatically handles multiple batches of job output, using the equivalent command in boto3.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/cluster_cloudCompute/",
        "teaser":null},{
        "title": "Large Scale Compute Overview",
        "excerpt":"This section contains articles that describe a range of high performance computing resource options available to Fred Hutch researchers.   Linux Operating System  The operating system for remote (non-local desktop) computing is Linux.  This page provides background on what Linux is and how you can learn to use it.   Technologies  This page describes all the various technologies and specifications for them that are supported by Fred Hutch for large scale computing.   Scientific Software  A wide range of pre-built scientific software is available for use in large scale computing work.  This page describes how to find out what is available, how to use it and what to do if something you need is not yet available.   Job Management  When doing large scale computing tasks, one often shifts to the use of jobs to perform specific tasks.  This page provides some background on managing and interacting with your tasks.   Cloud Computing  Beyond on premise resources, cloud computing access is available for Fred Hutch researchers and this page provides some basics on how to get started if you are in need of cloud computing specifically.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/cluster_overview/",
        "teaser":null},{
        "title": "Scientific Software",
        "excerpt":"The Fred Hutch provides researchers on campus access to high performance computing using on-premise resources.  The various technologies provided are outlined on our Technologies page along with the basic information required for researchers to identify which FH resource might be best suited to their particular computing needs.   The Scientific Computing group supports additional software used in scientific research beyond those available on local workstations. A large number of pre-compiled packages are already available on our high performance computing (HPC) cluster and Linux systems. Individual user installation of packages and language modules is also supported.   Reasons to use scientific software maintained by SciComp include:     packages are often faster due to compiler optimizations   packages are reproducible in or outside Fred Hutch   rapid access to many software packages and package versions   Environment Modules   On the command line and in scripts, we use the Environment Module system to make software versions available in a modular and malleable way. Environment Modules provide modular access to one version of one or more software packages to help improve reproducibility. We use a system called EasyBuild to create modules for everyone to use - there are over a thousand modules already available. The implementation of Environment Modules we use is Lmod, and the commands you use to interact with Environment Modules are module or ml.   EasyBuild Life Sciences  The full list of available software can be found on the Easy Build site.   How to Use Environment Modules  As you will learn below, Environment Modules can be referred to in two ways - generic and specific. Often the generic method is fastest, and this is an acceptable way to load Environment Modules when using a shell interactively. When using the generic method, you refer simply to the software package name you want to load (ex: module load Python). This is fast, but circumvents one of the reproduciblity supporting features of Environment Modules.   The default version of Python loaded using the generic reference will change as the Python package versions are updated. When using the specific method, you specify the verison of the software package you want to load (ex: module load R/3.5.1-foss-2016b-fh1). When you specify the version of a module, you will always load exactly the same version of the software package regardless of what new or different versions might also be available. For scripts, we recommend always using a specific Environment Module reference to ensure both reproducibility of your processes as well as making sure your process continues to work over time.   Interactively  When you log in to any SciComp managed server, your terminal session has Lmod pre-loaded. Commonly used shell commands around Environment Modules include:                  Command       Action                       module avail       Output a list of available Environment Modules                 module avail &lt;pattern&gt;       Output a filtered list of modules based on pattern (ex: module avail SAMtools)                 module load &lt;packagename&gt;/&lt;version&gt;       Load a specific version of a module into your environment (ex: module load Python/3.6.5-foss-2016b-fh1) - you can cut and paste from the output of module avail                 module load &lt;packagename&gt;       Load a generic Environment Module (ex: module load Perl)                 module list       Output a list of Environment Modules loaded in your current shell                 module unload &lt;packagename&gt;       Unload an Environment Module from your current shell                 module purge       Unload all currently loaded Environment Modules           There is also a short version of the module command: ml.  The ml command can substitute for module in any module command, behaves like module list when called with no arguments, and behaves like module load when executed with an argument (ex: ml R/3.5.1-foss-20167b-fh1 runs module load R/3.5.1-foss-2016b-fh1). The ml and module commands can be used in scripts, but see the section on Scripting with Environment Modules below.   Example:  $ which python /usr/bin/python $ module avail Python/2.7.15  -------------------------- /app/easybuild/modules/all --------------------------    Python/2.7.15-foss-2016b-fh1    Python/2.7.15-foss-2016b  Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".  $ module load Python/2.7.15-foss-2016b-fh1 $ which python /app/easybuild/software/Python/2.7.15-foss-2016b-fh1/bin/python   Scripting with Environment Modules  To use Environment Modules in a bash script, there are two Best Practices we highly recommend you integrate into your work.      Interactive shell session have the required module commands activated, but scripts can often be run in non-interactive shells, so it is best to explicitly activate the module command. Add the follow lines to the top of your script:     #!/bin/bash source /app/Lmod/lmod/lmod/init/bash module use /app/easybuild/modules/all          This snippet can be used as a template for bash shell scripts.       The source like activates the module and ml commands for you current shell, and the module use line loads our list of modules.   The next line you might use, for example, would be:  module load R/3.5.1-foss-2016b-fh1  This would load that specific Environment Module for use in your script.      Scripts are expected to be reproducible, so using a specific Environment Module reference is recommended:     module load Python/3.5.1-foss-2016b-fh1          Rather than:      module load Python          The above line will load a different version of the software package over time as the “pointer” to a specific version is changed.          Note: This does mean that your script will only work in environments with the specific Environment Module version you are loading. That environment module may not be initially available on systems outside Fred Hutch or on internal systems follow upgrades. You can either request the specific version be added, or edit your script to load an available package version.    Installing Custom Software Packages  If you do not find the software you need, a support package or library, or the specific version you need, you have two options:           Request the software be built: file an issue in our software repo and we will work with you to build a module for any software or version. This Environment Module will then be available to all.            If you cannot wait for the software to be built, you may be able to install it yourself. This is primarily supported for language (Python/R) packages.       Packages/Modules for Python and R  Normal install methods will work after loading an Environment Module:      R: install.packages(\"&lt;pkgname&gt;\")   Python: you can use pip or setup.py (specify --user with pip)   Any package you install this way will be installed into your home directory.   Remember that the environment module you have loaded will be used to install the package/module. For example, if you load Python/3.6.9 and use pip install --user &lt;newpkg&gt; then you will need to load Python/3.6.9 every time you wish to use newpkg. Using a different version of the language module may or may not work.   Other software installs and builds  If you want to install or build a standalone software package, you are also welcome to install into your home directory, with the following caveats:      We cannot install OS package dependencies (if your software has many dependencies, please file an issue here and we will be happy to work with you to offer a package build with all dependencies.   Ubuntu compilers are not optimized. We recommend loading a ‘toolchain’ module:                     module load foss-2016b                  This will get you GCC 5.4.0, binutils 2.26, OpenMPI 1.10.3, OpenBLAS 0.2.18, FFTW 3.3.4, ScaLAPACK 2.0.2 (most of our software on Ubuntu 14.04 is built against this toolchain).                   If you loaded a toolchain module when installing or building new software, you will must load that toolchain module before running that software, or you will get library errors.   Frequently Asked Questions     Note:  For announcements from Scientific Computing, please see the Announcements page, and for assistance email scicomp.  Also, see the Events page in CenterNet for current Office Hours.       Something weird is going on with my shell and/or job!?!            “Reset” your shell by logging out and back in. This will clear your environment. Users using screen or tmux will need to exit their session to clear their environment.           Why am I getting errors when running software from a module?            Unload all modules with module purge and re-load only the module(s) you need       Reset your shell - see above       Remove and reinstall software in your home directory not installed with the module you are using (~/R,~/.local) - this is key with toolchain modules and package/libraries that use compiled code           Only bash?            Our recommendation is to use bash as your shell. If you wish to use a different shell, please contact SciComp.           Is there a faster way?            The command ml is a shortcut for module and implies module load but will work with other module subcommands (ex: ml R/3.5.0-foss-2016b-fh1 or ml avail Python/3.5)           What is this “foss-2016b” stuff?            The EasyBuild project supports many different toolchains. The toolchain defines a compiler and library set, and also has a number of common support libraries (things like libTIFF) associated with it.           Should I load default modules?            It is faster and easier to type ml R than specifying the full package and version. However, the default version loaded by a generic module load &lt;pkg&gt; command will change over time.  If maintaining a specific version of a package is important to you, always specify the version.           Is there a list of included language libraries/modules/packages?            Yes! For R, Python, and some additional packages, look here.           What about Bioconductor for R?            Starting with R/3.4.3-foss-2016b-fh2 we include Bioconductor and many Bioc packages with the standard R module.           What are Best Practices with Environment Modules?            Specify the full Module name when loading, especially in scripts (see above for scripting information).       Avoid mixing Modules from different toolchains at the same time (unloading one and loading another mid-script works well if you need to).       If you can’t find a package you want, send an email us or file an issue requesting a new or updated package.           Batch Computing   Batch computing allows you to queue up jobs and have them executed by the batch system, rather than you having to start an interactive session on a high-performance system.  Using the batch system allows you to queue up thousands of jobs- something impractical to impossible when using an interactive session.  There are benefits when you have a smaller volume of jobs as well- interactive jobs are dependent on the shell from which they are launched- if your laptop should be disconnected for any reason the job will be terminated.   The batch system used at the Hutch is Slurm.  Slurm provides a set of commands for submitting and managing jobs on the gizmo and beagle clusters as well as providing information on the state (success or failure) and metrics (memory and compute usage) of completed jobs.  For more detailed information about Slurm on our systems see our Using Slurm page, which also links to a variety of detailed how-to’s and examples to get you started using the on-premise HPC resources available   Parallel Computing  There are many approaches to parallel computing (doing many jobs simultaneously rather than in series).  We have begun a Resource Library entry on Parallel Computing with Slurm, as well as created the FredHutch/slurm-examples repository containing community curated examples with additional documentation that can help you get started.   External Slurm and HPC Reference and Learning Resources  For more information and education on how to use HPC resources from external sources see the following sites:      Princeton’s Introduction to HPC systems and Bash.   Harvard’s Wiki site Slurm page.   The Carpentries lesson on HPC and job scheduling.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/cluster_software/",
        "teaser":null},{
        "title": "Using Slurm on Fred Hutch Systems",
        "excerpt":"This page is intended to be a basic introduction to using the workload manager for Fred Hutch managed clusters for high performance computing.  Slurm is the workload manager that manages both your jobs and the resources available in the clusters available.  There are two main clusters in use today that rely on Slurm - the on-campus Gizmo cluster and the cloud-based Beagle cluster (see our Technology page for more information about those resources.  Commands work the same in either environment.   Examples of Use  A GitHub repository has been created that is an evolving resource for the community containing working examples of using Slurm at Fred Hutch.  Please see the Slurm Examples repo for more specific guidance on using Slurm in variety of settings.  This is an evolving example repo that new users can refer to to begin to get into parallel computing and more adept use of Slurm.  If you are a Fred Hutch user and would like to contribute to the documentation or the examples there, to share with the community how you structure your interactions with Slurm, submit a pull request there.   Basic Slurm Terminology   Cluster   A cluster is a collection of compute resources (nodes) under the control of the workload manager (Slurm in our case).  At the Hutch we have two clusters, Beagle and Gizmo.  From most hosts the default cluster will be gizmo- selection of the target cluster is done via an argument to Slurm commands (see Multi-Cluster Operation below)   Partition   A partition is a collection of resources (nodes) inside of a cluster.  There are defaults, so specifying a partition name is not required.  While the different clusters may have different partitions, there are two partitions- a default partition with smaller nodes named campus and a partition with more capable nodes (more memory and CPUs) named largenode.   Node   A node is the basic computing unit that shares processors, memory, and some (limited) local disk.  As a rule, you don’t want to worry about choosing a node for your jobs.   Job   A job is a collection of tasks, typically implemented as a shell script.  Jobs have an ID (just a number) and a name.  The ID is automatically assigned, but you can assign a name to your job.   Account   When we refer to an “account” in the context of Slurm, we are referring to the PI account used to enforce limits and priority and not your HutchNet ID.  Your HutchNet ID is associated with an account.   Commands for Managing Jobs   squeue   The squeue command allows you to see the jobs running and waiting in the job queue.  squeue takes many options to help you select the jobs displayed by the command                  option/argument       function                       -M cluster       Only show jobs running on the indicated cluster                 -u username       Limit jobs displayed to those owned by a user                 -A account       Limit jobs displayed to those owned by an account                 -p partition       Only show jobs in the indicated partition           rhino[~]: squeue -u edgar       JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)    31071290    campus     wrap    edgar  R   19:03:13      1 gizmof404   The field NODELIST(REASON) will show either the name of the node(s) allocated for running jobs or the reason a job isn’t running.   There are many ways to alter which jobs are shown and how the output is formatted- refer to the squeue manpage for more details on using this command.   scancel   scancel allows you to signal jobs- most commonly this command is used to stop execution of a running job or remove a pending job from the job queue.  A job ID is the common argument though scancel will take many other arguments that allow bulk management of jobs- it shares many of the same arguments as squeue.  For example, the following command will cancel all jobs (pending or running) owned by the user edgar.   rhino[~]: scancel -u edgar   salloc   Obtain a Slurm job allocation (a set of nodes), execute a command, and then release the allocation when the command is finished.   hitparade (Fred Hutch homebrew)   The hitparade command will show a summary of all jobs running and queued on the cluster broken down by user and account.  Note that this isn’t a Slurm command, rather something built in-house at Fred Hutch.   hitparade takes the -M argument to select a cluster about which to generate the output.   rhino[~]: hitparade -M beagle loading Python/3.6.4-foss-2016b-fh2...    === Queue: campus ======= (R / PD) ======     poe_e (edgar) 300 / 72    === Queue: largenode ======= (R / PD) ===     schulz_cm (snoopy) 273 / 0    Commands for Submitting Jobs   sbatch and srun   sbatch is used to submit a job script to the cluster.  These run jobs without your intervention or input (i.e. non-interactively). Common arguments are:   srun is used to run a task on the cluster.  This is an interactive session, where you can directly view output as it’s produced or provide input (if needed by the task you are running).   These two take many of the same options:      -M select the cluster on which the job will run   -p change the partition   -t request a certain amount of time for the job.   -n request a number of tasks (default 1)   -c request a number of processors per task (default 1)   -J name a job   A useful option that is only applicable to sbatch is -o, which writes output to a different file.  The default will write the file _slurm-.out_ in the directory in which you submitted the job.   Examples   Submit a batch job (sbatch), that will run in one day, six hours (with the flag -t 1-6) in the largenode partition (with the flag -p largenode) in Beagle (with the flag -M beagle).  This will run one instance of the job with one processor (because no flags were provided to tell it to ask for more than the default).  Name the job “quoth-the-raven” (with the -J flag) and list the script to use in the job myscript.sh.   sbatch -M beagle -p largenode -t 1-6 -J quoth-the-raven myscript.sh   Submit a job using 6 cores (with the flag -c 6) and redirect output to a file named “my-output”:   sbatch -c 6 myscript.sh my-output   MultiCluster Operation   Most Slurm commands can operate against remote clusters (i.e. beagle from gizmo).  Typically the only change required is to add the argument -M &lt;cluster name&gt;.   sbatch -M beagle -c 6 myscript.sh my-output scancel -M beagle 12345   hitparade also supports -M and can be used to show the queue on the different clusters.  At this time, multi-cluster operations using the commands srun and salloc will not work.  If use of those commands is necessary, please contact SciComp.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/cluster_usingSlurm/",
        "teaser":null},{
        "title": "Overview of Computing at Fred Hutch",
        "excerpt":"Center IT supports a wide array of resources made available to researchers.  Much of the basic computing information needed on an ongoing basis can be found via Centernet and the Center IT pages.  However, much of the scientific computing resource documentation beyond this material is provided in this section of the Wiki, and links to existing resources and documentation are provided when available.   We maintain a space for both announcements from Scientific Computing as well as a Resource Library at the following links:     Updates and Announcements   Resource Library, Demo’s and HOWTOs   Access and Credentials  This section includes a variety of information about accessing computing resources at the Fred Hutch, including managing credentials for services when required.     Credentials   Methods   Data Storage  The Hutch, through Center IT and Scientific Computing, support a number of options for storing your data. The service you use to store your data will depend on the nature of the data and the anticipated use.      Database Systems   File Storage Systems   Object Storage Systems   Temporary Storage: Scratch   Collaborative Storage Systems   Large Scale Compute  This section contains articles that describe a range of high performance computing resource options available to Fred Hutch researchers.      Linux Operating System   Technologies   Scientific Software   Job Management   Cloud Computing   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/comp_index/",
        "teaser":null},{
        "title": "Scientific Computing Resource Overview for Grants and Publications",
        "excerpt":"Description of computational resources for grant writers   This is a description of Scientific Computing resources. For Shared Resources descriptions please see SR info for grant writers.   Long Description   The detailed description is a frequently changing document that contains some non-public information. Please email SciComp for the latest version of the document Fred-Hutch-Computational-Resource-Description-for-Grant-Writers.docx.   Short Description   If you are space constrained in your grant application or only need to include a summary of the resources avilable, this short description may suffice.   Local Computing Resources   The Gizmo cluster is currently equipped with almost 600 compute nodes, 4000 cpu cores, more than 40 TB of main memory (RAM) and is connected to a fully redundant high performance storage system via 100G networking equipment. Gizmo can directly access a storage capacity of more than 3 PB and has dedicated high performance scratch spaces of 500TB with a maximum throughput of nearly 10GB/s. Data in scratch storage spaces is automatically removed after 10, 30 or 90 days of inactivity depending on location.   Cloud Computing Resources   Fred Hutch is currently using cloud computing resources from the 3 major infrastructure (IaaS) cloud service providers. The internal network has been extended into several secure Virtual Private Clouds (VPC). The private cloud networks are covered under Business Associates Agreement (BAA). This environment is fully audited and integrated with our Security Information and Event Management (SIEM) system.   The Beagle and Koshu clusters are configured identical to Gizmo, however they execute their jobs in the cloud. More than 2000 cloud based compute cores are available to all investigators and they can seamlessly use their pipelines on premise and in the cloud.   Data Storage Service and Network Connectivity   The Fast File storage service provides a high performance Posix file system accessed via SMB and NFS protocols. A cloud-based mirror of this file system provides backup and disaster recovery capability.   The Economy File storage service is backed by two different technologies:           a high capacity object storage system that can scale to sizes &gt; 50 Petabyte at low cost. The service is based on commodity storage hardware, open source cloud storage technology (Openstack Swift) and is commercially supported. The system is can be accessed via Swift or S3 protocols. Three replicas (copies) of the data are stored in three different buildings on campus providing high resiliency and data protection.            the economy cloud service provides a secure object storage bucket for each investigator using AWS S3. The buckets can be used to store data as well as collaborate with external investigators.       Both systems are equipped with a recycle bin that protects against accidental deletion of files and currently allow for data restoration within 60 days after deletion.   Information for citing in publications   Fred Hutch is using compute equipment that has been partially funded by an NIH S10 instrumentation grant. If you use Fred Hutch computational resources in your publication, please cite: “Fred Hutch Scientific Computing, NIH grant S10-OD-020069”.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/grants_publications/",
        "teaser":null},{
        "title": "Linux 101",
        "excerpt":"What is Linux?   Linux is an operating system that has been developed over the past 27 years as a Unix-like operating system.  From hobbyist/student beginnings it has grown to be a versitile, mature, and fairly robust technology.   However, calling what we use “Linux” today glosses over what are many of the most important tools that make Linux useful: the GNU project. GNU, a recursive acronym for  Gnu’s Not Unix, is where most of the tools we use on Linux come from. Shells, compilers, utilities, and even games used in Linux come from the GNU project.  Thus you will sometimes see Linux referred to as “GNU/Linux”- mostly it’s just “Linux,” but it is important that much of the utility comes from that other important project.   Linux has become a core part of modern bioinformatic investigation- many of the most popular tools only run on Linux.  Thus, it’s important that you become at least comfortable using Linux and navigating the computational resources provided by the Hutch.   Learning Linux   Learning Linux means learning the shell.  The most common shell is bash and the one we’ll be assuming is in use here. A good way to get started learning Linux is going through some of the many tutorials that have been developed and are readily available from various providers:      The Unix Shell course from Software Carpentry (note, this organization has a number of different software oriented tutorials and resources as well).   The Introduction to Linux guide from The Linux Documentation Project   The rest of this document will expect you’ve gone through either one of the basic introductions above.  For more advanced use of the shell, tasks like scripting or programming, see:      Unix/Bash and Other Languages in this wiki   The Bash Beginner’s Guide from the Linux Documentation Project   Linux at the Fred Hutch   With these skills in hand, we will now discuss how to navigate the various Linux-based computational resources available to you in the Hutch computing environment.   Using the Network  These systems all live remotely: either in a server room on campus or possibly in a cloud provider’s datacenter.  Thus, we need to use the network to connect to them.  Most of our systems require that you are connected to the Campus network, either via wired network connection at a workstation, the Marconi wifi network, or via VPN from off-campus networks.   The next requirement is that you have a tool called “SSH” (for Secure SHell).  Mac OSX has one built-in and can be found by going to Applications, then select Utilities, and you will see the application Terminal. Windows users will need to find an add-on. PuTTY is a freely available SSH/terminal client that has been the go-to for Windows users for years.   An alternative is using a NoMachine client to start a graphical session.  That process is described here.  Within these NoMachine sessions you can start a terminal on the NoMachine server from whence you can start an SSH session.   With those tools, you are now ready to connect to one of the session servers described in our Technologies page.  Most commonly you will connect to the host rhino.   Setting up your Account   For the most part, your HutchNetID and password are all that are required to access the computational environment here.  If you do have trouble accessing hosts, contact Scientific Computing (scicomp).   Finding Data   Once you have access to these systems you’re ready to start computing.  Of course, computation really requires data, so next let’s discuss where all the interesting data resides on these systems.   This guide shows the common supported storage options. Additionally, see the Data Storage section in our Wiki for more guidance about storage locations and access.  All, except for the transfer drive, are available on SciComp supported systems:                  Service       Path                       Fast       /fh/fast                 Secure       /fh/secure                 Scratch       /fh/scratch                 Home       /home/           Next Steps   What you do next will depend on the direction your work will take you.  The Scientific Computing Resource Overview will have more details about the technologies available for your use.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/linux_linux101/",
        "teaser":null},{
        "title": "On-Premise High Performance Computing at Fred Hutch",
        "excerpt":"The Fred Hutch provides researchers on campus access to high performance computing using on-premise resources.  The various services provided are outlined here along with the basic information required for researchers to identify which FH resource might be best suited to their particular computing needs.   The systems listed here provided by the Fred Hutch, serve needs that rise above those that can be met using your desktop computer or web-based services. Often reasons to move to these high performance computing (HPC) resources include the need for version controlled, specialized package/module/tool configurations, higher compute resource needs, or rapid access to large data sets in data storage locations not accessible with the required security for the data type by desktop or web-based services. In the table below, gizmo is actually the compute resource that can be accessed via multiple tools, which are also listed below.   Overview of On-Premise Resources                  Compute Resource       Access Interface       Resource Admin       Connection to FH Data Storage                       Gizmo       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                 Beagle       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Center IT       home, fast, economy, AWS-S3, and Beagle-specific scratch                 Rhino       CLI, FH credentials on campus/VPN off campus       Scientific Computing       Direct to all local storage types                 NoMachine       NX Client, FH credentials on campus/VPN off campus       Scientific Computing       Direct to all local storage types                 Python/Jupyter Notebooks       Via Rhino (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                 R/R Studio       Via Rhino (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types           Rhino   Rhino, or more specifically rhinos are the locally managed HPC resources that are actually three different servers all accessed via the name rhino. These function as a data and compute hub for a variety of data storage resources and HPC tasks.   These are large shared Linux-based systems which are accessed via SSH.  As these are shared, you must take care not to overload these hosts.  As a rule, use the rhinos for cluster tasks, development, and prototyping.   NoMachine   The NoMachine (NX) servers provide a Linux desktop environment. These systems are useful if you use tools that require an X Windows display and you don’t wish to install an X11 server on your personal computer.  Another benefit of using these systems is that the desktop environment and any processes are preserved if you should disconnect- particularly handy for laptop users.   There are three NX servers: lynx, manx, and sphinx.  lynx runs the Unity desktop environment, the other two run Maté.   NoMachine requires you install the client (NX client) on your computer.  Clients are available for OSX and Windows.  Contact the helpdesk if you need assistance with installation.   Gizmo and Beagle Cluster   While we generally don’t recommend interactive computing on the HPC clusters- interactive use can limit the amount of work you can do and introduce “fragility” into your computing- there are many scenarios where interactively using cluster nodes is a valid approach.  For example, if you have a single task that is too much for a rhino, opening a session on a cluster node is the way to go.   If you need an interactive session with dedicated resources, you can start a job on the cluster using the command grabnode.  The grabnode command will start an interactive login session on a cluster node.  This command will prompt you for how many cores, how much memory, and how much time is required   This command can be run from any NoMachine or rhino host.      NOTE: at this time we aren’t running interactive jobs on Beagle nodes.  If you have a need for this, please contact scicomp.    Batch Computing   Batch computing allows you to queue up jobs and have them executed by the batch system, rather than you having to start an interactive session on a high-performance system.  Using the batch system allows you to queue up thousands of jobs- something impractical to impossible when using an interactive session.  There are benefits when you have a smaller volume of jobs as well- interactive jobs are dependent on the shell from which they are launched- if your laptop should be disconnected for any reason the job will be terminated.   The batch system used at the Hutch is Slurm.  Slurm provides a set of commands for submitting and managing jobs on the gizmo and beagle clusters as well as providing information on the state (success or failure) and metrics (memory and compute usage) of completed jobs.  For more detailed information about Slurm on our systems see our Using Slurm page, which also links to a variety of detailed how-to’s and examples to get you started using the on-premise HPC resources available   Parallel Computing  There are many approaches to parallel computing (doing many jobs simultaneously rather than in series).  We have begun a Resource Library entry on Parallel Computing with Slurm, as well as created the FredHutch/slurm-examples repository containing community curated examples with additional documentation that can help you get started.   External Slurm and HPC Reference and Learning Resources  For more information and education on how to use HPC resources from external sources see the following sites:      Princeton’s Introduction to HPC systems and Bash.   Harvard’s Wiki site Slurm page.   The Carpentries lesson on HPC and job scheduling.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/oldpages/cluster_rhinoGizmo/",
        "teaser":null},{
        "title": "Scientific Software Overview",
        "excerpt":"In-Person Training and Office Hours  SciComp provides in-person training via weekly office hours; this page contains the current times and locations.   Current Software  For software running on your Windows or Mac laptop or desktop, please contact your Division IT support.  However, when running software on our Linux servers (including Ubuntu Desktops), we have a system of pre-compiled software packages that should be your first stop when using a new software package.  This links directly to the most updated software catalog for these servers.   Updates and Announcements  Notifications related to Scientific Computing changes and announcements can be seen here.   Resource Library  This section contains short demo examples regarding a variety of supported resources supported at the Fred Hutch.  Check back here for ongoing additions of new demo’s and examples.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/oldpages/software_overview/",
        "teaser":null},{
        "title": "Training Overview",
        "excerpt":"While the content of this wiki aims to provide background information and basic knowledge of the systems and uses of Fred Hutch resources, it cannot address all the unique challenges involved in scientific research using these resources.   Scientific Computing In-Person Office Hours  Scientific computing offers office hours for in-person assistance to researchers.  As of 6/6/18, there are two office hour availabilities:      SciComp General Consulting: Wed. 10am-noon in room M4-B102 (with the exception of 11/21/2018 when the location is next door in M4-B104)   SciComp Cloud Consulting: Tue. 3pm in room D3-106   Fredhutch.io  Fredhutch.io (supported by Erik Matsen in PHS) provides a variety of in-person training opportunities on topics such as an introduction to unix/shell scripting, basic Python programming, basic R programming and guidance for use of Galaxy.   For updates on what is currently being offered, see the fredhutch.io website.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/oldpages/training_overview/",
        "teaser":null},{
        "title": "Supported Resources and Technologies",
        "excerpt":"The Fred Hutch provides researchers on campus access to high performance computing using on-premise resources.  The various technologies provided are outlined on here along with the basic information required for researchers to identify which resource might be best suited to their particular computing needs.   The Fred Hutch managed systems listed serve needs that rise above those that can be met using your desktop computer or web-based services. Often reasons to move to these high performance computing (HPC) resources include:     reproducible compute jobs   version controlled and/or specialized software   increased compute capability   rapid access to large data sets in central data storage locations   Overview of On-Premise Resources                  Compute Resource       Access Interface       Resource Admin       Connection to FH Data Storage                       Gizmo       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                 Beagle       Via Rhino or NoMachine hosts (CLI, FH credentials on campus/VPN off campus)       Center IT       home, fast, economy, AWS-S3, and Beagle-specific scratch                 Rhino       CLI, FH credentials on campus/VPN off campus       Scientific Computing       Direct to all local storage types                 NoMachine       NX Client, FH credentials on campus/VPN off campus       Scientific Computing       Direct to all local storage types                 Python/Jupyter Notebooks       Via Rhino (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types                 R/R Studio       Via Rhino (CLI, FH credentials on campus/VPN off campus)       Scientific Computing       Direct to all local storage types           Gizmo and Beagle Cluster   While we generally don’t recommend interactive computing on the HPC clusters- interactive use can limit the amount of work you can do and introduce “fragility” into your computing- there are many scenarios where interactively using cluster nodes is a valid approach.  For example, if you have a single task that is too much for a rhino, opening a session on a cluster node is the way to go.   If you need an interactive session with dedicated resources, you can start a job on the cluster using the command grabnode.  The grabnode command will start an interactive login session on a cluster node.  This command will prompt you for how many cores, how much memory, and how much time is required   This command can be run from any NoMachine or rhino host.      NOTE: at this time we aren’t running interactive jobs on Beagle nodes.  If you have a need for this, please email scicomp.    Available Resources  VMs, shiny, rancher, data transfer   Community Resources (not specifically supported by IT)  Are there things people use that we don’t really support?   Proposal Preparation  A description of computational and storage resources from Scientific Computing for grant writers can be found here.   &lt;!– ## Self Service Resources Jupyterhub, RStudio, db4sci, Galaxy, etc.   Gory Details on Node Classes   Resource Table  This table is auto-generated based on the yaml in _data/scicomp_resources.yaml, and is a work in progress.                  Name       Type       Authentication       Authorization       Location                       rstudio       web       web       hutchnetID       FHCRC                 proxmox       VM cluster       web       hutchnetID       FHCRC           Cluster Node Table  This table is auto-generated based on the yaml in _data/cluster_nodes.yaml:   GIZMO  Location: FHCRC                  Partition       Node Name       Node Count       CPU       Cores       Memory                       campus       f       456       Intel E3-1270v3       4       32GB                 largenode       g       18       Intel E5-1234v666       6       256GB                 largenode       h       3       Intel E5-1234v666       14       768GB                 none (interactive use)       rhino       3       Intel E5-1234v666       14       384GB           Additional resources                  Node Name       Network       Local Storage                       f       1G (up to 100MB/s throughput)       800GB @ /loc  (ca 100 MB/s throughput)                 g       10G (upto 1GB/s throughput)       5TB @ /loc (300MB/s throughput / 1000 IOPS) and 200GB @ /loc/ssd (1GB/s throughput / 500k IOPS)                 h       10G (upto 1GB/s throughput)       5TB @ /loc (300MB/s throughput / 1000 IOps) and 200GB @ /loc/ssd (1GB/s throughput / 500k IOPS)                 rhino       10G (up to 1GB/s throughput)       5TB @ /loc (300MB/s throughput/ 1000 IOps)           BEAGLE  Location: AWS                  Partition       Node Name       Node Count       CPU       Cores       Memory                       campus       f       777       Intel c5       4       15GB                 largenode       g       103       Intel c5       18       60GB                 largenode       h       34       Intel r4       16       244GB           Additional resources                  Node Name       Network       Local Storage                       f       EC2       EBS                 g       EC2       EBS                 h       EC2       EBS          ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/resource_overview/",
        "teaser":null},{
        "title": "Data Storage for Collaboration",
        "excerpt":"There are Fred Hutch supported data storage systems that allow you to share data with people outside the Hutch, with or without a Hutch ID in order to facilitate data transfer and receipt in collaborations within or outside of the Fred Hutch.   Aspera   The Aspera is a storage appliance that runs a heavily tuned storage server and client that enables fast transfer of large data between this system and a host using the Aspera client (either command line or via a browser).  The primary method of operation is to upload the data to the server, then use the web interface to create an email with a link you would then send to those outside the Hutch network.      NOTE: space is limited. Because of this, data stored here is deleted after a short period of time making the Aspera inappropriate for primary storage.  Always keep the primary source on one of the other options above (fast, economy, etc.)    Visit the Aspera information page for more details and information on using this storage service.   OneDrive  OneDrive is a cloud service that securely stores your files and folders in one place, share them with others, and update your files from any device. OneDrive is a benefit available individual users at the Fred Hutch that allows for private storage of files with the ability to share those files with others for collaboration.  With OneDrive you can:      Create documents on your computer and edit on your laptop, phone, or tablet   Collaborate with others in real time   View, store and share files and folders easily   Automatically sync files to your desktop for offline access   Simultaneously edit shared files with other collaborators   The Fred Hutch service Office365 (which includes OneDrive) has been designed with security in mind and comes with features that help achieve compliance with regulations such as HIPAA and FISMA. With that said, the safety of your data depends not only on the design of OneDrive but also on how you use it. You also have control over more sharing options and the ability to restore a previous version of a file.  Your files are viewable only by users to whom you have granted access. Unless a file or folder has been shared, it will remain private.  Once you have installed the OneDrive application on a mobile device you will be able to upload and share documents between computers and devices as well.   Visit the OneDrive CenterNet page for more details and information on using this storage service.  As of Oct 2018 the OneDrive Getting Started Guide is available and currently free storage per user is limited to 2TB.  Please check the linked CenterNet pages for up to date information on OneDrive.   Examples of best practices for using OneDrive include:     Do not sync your Fred Hutch OneDrive with any non-Hutch device   Do leave copies of sensitive data on a non-Hutch device from which you have accessed OneDrive   Sharing Links: Do not select “Everyone” when sharing, instead choose the “Specific people” option whenever possible. If you choose the “People in Fred Hutchinson Cancer Research Center” option, anyone at the Center with a link to your shared file can access it.   Once a file is shared with someone and they download it to their device, they can share it with others.  File protection may also remain an appropriate practice.   Links that share documents do NOT expire.  Remember to remove ability to share when no longer needed.      See additional Best Practices in CenterNet.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/store_collaboration/",
        "teaser":null},{
        "title": "Data Storage in Databases",
        "excerpt":"Database Management Systems (DBMS) are useful if you need to manage large data tables of structured data and / or multiple tables that can be linked to each other (relational databases). Databases are most successfully used by researches for data set for which the data structure (e.g. column names or tables) on a very frequent basis.   Important issues to consider before choosing a data storage location for a data set are:     How does the data need to be accessed by staff members and what is their experience level with databases?   What is the structure of the data set?  E.g., could it be contained in an Excel file, or a set of related Excel worksheets in a file, or are the data more complex in their structure?   Are logging, user-specific permissions, or data capture features desired or required?   How will backups and maintenance be addressed by the research staff?   Depending on the particular needs of the project, there are different data set storage options available supported by the Fred Hutch.   REDCap  REDCap, or Research Electronic Data Capture, is an open source, mature, secure web application for building and managing CRFs, data forms, and participant-completed surveys. REDCap is HIPAA-compliant and supports advanced features such as logging, validation, branching logic, e-signatures, randomization, calculated fields, and programming hooks. REDCap’s mobile app allows allows for offline data capture using iPad and Android tablets. Collected data can be easily exported from REDCap to Excel and common statistical packages (SPSS, SAS, Stata, and R) and has an API.   The beauty of REDCap is that it’s easy to get started but it also has advanced features and programming hooks to serve researchers needs when creating simple surveys all the way to conducting large complex research studies and projects.  For other types of research data, the ability to customize the data dictionary to meet the needs of a particular use make it an excellent option to consider when multiple users need to have a common data source with easy access and no database management knowledge is required.   REDCap has been used by thousands of non-profit institutions throughout the world on hundreds of thousands of projects. The REDCap group at Vanderbilt University manages the development of the REDCap code base; new features are being added every month that make REDCap better and better. At Fred Hutch, the instance of REDCap is supported by Collaborative Data Services (CDS), which is an active member of the REDCap consortium, regularly maintains our REDCap installation so that it remains up-to-date.   CDS provides backup services, database maintenance, support for developing REDCap projects, and is very accessible for staff without previous database experience.   For more information about REDCap at the Fred Hutch, training materials and documentation provided by CDS, visit their site here.   Look for in-person trainings available on a regular basis around campus, here. You will also find recorded training videos and presentation materials at this url..   The login page for the Fred Hutch instance is here.   MS SQL Server  For groups for whom a REDCap system is not suitable, Microsoft SQL technology is available in conjunction with Database Administrator Services such as:      Requirements analysis, review and documentation for internally developed and third party application databases   Provision of database environments for testing and development   Management of database users and permissions   Support for SQL reporting (SSRS), analysis (SSAS) and integrated services (SSIS) platforms   For further info about how to get started with a SQL server, please see the Database Hosting page on Centernet.   DB4Sci  (previously MYDB)  DB4Sci is a service supported by Scientific Computing (SciComp) that allows you to provision a dedicated server (aka, instance).  DB4Sci is a self-service website for creating containerized databases. The service is available to anyone at the center without cost. Users are given full admin rights to the database but are not given shell access to the database system. DB4Sci service performs daily database backups which are stored in the Amazon cloud. To access DB4Sci click the link and use your Hutch credentials to log in.      MariaDB Is the latest open source version of MySQL. MariaDB is traditional relational database that supports SQL. MariaDB has an option for data encryption and data encryption in transit (TLS).  If your project requires encryption at rest you should use MariaDB. For more information visit the MariaDB official site   Postgres Postgres is a very popular open source relational database. Postgres is very performant and capable of storing large databases. For more information visit the Postgres official site.   MongoDB Mongo is a specialized database for storing JSON documents. Mongo is a NoSQL database. For more information visit the MongoDB official site   Neo4j is a NoSQL database for representing graph data. For more information visit the Neo4j official site   More detailed information about DB4Sci can be found on the application website. DB4Sci is and open source project that is under development. Future features are: Self serve restore and Point in Time recovery. Project web site DB4Sci.org  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/store_databases/",
        "teaser":null},{
        "title": "Data Storage in Object Storage Systems (*Economy*)",
        "excerpt":"Object storage systems are not directly attached to your computer via drive mapping, a mount point or the Mac Finder, so you cannot just (double)click on a file to edit it with your favorite application. Most software used in life sciences cannot work directly with an object storage system as if the files were stored in traditional file storage systems. So why would you even want to use it if it seems more complicated than file storage? Object storage systems scale better in capacity and performance and are much cheaper to operate than traditional file storage systems. Cloud Computing depends very much on object storage systems such as Amazon’s AWS S3 or Google Cloud Storage.   Object Storage PI Allocations  As the amount of research data grows, which can occur rapidly when new large scale data is generated, existing externally generated datasets are transferred into the Fast storage, OR if existing data is inadvertently duplicated.  When the space requirements become larger, it is recommended that researchers begin implementing a set of more active data management practices.  Each PI is provided with 5TB of free storage space via Economy storage above which a relatively low cost per TB per month is charged.      Note:  Currently it is recommended to use a combination of Economy, Scratch and Fast.  Please see our other storage pages for more information about what storage locations are best for what type of data and uses.    Economy is less expensive than Fast and is suitable for large scale data sets that are not frequently accessed (i.e., ~monthly or less) but that require a relatively large amount of storage space.  For example, Economy would be suitable for a set of large files such as fastq’s or bam’s that on a daily basis are not accessed, but when a new bioinformatic process is desired, a large “burst” of activity will be occurring that will need to interact with the data.  Economy serves as an archive for these data, and typically compute processes do not directly access these files.   Features &amp; Benefits of Object Storage Systems   Some features and benefits of object storage systems include:           if you need to transfer data from Hutch Campus to cloud the network throughput performance of Object Storage is 10x higher than file storage.            greatly increased file listing performance, for example if you need to list 50000 files in a single directory file storage can take minutes to return the list but if you list an object storage bucket it can return millions of file names within seconds.            it mostly uses the standard http/https protocol which makes it much easier to share data with collaborators all over the world than using a file server and complex VPN technology            you can add additional and arbitrary attributes to each file. Why is this a benefit? Well, normally you just organize your files in folders but what if one file really belongs in multiple folders or projects or departments? Many users end up storing files in multiple different folders to keep relevant data together in one place. Object storage systems do away with folders all together, you just store all files in a single bucket and you can then tag it with many different attributes. These attributes or metadata are stored with the file as key=value pairs such as “cancer=breast” and “grant=P01-123456”. This additional metadata makes it extremely easy to retrieve data for automated pipelines based on different criteria.       Given these benefits it is expected that Object Storage systems will become more common in the future, especially as datasets are getting larger and larger.  Today Fred Hutch offers access to two different Object Storage systems through the Economy Local service. We recommend these systems typically for large genomic data and imaging files that require computational pipelines for processing (e.g. large BAM files) as well as for archival of infrequently used data. Both options for Economy storage are encrypted at rest and are approved to store strictly confidential data such as PHI.   Access to Economy storage is governed by Hutch Net ID (Active Directory) authentication and group membership in a security group called lastname_f_grp (e.g. groudine_m_grp). This is the same security group that governs access to Fast storage.      In the future Fred Hutch Shared Resources data delivery processes (e.g. through  HutchBase) will be modified to deliver data directly to Economy and Scratch storage as opposed to Fast as it happens today.    Accessing Economy Storage  You can access Economy Local or Economy Cloud resources with command line tools such as swc, swift, aws s3 or rclone or libraries such as swiftclient or boto3 for Python or aws.s3 for R.  As of March 2016, Center IT officially supports the use of Mountain Duck and Cyberduck desktop clients on Windows or Mac to move small amounts of data (Gigabytes rather than Terabytes) and occasionally open a file for editing. It is also possible to use Synology to backup data to Economy Local.   Economy Local (Swift)   Economy Local is an object storage system based on Openstack Swift. Economy Local is recommended for research groups who keep large amounts of data (&gt;100TB) on the Fred Hutch campus and frequently use the Gizmo cluster with local storage. We also recommend it for data that is explicitly forbidden to be stored in public cloud storage resources.      In the near future Economy Local will be retrofitted to become a hybrid storage solution. You will be able to access your data conveniently through a file access mount point such as /fh/economy/ in addition to the faster object storage access.    Economy Local is well suited to store petabytes of data at low cost and a high level of data protection. Economy File does not require tape backup as data is replicated to multiple sites. If you accidentally delete data it will be held in a “Trash can” for multiple months during which you have read-only access to the deleted data. Economy File is approved for PHI / PII data.  It is a suitable location to store genomic data  including those governed by the NIH Genomic Data Sharing policies or originating from public repositories while in use locally. Please the demo section for examples of Economy Local   Economy Cloud (S3)   Economy Cloud is a public cloud based object storage service that uses Amazon Web Services Simple Storage Service (S3) to offer managed and secure (encrypted) AWS S3 buckets to Fred Hutch investigators.  While it is not accessible by non-Hutch investigators by default, you can contact scicomp to request access for external research groups.Economy Cloud  is the default choice for Object Storage for every Hutch investigator who does not have any specific requirements.   You can use the Economy Cloud S3 buckets created for each PI to collaborate with external research groups. In addition to the Economy Cloud S3 buckets SciComp maintains S3 transfer buckets for external institutions and sequencing centers. These buckets may not be encrypted to increase interoperability. Please ask your PI to contact SciComp to enable the bucket of your PI for external collaboration or to enable a transfer bucket into which your sequencing center or other large scale data provider can drop large files.   How it Works   S3 (the Simple Storage Service) is an object store very much like the Economy file service, though provided by Amazon Web Services.  Storage resources in S3 are organized much like the other Fred Hutch Object and Block systems, with a “PI bucket” for each investigator at the Hutch which is analogous to the investigator directories available in the traditional file system. A specialized client (the AWS command line interface) is used to upload the data from local storage to S3 storage.  Once there, a temporary URL is created that has the necessary credentials embedded within and is then shared with those needing access.  A secure (HTTPS) connection is then used to download the data (via browser or other client like wget or curl). This URL is temporary and set with a date after which the generated URL is no longer able to access the data, though the data stored here is not removed as with the Aspera.  That temporary URL can be regenerated as necessary.   Backup and Security  Data on this service is not backed up in the traditional sense, but rather versioned: if a new version of a file is uploaded, the older version is saved in S3.  Similarly, if data is deleted, the versions aren’t and can be retrieved.  The Fred Hutch supported PI buckets in S3 are appropriate for storage of restricted data, including PHI.   Credentials and Permissions  Once you have obtained S3 credentials, you can use them to transfer files from/to the PI S3 buckets. If you work in the lab of the PI Jane Doe, your lab’s S3 bucket name will be fh-pi-doe-j. Please substitute your lab’s actual bucket name when using the examples in our Resource Library demos.   User Demos  We have a number of demos in our Resource Library related to how to interact with Economy Storage, specifically via a desktop client, via the AWS CLI, via R, or via Python and various methods for Economy Local      NOTE: This article is a work in progress. If you have suggestions or would like to contribute email sciwiki.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/store_objectstore/",
        "teaser":null},{
        "title": "Overview of Data Storage at Fred Hutch",
        "excerpt":"The Hutch supports a number of options for storing your data, specifically Database storage, File storage, Scratch storage, Object storage and Collaborative storage options.  The storage you use to store your data will depend on the nature of the data and the anticipated use. Here we provide a basic overview of what resources are available to researchers for data storage. For more detailed information on each of these topics summarized here including setup instructions and limitations, please refer to each of the articles in this section of the sidebar.   Storing protected health information (PHI) data   Please check with your supervisor before you store any PHI data on Fred Hutch systems as your data set may have compliance requirements that would not allow you to store the data on any of the existing systems. In almost all cases you should be working with de-identified data which can be stored on any of the above storage systems. If you require to store PHI data you should only use systems that support 1. Encryption at rest (e.g. on the hard drive), 2. Encryption in transit (e.g. through the network) and 3. access auditing (a systems log who accessed a file what time). Also, PHI can only be stored on systems that are approved by ISO.   This is an overview of supported features:                  Feature       Secure File       Fast File       Economy File       OneDrive                       Encryption at Rest       -       -       X       X                 Encryption in Transit       X       -       X       X                 Access auditing       X       -       X       X                 Approved for PHI by ISO       Yes       No       Yes       pending           Additional resources for identifying whether your data is de-identified or not and the Fred Hutch IRB’s information on HIPAA compliance can be found at the linked CenterNet pages.   Storage Allocation and Costs   Please see an overview of file allocation amounts, features and costs on Centernet. For expenses charged for your group for data storage in these systems, please see the storage chargeback portal.  This portal also displays the amount of data in TB that is stored in each location for each Investigator.      Note: You can typically access the monthly billing information during the first 10 days of every month    Database Storage Systems: REDCap, SQL and DB4Sci   There are several options available at the Fred Hutch for storing data in a database system.  These supported systems span a wide range of services to meet the various needs of Fred Hutch researchers.  These include REDCap (supported by Collaborative Data Services based in PHS), MSSQL Server (supported by CIT Enterprise Application Services) and DB4Sci (aka MyDB, supported by SciComp, and provides access to four database engine types including Postgres, MariaDB (MySQL), MongoDB, and Neo4j).   File Storage: Storage in Home, Fast, Secure   File storage keeps your data on disks and allows access to your data using familiar tools you’re used to: Unix commands like cat, cp, ls, and rm,  browser tools like Windows Explorer or OSX’s Finder (to browse drives mapped to your local workstation), and most common Bioinformatic tools. These storage systems are similar to the hard drive on your computer, just typically larger and faster.   Economy Storage: Object Storage   Object storage systems scale better in capacity and performance and are much cheaper to operate than traditional file storage systems. Cloud computing depends very much on object storage systems such as Amazon’s AWS S3. There are a number of features and benefits of object storage systems, such as better sharing of data and much better handling of meta data (e.g. annotations). At Fred Hutch we use object storage predominantly for very large files (e.g. BAM files) and for archiving purposes.   Temporary Storage: Scratch   The scratch file system is a file storage system that works differently than the systems intended for long term data storage. It is maintained by SciComp for temporary storage of research data during active analysis.  This is a large, high-performance storage system.  It is not designed to be as available or as robust as the home or fast file systems (these features were traded for lower cost and greater volume)- that said, it has shown itself to be quite reliable and reasonably fault tolerant.   Collaborative Storage Options   These storage systems have capabilities allowing you to share data with people outside the Hutch, with or without a HutchNet ID.  These include Aspera and OneDrive.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/store_overview/",
        "teaser":null},{
        "title": "Data Storage in File Storage Systems",
        "excerpt":"Filekeeps your data on disks and allows access to your data using familiar tools you’re used to: Unix commands like cat, cp, ls, and rm,  browser tools like Windows Explorer or OSX’s Finder (to browse drives mapped to your local workstation), and most common Bioinformatic tools.  These storage systems are similar to the hard drive on your computer, just typically larger and faster.   There are multiple file storage resources available to researchers including:     Home for personal files   Fast for shared data, including the majority of large scale research data   Scratch for personal and shared temporary data   Secure for data with higher-level security needs (PHI/encryption/auditing)      Note:  Currently it is recommended to use a combination of Economy, Scratch and Fast.  Please see our other storage pages for more information about what storage locations are best for what type of data and uses.    Home   Home storage is your own personal file storage space at the Fred Hutch.  It is a default place for your Linux login files (profiles &amp;c) as well as an appropriate place to store your own private information.   While you are allowed up to 100GB of storage in this home directory, it is not tailored for heavy use and won’t accommodate large data sets- the 100GB limit cannot be extended.  Also, data in this file system cannot be shared with others.   Fast   Fast storage is a large high-performance system that holds the bulk of the scientific data at the FredHutch.  Each PI is provided with 5TB of free storage space via Fast storage above which a cost per TB per month is charged.  This storage can be accessed by mapping the drive to their workstation (using //center/fh/fast/lastname_f on a Mac or \\\\center\\fh\\fast\\lastname_f or X:\\fast\\lastname_f on a PC). This storage access point can provide members of research groups access to groups of datasets that can have different types of permissions.  Within a PI’s Fast directory, directories can exist for data shared to any Fred Hutch user (/pub), to any member of the PI’s immediate research group (/grp), or private to each individual user in a PI’s immediate research group (/user).  Additionally, links to other data sources can appear here, such as data from the Fred Hutch Genomics Shared Resource (/SR).  This can be customized for a given researcher in conjunction with Scientific Computing (see Available Resources).   The data here is organized by investigator- each folder at the top level is named using the investigators last name and first initial: investigator “Sneezy Smith” would have a directory on fast called smith_s.   On SciComp supported Linux systems you will see this storage in the path /fh/fast.  Windows systems can access this via the UNC path \\\\center.fhcrc.org\\fh\\fast or X:\\fast and OSX hosts using the path smb://center.fhcrc.org/fh/fast.   This storage platform is appropriate for most scientific data- particularly large data sets.  There is no charge for the first 5TB of storage on this system: there is a $30 per-month charge for every terabyte above this.   Secure   Secure storage provides a higher-level of security controls than available on other services- the key difference is access auditing.  This is also organized by investigator with a 1TB free allocation above which a cost per TB per month is charged.   Secure file is available via the path /fh/fast/secure/research/lastname_f on SciComp Linux systems, \\\\center.fhcrc.org\\fh\\secure\\research\\lastname_f or X:\\secure\\research on Windows hosts, and smb://center/fh/secure/research/lastname_f on OSX.   This storage platform can be used for storing PHI.  It must be noted, however, that many connected systems may not be appropriate for analysis of PHI data.  The first terabyte of data is provided by CIT with a charge of $50 per-terabyte for any amount above that.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/store_posix/",
        "teaser":null},{
        "title": "Data Storage in Temporary Storage (Scratch)",
        "excerpt":"The scratch file system is maintained by SciComp for temporary storage of research data during active analysis.  This is a large, high-performance storage system.  It is not designed to be as available or as robust as the home or fast file systems meant for long term data storage (these features were traded for lower cost and greater volume).   Data here is purged when unused for some amount of time (10, 30, and 90 days depending on the location).   Data on this platform is not backed up.  This storage is not appropriate for storing the primary or only copy of any data.   Similar to the Fast File system above, the scratch file system is available on the path /fh/scratch on SciComp supported Linux systems, \\\\center.fhcrc.org\\fh\\scratch on Windows, and smb://center.fhcrc.org/fh/scratch on Mac.   There is no charge to the investigator for data stored here.   Types of Scratch Storage Available   On Gizmo there are three forms of scratch space available: “node local job scratch”, “network job scratch” and “network persistent scratch”.  The “network job scratch”  and  “node local”  scratch directories and their contents exist only for the duration of the job- when the job exits, the directory and its contents are removed.  For more persistent scratch space, ​please see the persistent Scratch section.   Node Local Job Scratch   There are varying volumes of local storage depending on node configuration and utilization by other jobs.  If you require a large volume of local disk, request it with the “–tmp” argument:  sbatch -n 2 -t 1-0 --tmp=4096 # requests 4GB of disk space  Note that this only ensures that the disk is available when the job starts.  Other processes may fill up this scratch space, causing problems with your job. The location of this local scratch space is stored in the environment variable “TMPDIR” and “SCRATCH_LOCAL- use this environment variable if you need local storage on the node- do not use “/tmp” for storage of files or for scratch space. Node local job scratch spaces are only available on gizmo nodes, not on rhino.   Network Job Scratch   Network global scratch space is a scratch directory that is created on storage that is available to all nodes in your job’s allocation.  The directory is based on the job ID.  You should access the job scratch directory by using the environment variable “$SCRATCH” in your shell scripts, for example use $SCRATCH/myfile.csv to write to a file. Node local job scratch spaces are only available on gizmo nodes, not on rhino.   Persistent scratch   Sometimes you need to work with temporary data that is not part of a specific pipeline, for example if you are doing manual QA on data for a few days or even weeks. The persistent scratch file system is accessible via environment variables $DELETE10, $DELETE30 and $DELETE90 and the files in these folders will be removed after 10, 30 or 90 days of inactivity. The $DELETE30 folder is currently available on Gizmo and $DELETE10 folders are currently avialble on Beagle and Koshu. These folders can also be reached from other operating systems: In Windows you can select (x:\\scratch\\delete30 ) and on Mac you select smb://center.fhcrc.org/fh/scratch/delete30.   How long will my data stay in persistent scratch?   In $DELETE30 the data will stay on the file system for 30 days after you have stopped accessing it. 3 days before the data is deleted you (the owner of the files created) will receive an email with a final warning:       From: fs-cleaner.py-no-reply@fhcrc.org [mailto:fs-cleaner.py-no-reply@fhcrc.org]     Sent: Tuesday, August 23, 2016 11:32 PM     To: Doe, Jane &lt;jdoe@fredhutch.org&gt;     Subject: WARNING: In 3 days will delete files in /fh/scratch/delete30!      This is a notification message from fs-cleaner.py, Please review the following message:      Please see attached list of files!  The files listed in the attached text file will be deleted in 3 days when they will not have been touched for 30 days:  # of files: 247, total space: 807 GB You can prevent deletion of these files by using the command 'touch -a filename' on each file. This will reset the access time of the file to the current date.   As an alternative to the environment variable $DELETE30 you can also reach scratch through the file system at /fh/scratch/delete30, however the file system may be subject to change whereas the environment variable will be supported forever.   How can I use Scratch?   In jobs on Gizmo, environment variables can be used to write and then read temporary files, e.g. $SCRATCH/myfile.csv, $SCRATCH_LOC/myfile.csv or $DELETE30/lastname_f/myfile.csv ($DELETE10 and $DELETE90 are in preparation).  Similarly, jobs on Beagle can currently use $SCRATCH/myfile.csv and $DELETE10/lastname_f/myfile.csv.   The files under $SCRATCH_LOC and $SCRATCH are automatically deleted when your Gizmo or Beagle job ends. You can also reach Scratch storage space via Windows (via the X: drive) or Mac, e.g. smb://center.fhcrc.org/fh/scratch.   Note: lastname_f stands for the last name and the first initial of your PI. If you do not see the folder of your PI please ask Helpdesk to create it for you.   Examples   In your Bash Shell:     #! /bin/bash     echo -e $TMPDIR     echo -e \"Network Job Scratch:​ $SCRATCH\"     echo -e \"Node Local Job Scratch: $SCRATCH_LOCAL\"     echo -e \"Node Local Job Scratch: $TMPDIR\"     echo -e \"Persistent Scratch: $DELETE30\"      runprog.R &gt; $DELETE30/lastname_f/datafile.dat   In Python:       #! /usr/bin/env python3     import os      print(\"Network Job Scratch: %s\" % (os.environ['SCRATCH']))     print(\"Node Local Job Scratch: %s\" % (os.environ['SCRATCH_LOCAL']))     print(\"Node Local Job Scratch: %s\" % (os.environ['TMPDIR']))     print(\"Persistent Scratch: %s\" % (os.environ['DELETE30']))      MYSCRATCH = os.getenv('DELETE30', '.')     myfile = os.path.join(MYSCRATCH,'lastname_f','datafile.dat')     with open(myfile, 'w') as f:         f.write('line in file')   In R:      #! /usr/bin/env Rscript     MYSCRATCH &lt;- Sys.getenv('DELETE30')     MYSCRATCH[is.na(MYSCRATCH)] &lt;- '.'​     save('line in file', file=paste0(MYSCRATCH,'/lastname_f/datafile.dat'))   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/computing/store_scratch/",
        "teaser":null},{
        "title": "STTR Group",
        "excerpt":"Affiliation   Seattle Translational Tumor Research Group   Members/Titles   Rachel Galbraith, Program Operations Director  Nola Klemfuss, Program Manager  Kara Colevas, Clinical Coordinator  Kylie McCloskey, Program Assistant  Jenny Zhang, Research Technician    Contribution Domain and Type   Data Generation - writing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/STTR/",
        "teaser":null},{
        "title": "Amy Paguirigan",
        "excerpt":"Affiliation  Clinical Research Division   Title  Senior Staff Scientist and Wiki Lead   Contribution Domain and Type  Wiki team leader   Postdoc Writer - Editor leader   Data Generation - writing and editing   Bioinformatics - editing   Computing - writing and editing   Github  vortexing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/apaguiri/",
        "teaser":null},{
        "title": "Ben McGough",
        "excerpt":"Affiliation  Scientific Computing   Title  HPC Systems Engineer   Contribution Domain and Type  Computing - editing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/bmcgough/",
        "teaser":null},{
        "title": "Chao-Jen Wong",
        "excerpt":"Affiliation  Human Biology, Tapscott Lab   Title  Bioinformatics Analyst   Contribution Domain and Type   Bioinformatics - writing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/cwon2/",
        "teaser":null},{
        "title": "Dan Tenenbaum",
        "excerpt":"Affiliation  Scientific Computing   Title  HPC Systems Engineer   Contribution Domain and Type  Computing - editing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/dtenenba/",
        "teaser":null},{
        "title": "Genomics Shared Resource Group",
        "excerpt":"Affiliation  Fred Hutch Shared Resources   Members/Titles  Andy Marty, Genomics Specialist   Cassie Sather, Senior Genomics Specialist   Alyssa Dawson, Genomics Specialist   Primary Contribution Domain  Data Generation - writing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/genomics/",
        "teaser":null},{
        "title": "John Dey",
        "excerpt":"Affiliation  Scientific Computing   Title  HPC Systems Engineer   Contribution Domain and Type  Computing - editing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/jfdey/",
        "teaser":null},{
        "title": "Jordan Smith",
        "excerpt":"Affiliation  Clinical Research Division   Title  Research Technician III, Biomedical Data Scientist   Contribution Domain and Type   Data Generation - writing and editing   Bioinformatics - editing   Github  jordanlsmith  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/jlsmith/",
        "teaser":null},{
        "title": "Jenny Smith",
        "excerpt":"Affiliation  Clinical Research Division   Title  Research Bioinformatician, Meshinchi Lab   Contribution Domain and Type  Computing - editing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/jlsmith3/",
        "teaser":null},{
        "title": "Jenny Zhang",
        "excerpt":"Affiliation  Seattle Translational Tumor Research   Title  Research Technician   Contribution Domain and Type  Computing - editing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/jzhang23/",
        "teaser":null},{
        "title": "Kylie McCloskey",
        "excerpt":"Affiliation   Seattle Translational Tumor Research   Title   Program Assistant   Contribution Domain and Type   Data Generation - writing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/kmcclosk/",
        "teaser":null},{
        "title": "Michael Gutteridge",
        "excerpt":"Affiliation  Scientific Computing   Title  HPC Systems Engineer   Contribution Domain and Type  Computing - writing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/mrg/",
        "teaser":null},{
        "title": "Dirk Petersen",
        "excerpt":"Affiliation  Scientific Computing   Title  Director   Contribution Domain and Type  Computing - curation lead, writing, editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/petersen/",
        "teaser":null},{
        "title": "Patty Galipeau",
        "excerpt":"Affiliation  Human Biology   Title  Professional Support Specialist, Reid Lab   Contribution Domain and Type  Data Generation - editing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/pgal/",
        "teaser":null},{
        "title": "Phu T. Van",
        "excerpt":"Affiliation  Vaccine &amp; Infectious Disease Division  Title  Bioinformatic Analyst and SciWiki Writer-Editor  Contribution Domain and Type  Data Generation - editing Bioinformatics - writing and editing Computing - editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/ptvan/",
        "teaser":null},{
        "title": "Richard Barfield",
        "excerpt":"Affiliation  Public Health Sciences Division   Title  Post-Doctoral Research Fellow   Contribution Domain and Type   Bioinformatics, Scientific Computing - writing/editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/rb/",
        "teaser":null},{
        "title": "Robert McDermott",
        "excerpt":"Affiliation  Scientific Computing   Title  IT Solutions Architect   Contribution Domain and Type  Computing - editing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/rmcdermo/",
        "teaser":null},{
        "title": "Susan Glick",
        "excerpt":"Affiliation  Hutch Data Commonwealth   Title  Research Compliance Specialist   Contribution Domain and Type  Data Generation - writing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/sgglick/",
        "teaser":null},{
        "title": "Sam Minot",
        "excerpt":"Affiliation  Vaccine and Infectious Disease   Title  Staff Scientist, Fredricks Lab   Contribution Domain and Type  Bioinformatics - editing and editing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/contributors/sminot/",
        "teaser":null},{
        "title": "Topic Title",
        "excerpt":"Introduction to what is on this page, why you might want to read it, definitions of any words a novice might not already know, and some basic background information you’d need to understand the detailed content.   Subtopic 1 Title  Put basic content here.   Subheading 1  Put more detailed content here.   Subheading 2  Put more detailed content here.   Available Resources and links  Here you can provide contextualized links to humans to contact about this (names only, let’s leave emails out of this), or links to good websites, or links to places in Centernet where more detailed information can be found.  Please try to keep the above text sections as text ONLY, and put all links in this links section!!   Subtopic 1 Title  Put basic content here.   Subheading 1  Put more detailed content here.   Subheading 2  Put more detailed content here.   Available Resources and links  Here you can provide contextualized links to humans to contact about this (names only, let’s leave emails out of this), or links to good websites, or links to places in Centernet where more detailed information can be found.  Please try to keep the above text sections as text ONLY, and put all links in this links section!!  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/drafts/contentTemplate/",
        "teaser":null},{
        "title": "Your Name",
        "excerpt":"Affiliation  Clinical Research Division  Title  Senior Staff Scientist and Wiki Lead  Contribution Domain and Type  Data Generation - writing and editing Bioinformatics - editing Computing - demos  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/drafts/contributorTemplate/",
        "teaser":null},{
        "title": "Scientific Computing Supported Resources - DRAFT",
        "excerpt":"   NOTE: This article is a work in progress. If you have suggestions or would like to contribute email sciwiki.    A description of computational and storage resources from Scientific Computing for grant writers can be found here.   Self Service Resources  Jupyterhub, RStudio, db4sci, Galaxy, etc.   Available Resources  VMs, shiny, rancher, data transfer   Community Resources (not specifically supported by IT)  Are there things people use that we don’t really support?   Auto-generated Table  This table is auto-generated based on the yaml in _data/scicomp_resources.yaml:                  Name       Type       Authentication       Authorization       Location                                                       rstudio       web       web       hutchnetID       FHCRCproxmox       VM cluster       web       hutchnetID       FHCRC          ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/drafts/resource_overview/",
        "teaser":null},{
        "title": "Flow Cytometry Startup Instructions",
        "excerpt":"Changing the Sheath Tank Fluid and Waste     Check the fluid levels. This can be done by checking their weight or opening the lids. The weight of the sheath tank (looks like a box) when full (20L) is approximately 20 kg. Tanks should never run empty. There is a level indicator in the diva program on the cytometer window.   WASTE: Disconnect waste tank and unscrew the large lid to pour out the waste into the lab sink.   SHEATH FLUID: Disconnect and unscrew the level sense probe to remove it.   Add 1x PBS (sheath fluid) to the tank and fill to the top weld line.   Replace and secure the lid by pressing the down the latch or screwing tight the fastener at the top.   Reattach the tank to the fluidics cart   Bleed the three fluid filters on the front of the fluidics cart by unscrewing the bleed port.  These are gravity fed and pressure from the tanks should force air out of the bleed ports.  Replace the plugs once the air is removed.   Overview of the DIVA software          Cytometer settings file (also known as instrument settings in older files): This file contains parameter voltage settings, threshold set point, and laser delay settings that are used to acquire data. Only the FS voltage, SSC voltage and the threshold are adjustable.  All other settings are predetermined in the control file.            Cytometer settings file (Global vs tube): The global cytometer settings file is located directly under the experiment name. After collection this file is automatically copied to each tube run and is saved with each tube.            Parameter usage: determining the detectors needed for each experiment and delete all parameters not being used in the experiment. This is located in the inspector window period.            Creating a new experiment: Under the Experiments drop down select new experiment. This will bring up previously saved templates. Select your template or under the general tab find New Experiment with blank sample tube. Experiments must be labeled with your hutch net ID first and then the days date (mmddyr).            Creating a new experimental template file: Templates must be labeled using your hutch net ID and then the name of your experiment.            Export FCS v. 3.0 files: While other formats can be exported always export using this format.       QC Bead Setup   Cytometer Setup and Tracking (CST) Beads     Under Cytometer select CST. This will open up a window for running a cytometer setup and tracking performance check.   Remove the HTS coupler.   Load a facs tube with the bead LOT number that corresponds to the bead lot set for this machine. If the LOT number is listed as 0000 check with the flow lab staff for the current bead lot number for that machine.   Click Run to start the setup and tracking script.   Hopefully, the CST performance check has passed. Then you click finish and exit out of the CST setup window. If the CST performance does not pass. Proceed to the Ultra Rainbow section below and check the CVs. If they are all \\&lt;10 then it is fine to run your samples.   A prompt will then appear asking what settings you would like to use. You will select Use CST Settings as you have just run a performance check that passed on the CST beads.   Remove the CST beads from the cytometer.   Ultra-Rainbow (UR) Beads 3.8     Click the green arrow to the left of the date you have created after opening up the DIVA software.   Load the UR 3.8 beads onto the cytometer and select Run and low for the speed. Keep the fine adjusted speed at 250.   In the DIVA software click acquire. Ensure that the beads are falling within the set gates, then click Record.   After the 10,000 events are recorded, unload the tube and record the values corresponding values for FSC, FITC etcetera in the cytometers binder.   Once the UR values are recorded click File and select logout. This will bring you to the DIVA start screen. On the drop down Select FHCRC and enter. No password is needed.   Create a new daily folder and label it with the current days date: mm/dd/yr.   Connecting to your Fred Hutch User Account to Transfer Data      Click on windows explorer, which is the file icon near the windows home icon (lower left corner)   Right click on Network and select map network drive…   A new window will appear. Select any letter drive and then select \\\\homelink\\homes. Then check the box next to connect using different user.        Enter the following to connect to your Fred Hutch account: fhcrc.org\\_your username_, then _Your password_       Right click on your experiment. Select export and fcs files. They should be FCS v. 3.0 and the files should be in linear mode. Export to the file you want in your account that you just connected.   Once transfer is complete, disconnect your account by again right clicking on Network, but now select disconnect network drive…   Close your experiment and sign out of iLab.   Canto II Instrument Setup and Operation  Starting up the Cytometer      Start the cytometer by pressing the GREEN button on the side of the machine.   Turn on the computer and log on to the admin user for windows using the password (see Flow Core Staff for these).   Sign into iLab using your Hutch Net ID.   Open the DIVA software. Run the fluidics start up and Bead QC in the administrator profile. No password is needed.   Fluidics      Be sure that the HTS coupler is connected to the probe before running setup. If not, slightly loosen the coupler and insert the probe until it is flush. Then screw tight the coupler to fasten it to the probe.   On the desktop left click on the Facs DIVA software. On the drop down Select Administrator and enter. No password is needed.   A prompt will then appear asking what settings you would like to use. You will select Use CST Settings as you have just run a performance check that passed on the CST beads.   Open the file for the current year for UR and select the experiment for current month. Expand out the specimen and add a new tube label this tube with the current days date: mm/dd/yr   Startup after turning on the lasers      From the drop down, menu at the top select Cytometer, then select Cleaning Modes and Prime Tank after Fill. A window will appear with three boxes that can be check marked. Check all three boxes, then select Run.  Perform this action twice. ``` ✓ FACSFlow   ✓ Shutdown Solution   ✓ Cleaning Solution ```      From the drop down, menu at the top select Cytometer, then select Bubble Filter Purge and Degas Flow Cell. Perform this action twice.   From the drop down, menu at the top select Cytometer, then select Fluidics Startup. A window will appear notifying you that this action will take 10 minutes to complete. Click Run to proceed.   See Section on QC bead setup and continue with the alignment procedures using CST and Ultra rainbow beads before running.   Shutdown Specifics     Be sure that the HTS coupler is connected to the probe before running setup. If not, slightly loosen the coupler and insert the probe until it is flush. Then screw tight the coupler to fasten it to the probe.   From the drop down, menu at the top select Cytometer, then select Fluidics Shutdown. A window will appear notifying you that this action will take 5 minutes to complete. Click Run to proceed.   Once completed. Click Ok Press the GREEN button on the side of the cytometer to turn off the machine.   On the computer close all software and running programs, but DO NOT log out from iLab.   Click the windows home icon and select shutdown. A window will appear asking if you want to force the shutdown while iLab is running. Select force shutdown. This will turn the computer and monitors off.   LSR II Instrument Setup and Operation   Starting up the Cytometer      Start the cytometer by pressing the GREEN button. Once the tank is pressurized bleed air from the filter attached to the sheath fluid line.   Turn on the computer and log on to the admin user for windows using the password: BDIS#1   Sign into iLab using your hutch net ID.   After powering on the cytometer by pressing the GREEN button, the red laser must be turned on. This is done by clicking on the Fiber Laser software application icon on the desktop. Clicking this will open up the application seen in Figure 1.   Press the On button in the application window to turn on the laser.   Set the power level to 200 by typing it in the Power set section. Then click Activate.   The current mA (milliamps) should increase, which indicates that the laser is powered on.  Fiber laser application for turning on the red laser. Power must be set to 200mW.   If the current, power, and SHG temp. values are blank without green numbering, then you are not connected to the correct com port.   You must click the com port icon in the upper left-hand corner of the window and select com serial port 1.   Fluidics   Startup after turning on the lasers      Prime the fluidics by pressing the PRIME button twice.   Place a facs tube with 40% Contrad onto the cytometer and run on high at a flow rate of 500 for 5 minutes. Do this by turning the black dial all the way clockwise.   Bleed the filters so that the lines are free of air bubbles.   Prime the fluidics by pressing the PRIME button twice.   Replace the tube containing contrad with a tube containing blue detergent (Coulter Clenz) and let run for 5 minutes.   Prime the fluidics by pressing the PRIME button twice.   Replace the tube containing detergent with a tube containing ethanol (EtOH) and let run for 5 minutes.   Replace the tube containing ethanol with a tube containing DI water and let run for 5 minutes.   Remove the tube and press prime while the stage is off to the side. Wait until the STANDBY button turns on (amber color).   See section III. for turning on appropriate lasers for running alignment and samples. See Section V. for QC bead setup and continue with the alignment procedures using CST and Ultra rainbow beads.   Shutdown      Place a facs tube with 10% bleach (NaOCl) onto the cytometer and run on high at a flow rate of 500 for 5 minutes.   Replace the tube containing bleach with a tube containing blue detergent (Coulter Clenz) and let run for 5 minutes.   Replace the tube containing detergent with a tube containing DI water and let run for 5 minutes.   Press the GREEN button on the front of the cytometer to turn off the machine.   On the computer close all software and running programs, but DO NOT log out from iLab.   Click the windows home icon and select shutdown. A window will appear asking if you want to force the shutdown while iLab is running. Select force shutdown. This will turn the computer and monitors off.   Symphony and Fortessa X-50 Instrument Setup and Operation   Starting up the Cytometer      Start the cytometer by pressing the GREEN button. Once the tank is pressurized bleed air from the filter attached to the sheath fluid line.   Turn on the computer and log on to the admin user for windows using the password: BDIS#1   Sign into iLab using your hutch net ID.   After powering on the cytometer by pressing the GREEN button, the red laser must be turned on. This is done by clicking on the Fiber Laser software application icon on the desktop. Clicking this will open up the application seen in Figure 1.   Press the On button in the application window to turn on the laser.   Set the power level to 200 by typing it in the Power set section. Then click Activate.   The current mA (milliamps) should increase, which indicates that the laser is powered on. Fiber laser application for turning on the red laser. Power must be set to 200mW.   If the current, power, and SHG temp. values are blank without green numbering, then you are not connected to the correct com port.   You must click the com port icon in the upper left-hand corner of the window and select the appropriate com port ( Symphony 1: 11, Symphony 2: 9 and Fortessa X-50: 6 ).  Fiber laser application with COM port number 11 selected.   Double click the BD Coherent icon on the desktop and the coherent software will open as seen in Figure 3.   The 355nm laser does not automatically start when opening the program and must be started manually.  Enter 65 as the mW power then click laser STOP and laser START. This should start the laser and all four should then be operational.   Fluidics   Startup after turning on the lasers      Prime the fluidics by pressing the PRIME button twice.   Place a facs tube with 40% Contrad onto the cytometer and run on high at a flow rate of 500 for 5 minutes.   Bleed the filters so that the lines are free of air bubbles.   Prime the fluidics by pressing the PRIME button twice.   Replace the tube containing contrad with a tube containing blue detergent (Coulter Clenz) and let run for 5 minutes.   Prime the fluidics by pressing the PRIME button twice.   Replace the tube containing detergent with a tube containing ethanol (EtOH) and let run for 5 minutes.   Replace the tube containing ethanol with a tube containing DI water and let run for 5 minutes.   Remove the tube and press prime while the stage is off to the side. Wait until the STANDBY button turns on (amber color).   See section III. for turning on appropriate lasers for running alignment and samples. See Section V. for QC bead setup and continue with the alignment procedures using CST and Ultra rainbow beads.   Shutdown      Place a facs tube with 10% bleach (NaOCl) onto the cytometer and run on high at a flow rate of 500 for 5 minutes.   Replace the tube containing bleach with a tube containing blue detergent (Coulter Clenz) and let run for 5 minutes.   Replace the tube containing detergent with a tube containing DI water and let run for 5 minutes.   Press the GREEN button on the side of the cytometer to turn off the machine.   On the computer close all software and running programs, but DO NOT log out from iLab.   Click the windows home icon and select shutdown. A window will appear asking if you want to force the shutdown while iLab is running. Select force shutdown. This will turn off the computer and monitors.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/gendemos/FlowCytometryStartupAndConfiguration/",
        "teaser":null},{
        "title": "Human Data GDS Proposal Development: Regulatory User Scenarios",
        "excerpt":"Investigators with questions about whether the NIH GDS Policy applies to their current or proposed research should consult the relevant NIH Program Official or Project Officer or the IC’s Genomic Program Administrator.   GDS Human Data Study Development: Scenarios by Data Type  Case 1  Identifiable: Prospective Study  A Fred Hutch investigator is starting a new study prospectively banking human specimens from which they intend to generate genomic data sets subject to the GDS policy. Research staff will have access to patient identifiers.  This is a single site study.   Case 2  Identifiable and FH Sourced: Retrospective Study  A Fred Hutch investigator is starting a new study prospectively banking human specimens from which they intend to generate genomic data sets subject to the GDS policy. Research staff will have access to patient identifiers.  This is a single site study.   Case 3  De-Identified and FH Sourced: Retrospective Study  A Fred Hutch investigator is starting a new study using human specimens already banked and collected at FH and from which they intend to generate de-identified genomic data sets subject to the GDS policy. This is a single site study.   Case 4  De-Identified and Sourced Outside Fred Hutch: Retrospective Study  A Fred Hutch investigator is starting a new study using human de-identified genomic datasets that were generated by someone else outside of Fred Hutch. This is a single site study.   Case 5  De-Identified and Sourced from dbGaP: Restrospective Study  A Fred Hutch investigator is starting a new study using human de-identified genomic datasets from dbGAP. This is a single site study.   Process Overview for Each Case Scenarios  This table specifies the steps involved in performing these types of studies from a regulatory perspective (and that occur alongside the scientific process involved).                  Steps in the Process       Case 1       Case 2       Case 3       Case 4       Case 5                       Data Sharing Plan(DSP) Needed? Submit to FH IRO and NIH       Y       Y       Y       Y       Y                 Need to Select with which NIH Trusted Repository to Share - Aggregate or Individual level?       Y       Y       Y       Y       Y                 Need to complete both FH GDS Institutional Certificate and Certificate Information Sheet?       Y       Y       Y       Y       Y                 Need GDS appropriate consent (with broad sharing language) for specimen or cell information collected after January 25, 2015?       Y       Y       Y       Y       Not Likely* - Confirm IRO Pre-Approved                 Is proposal under Human Subjects? Complete the FH IRO Determination Form       Very Likely       Very Likely       Not Likely       Not Likely       Not Likely* - Confirm IRO Pre-Approved                 Does the proposal require a DUA or other sharing agreement**?       N       N       N       Y       N              * dbGaP is part of the Fred Hutch IRO pre-approved list of data sources; such sources are presumptively considered to be research not involving human subjects.       ** DUA: What are the security requirements if any of the DUA? Are there data use limitations on the data?    What should be included within the GDS proposal package to the IRO/IRB?  Proposal, completed FH GDS Institutional Certificate Information Sheet (OSR), a signed Intuitional Certificate, if needed a A GDS compliant consent (see chart above and note below), FH IRO Genomic Data Supplement form, Data Sharing Plan.      Note: For samples generated after January 25, 2015, inquire as to their foundational (collection) consent. This consent should be included in the documents submitted to the IRO/IRB.  No consent may be needed for speciments/data collected without consent before January 25, 2015. An NIH Institutional Certificate is required regardless the date the of specimen/data collection.    Using dbGaP-sourced data in a proposal (Secondary Research Source):     FH IRO pre-approves data in dbGap as non-Human Subjects.  This may not apply to ALL NIH trusted repositories.   Address plans for requesting access to the data, state intention to abide by the NIH Genomic Data User Code of Conduct in the Research Plan of the NIH application.   Researchers may wish to secure access to dbGaP controlled-access data prior to submitting their application for NIH support.   For Identified Data after NIH Approval: Notes on Submitting to NIH for Data Sharing     Any unique random ID per person and all other coding keys to remain at Fred Hutch when data is submitted to data share.   Data remaining at Fred Hutch stored according to Fred Hutch Information Classification Handling Standard and other Fred Hutch Information Security standards.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/gendemos/HumanSpecimensStudies/",
        "teaser":null},{
        "title": "Guidance for Making \"Tidy\" Data",
        "excerpt":"All collaborations, even when your collaborator is your future self, benefit from all parties learning the art of tidying data.  In many cases the best time to do this is to implement Tidy Data approaches when the data are generated in the first place, thus minimizing issues with data loss, corruption and loss of effort to cleaning messy data.   General Concepts  Use open file formats: assume that your collaborator does not have time or resources to buy commercial software. Use plain-text .csv for numeric data, and PNG/JPEG/TIFF for image data. For data compression, use GZIP or BZIP2. If you must use proprietary formats, include instructions for getting free converters/processors.   Use concise, descriptive filenames: moreData.txt is not informative. gatedCellCounts_20000101.txt is better. Use underscores or dashes instead of spaces, CamelCaseLikeThis to make long filenames easier to read, and avoid symbols in filenames.   Be consistent: use the same convention throughout your data. For example, all dates should be in the same format (01-01-2000 vs. January 1st, 1900), demographics should use controlled vocabulary (white vs. caucasian, Female vs. f), missing values should be consistent (none vs. na vs. N/A)   Data should only contain values: don’t include units (only 100, not 100seconds or 100mmol) in the entries, consider including this information in the column names OR in the data dictionary.   Notes should be standardized: don’t use text formatting to code data (eg. coloring blue for male subjects and red for female in an excel sheet). Don’t include free-form notes (eg. “this subject was lost to follow-up). Rather use an additional column to indicate status and describe your convention in the data dictionary (see below).   Include a “data dictionary”:  Describe what units are for each measurement and which values are acceptable.   Data-Type Specific Suggestions  Flow-cytometry data  More to come.  Sequencing data  More to come.  Imaging data  More to come.   Available Resources  There are a variety of online resources where you can learn more about what Tidy data is.     Coursera Getting and Cleaning Data Course  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/gendemos/MakingTidyData/",
        "teaser":null},{
        "title": "Clinical Data Management",
        "excerpt":"The ability to identify suitable biospecimens associated with a cohort–available via collaboration with other researchers or via a specimen repository–is critical to enable the generation of large-scale data sets associated with patient phenotypes.  On this page we describe what data management systems are available to Fred Hutch researchers, to access and interpret clinical data for a select cohort.   Clinical Data Abstraction in Caisis  Caisis is an open source data management platform in use by STTR to facilitate data abstraction and programmatic resources to support translational research. Abstraction of data into Caisis is covered by the HIDRA IRB and does not require program-specific IRBs until the data is being requested out for research purposes. Caisis is centered around individual patients and helps create a story about their clinical experiences and health status at various times in the course of their treatment at UW Medicine or SCCA. Data entered into Caisis builds upon the discrete data elements available through the Hutch Data Repository. Approximately 30 fields from that data repository are fed directly into Caisis, along with a linkage to SEER data which pulls in death dates when available. Abstracted data elements are recorded in discrete fields, per data dictionaries which are set up by consensus by multiple members of each program. Wherever possible, data elements are standardized across disease programs, enabling data sharing across programs and larger research questions to be answered in the future. All data in Caisis is audited (real-time auditing is performed on at least 10% of patients entered for each program). To view fields and disease-specific data dictionaries, see the Fred Hutch Ontology Browser.   Currently, there are 12 programs with data abstracted into Caisis:     Brain   Breast   Colorectal   Head &amp; Neck   Lung   Liver   Lymphoma   Myeloma   Pancreas   Prostate   Sarcoma   Thyroid   Many of these have ongoing abstraction. STTR contracts with abstractors through a company called Vasta Global at a cost of $24,000 per abstractor per year. Each group selects and prioritizes the patients for which they would like data abstracted, supplying the abstractors with an updated list on a regular basis. Please note, the contract with Vasta Global only allows them to access the UW/SCCA medical records, not those from outside institutions. The data abstracted into Caisis is available to any interested Consortium investigator, with valid IRB approval or for activities preparatory to research.   STTR REDCap Template  In partnership with HDC, STTR has created a REDCap template which is available to any Fred Hutch investigator. The template provides an opportunity for groups who are interested in abstracting their own patient/participant data (either for a specific study group, or from an outside medical institution). The template includes the fields which are common across programmatic data dictionaries and allows each group to tailor the data collection to their own needs and timeline.   Whether or not a REDCap should be spun up will depend on what the investigator is intending to use the Caisis data for. If investigators will be adding their own data elements (from a survey or additional data abstraction) then spinning up a new REDCap project for their study makes sense. If investigators will be working directly with the data and performing their data cleaning and analysis, then the data should be distributed by depositing it into the investigator’s Secure storage.   There are three primary scenarios where the template may be especially useful: A group would like to start with existing Caisis data: they could make a data request that would populate the REDCap template and the group would then have an easy interface for entering additional study-specific data.   A group would like to begin data abstraction and wants the benefits of standardized data elements, but does not want to go through the formal process of beginning abstraction into Caisis: the STTR REDCap template will allow groups to being abstraction using common data elements and standards which enable sharing of data across research groups – if a group would like to follow a specific data dictionary, they can request the STTR REDCap template and then modify the interface to match the disease program data dictionary as found in the Fred Hutch Ontology Browser   If a group starts abstraction using REDCap, but would like to contribute the data back to the institutional database in Caisis at a later date: When the group has finished their data entry into REDCap for their specific project, STTR will provide auditing services to determine the accuracy of the entered data and, if it passes our threshold, we can push the data into Caisis.   If your team has questions about Caisis, the STTR REDCap template, or data abstraction, please reach out to us by emailing STTRCancer.   Data Requests  To request data which has been abstracted into Caisis and/or data from the Hutch Data Repository, investigators or research coordinators need to fill in the data request form available here. The data request form and instructions can also be found here. Requests for data can be made for the purposes of research, preparatory for research, operations, quality assurance/quality improvement and clinical care. For research projects, IRB documentation will be requested to verify whether the requestor should be granted access to the requested data.  Requests for identified data in support of preparatory to research activities will be reviewed for potential documentation requirements, such as a confidentiality pledge.   The Fred Hutch Ontology Browser can be a useful tool in developing data requests as it will help researchers understand how, when and in what format each data element is collected.     Tip: Before submitting a request, use the Ontology Browser to create an Excel file/csv containing your fields of interest that are already in existence then that file can be uploaded within the data request form.    Detailed Instructions      If you have access to Centernet, search for and select “HDC Request for Data Services” or go to Centernet page here. If you do not have access to Centernet go here and select “HDC Request for Data Services.”   This page will provide an FAQ, instructions and related documents or forms which may need to be downloaded.   Select the box “Complete User Access Request VIA REDCap” to open access to the electronic submission form.   Broadly describe the request. If you are requesting CAISIS data from an STTR supported group (Lymphoma, Myeloma, etc) then you can  select “Yes” to the STTR question. This just means someone on the STTR team will help with identifying the data from our data dictionary.   PHI: Select whether you need identified data or de-identified data   Identify how you intend to use the data, whether for research (or activities preparatory to research), clinical care, etc.   Include the IRB number related to this work. If requested, include the IRB number related to this work. You can search CORE FYI to find the appropriate IRB number.   For the question relating to the confidentiality pledge, this can be found from the link on the page, which is here. The confidentiality pledge can also be attained through a link within the electronic survey.   The form does not require specific data fields to be selected. You may write in specific data fields, or broadly describe what you want. To request specific fields found within Caisis, you may refer to the existing fields listed in the Fred Hutch Ontology Browser under CAISIS here. The submission form does NOT link to the Ontology Browser.   For the question related to the HDC PHI Data Training Verification, you can get more information here.   If you wish to submit a list of MRN’s to identify specific individuals for whom you want data, the request form allows you to upload the list, Excel format perferred.  Password protect all files with identifiable data, including lists of MRNs; the submission form will ask you to provide the password.   That’s it! A data analyst from HDC will contact you to discuss the specifics, including how to share identifiable data.   Additional Resources     REDCap: REDCap projects are flexible and secure ways to capture multiple types of data, including clinical covariates. REDCap includes version-control and access-restriction features. Fred Hutch has its own instance of REDCap and offers many training opportunities to support researchers.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/clsp_clinicalCov/",
        "teaser":null},{
        "title": "Experimental Data Management",
        "excerpt":"While clinical covariates (like age, race, or disease type) are often part of a study’s experimental design, additional covariates may be of interest that originate from laboratory based processes.  There are two main sources of laboratory based covariates that are important to track and consider when using large scale molecular data in a study:  Experimental and Process-Related.   Experimental and Process-Related Laboratory Covariates  Because large scale molecular datasets can measure so many targets simultaneously, it becomes important to record sample processing information for each specimen, so that any variables that might create a “batch effect” can be addressed in the final analysis.  This review highlights the need to include processing data in analyses, to assess the impact of batch effects on downstream results.   The following list is far from exhaustive, but here are a few important process-related covariates to consider:     time it takes to preserve specimen   method of specimen preservation   any intermediate specimen processing (e.g. cell-selection techniques)   type and length of specimen storage   specimen transport information   method of nucleic acid isolation   nucleic acid quantity and quality metrics   lot numbers of processing reagents   processing procedure dates   technician performing each procedure   Available Resources  The tools to capture laboratory covariates depend on the workflow needs.  Any type of data capture (lab notebooks, excel spreadsheets, etc.) is better than nothing; however, there are some tools that make the downstream utilization of those laboratory data easier. We have a few suggestions of platforms here at Fred Hutch:           LabMatrix: LabMatrix is a specimen tracking platform that can be used to capture clinical and laboratory covariates. One of its main strengths is the ability to capture lineage of specimen processing steps in a workflow (for example, multiple nucleic acid types could be extracted from the same tissue). More information on this system can be found on the Specimen Banking page.            REDCap: REDCap projects are flexible and secure ways to capture multiple types of data, including experimental and laboratory covariates. REDCap includes version-control and access-restriction features. Fred Hutch has its own instance of REDCap and offers many training opportunities to support researchers.       Laboratory Data Management   LabArchives Electronic Lab Notebooks is an alternative to paper lab notebooks to record and share laboratory data.  Electronic notebooks can be quite useful in capturing and collaborating on lab projects. Currently, Center IT is supporting LabArchives as a Fred Hutch ELN solution.   LabArchives (Professional Edition) electronic lab notebook software is a secure, browser-based, and collaborative research-specific workflow tool that was designed for the storage, organization, sharing, and publishing of laboratory research and data for scientific research. You can create a free trial account at LabArchives to explore more.      Account creation for Non-Fred Hutch users: Register Here   Account creation for Fred Hutch users: Register Here Register using your Hutch ID (@fredhutch.org) email and you will be able to access your secure account through your MyHutch login. the registration will route you to access the application through your MyHutch page. For additional details on Fred Hutch ELN access, please refer to additional documentation in CenterNet.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/clsp_labCov/",
        "teaser":null},{
        "title": "Overview of Clinical and Experimental Data",
        "excerpt":"For each study, the particular covariates associated with large scale data sets typically come from clinical or laboratory data. When these data are originating from human samples, certain protections need to be in place to ensure patient privacy. There are resources at the Fred Hutch which can help researchers effectively manage these data so that they can be associated with downstream molecular data sets more consistently and securely.   Clinical Data  This page includes an evolving discussion of data management systems, that are available to Fred Hutch researchers, to access and interpret clinical data for a select cohort.  The ability to identify if there are suitable biospecimens associated with that cohort available via collaboration with other researchers or via a specimen repository is critical to being able to generate large scale data sets associated with those patient phenotypes. There are multiple groups involved in the data access and management of clinically originating data, including the Hutch Data Commonwealth (HDC) and the Seattle Tumor Translational Research group (STTR).  The current recommendations and descriptions of available resources for this work will be described here.   Specimen Banking  This page contains evolving guidance for using the software for prospective specimen banking and laboratory organization of retrospectively banked human specimens.  To generate large scale data sets associated with a particular patient phenotype, the ability to identify if there are any matching biospecimens avaiable becomes critical. This page will provide resources for the Fred Hutch researchers to determine if the associated cohort available via collaboration with other Fred Hutch researchers or via a specimen repository and different approaches to effectively access the specimens.   Experimental Covariate Management Approaches  This page contains guidance for tools of particular utility to documenting, organizing and linking descriptions of laboratory based processes or experimental conditions in such a way as to provide accurate and easy linking of these data to the downstream large data sets.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/clsp_overview/",
        "teaser":null},{
        "title": "Specimen Processing and Banking",
        "excerpt":"Labmatrix For Specimen Management  Labmatrix is a web-based biospecimen management system that allows researchers to track their biospecimens and associated data in discrete fields through a relational database. STTR, with input from representatives from solid and liquid tumors, infectious disease, and other research areas developed a disease-agnostic data model that can work for any researcher using specimens. Having a standardized data model is beneficial for collaborations and downstream research. This model also provides a framework for investigators to collect the information necessary to determine whether a specimen has value in answering a research question.   Key Features  Lineage tracking of specimens and detailed audit trails allow users to have an accurate record of the life of the specimens and what was done to them at what time. Participant or patient information can be linked directly to the specimens collected from an individual at a specific point in time, along with other important information about the patient and specimen at the time of collection. Labmatrix automatically generates a unique, customizable barcode for each specimen logged and enables results storage can link out to other databases that house larger data, such as genomic testing results or staining images.   The storage module allows a lab to track the exact storage locations of their specimens, so you always know where your specimens are and the status of specimens. Labmatrix can also connect with barcode scanners and print unique, customizable barcodes for all specimens and participants. The system’s robust querying abilities also enable researchers to easily ask questions of their specimen inventory and export reports of the results. Labmatrix connects with other databases through their Rest API, which supports seamless integration of Labmatrix into the current systems used at Fred Hutch.   Getting Started  If you are looking for a new specimen management system or have questions about Labmatrix, please email STTRCancer and we are happy to further discuss Labmatrix and how to implement the system in your lab.   Costs  Labmatrix offers only single-user, named licenses at a rate of $5,000 per year per license at the time of this writing. The accounts are assigned to an individual and can only be accessed by that person. There are no floating or lab licenses. If you are interested in purchasing a Labmatrix license, you will need to send an email to STTRCancer outlining your request and including the project ID that the license can be charged to. Fred Hutch will work with Labmatrix to put in the purchase order the license and STTR can assist in setting up the environment.   Fred Hutch Specimen Processing Shared Resources  Fred Hutch researchers have access to retrospectively banked specimens through Northwest Biotrust.  Additionally, the two previous shared resources called Specimen Processing and Research Cell Bank have merged and now the group is available to provide services such as receipt and processing of biospecimens with existing or researcher-defined protocols, nucleic acid extractions, DNA fingerprinting (e.g. for confirmation of cell line identities as required now by many funding agencies), to specimen storage resources.   Available Resources     Shared Resources link to how to contact Northwest Biotrust about previously banked specimens is here.   Shared Resources link to how to contact, what services are offered and forms required for Specimen Processing/Research Cell Bank services are  here.   Impact of Specimen Processing and Handling on Downstream Data  For more information about the common types of specimen preservation, processing and handling approaches that are particularly impactful on what types of data are feasible and desirable to generate, please refer to our page on Assay Material Prep and QC.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/clsp_specimenBanking/",
        "teaser":null},{
        "title": "Assay Material Prep and QC",
        "excerpt":"Multiple processes can be involved in the isolation, quality assessment and preparation of biological materials upstream of large scale data collection. For many types of datasets, the particulars of the method of nucleic acid extraction and quality assessment process can have a large impact on the quality, relevance, and interpretability of the resulting data. Thus, it is critical to ensure that a correct set of conditions are chosen before any specimens are prepared for analysis.   The sections below contain an array of background information about isolation and quality control (QC) of nucleic acids when the downstream application is a large scale molecular data type.   Nucleic Acid Isolation Methods   There is a tradeoff between purity and yield when it comes to nucleic acid isolation. Thus choosing an appropriate nucleic acid isolation method for the sample and the data intended to be made is critical. Precipitation-based methods tend to provide a larger representation of total nucleic acid types (both large and small fragments), but can have higher levels of “contaminating” nucleic acid types that are not the intended target (e.g., miRNA in the mRNA, RNA in the genomic DNA). Selection-based isolation techniques (affinity column or magnetic-bead) tend to produce higher purity nucleic acids at the expense of yield due to their selectivity. Important considerations for which approach to isolation is desired will include the minimum amount of nucleic acid required for a given assay as well as the inherent sample preparation steps involved in generating those data. Specimen preservation types can also influence the type of nucleic acid extraction approaches that are desirable. For example, cryopreserved, viable cell suspensions tend to provide more intact nucleic acids, while FFPE tissues will tend to produce fragmented nucleic acids due solely to their preservation method, even without issues such as degradation and human error in processing.   RNA Considerations   RNA extraction kits will typically isolate total RNA (small RNAs, mRNAs, rRNAs among other types of RNA or are moiety-specific (such as mRNAs only). Depending on the downstream application, one approach may be more successful than another despite the intended target. An example is in the case of cDNA preparation for mRNA sequencing via Illumina’s TruSeq reagent set. While the final cDNA that is sequenced is enriched for mRNA, the first step in the sample preparation is an mRNA bead based isolation, which means upstream, the ideal sample would be intact, pure total RNA of sufficient quantity and isolating mRNA specifically can be redundant and can result in insufficient yields. Thus, matching isolation approaches to the downstream sample preparation and data type is important to consider prior to beginning a project.   DNA considerations   DNA extraction from samples of varying quantities and preservation methods can be limited by the fragment length of the isolated DNA and its appropriateness to the downstream sample prep steps. Choosing a method that incorporates the specimen type is important to avoid situations where sufficient yield of DNA is obtained, but the resulting fragments are so small that they are incompatible with the analysis type. For example, fragmented DNA may not provide usable data for a capture based sample preparation, such as an exome sequencing library preparation, that starts with a probe based hybridization because the selected fragments bound to the probes may not be long enough to then sequence sufficiently far to produce usable data.   Available Resources   Some summaries of extraction kits can be found at the following links:           DNA extraction - Labome overview of DNA extraction kits and applications            RNA extraction - Labome overview of RNA extraction kits and application’s       Quantification Methods   There are two main approaches to quantifying nucleic acids in solution after isolation. Both provide important information that will influence what downstream analyses are possible and suitable, but are not interchangeable, thus it is important to know what types of data describing the nucleic acids can be obtained via each method.   Spectrophotometric analysis   Spectrophotometic analysis (e.g. via NanoDrop systems is a common quantification method that allows measurement of quantity and purity of nucleic acid samples, via their absorbance of light. Total amount of nucleic acid is determined by the absorbance at 260nm. Amounts of common contaminants can also be measured with this method - remaining protein is measured via absorbance at 280nm; remaining organic solvents and salts are measured via absorbance at 230nm. Purity of the sample is assessed by calculating ratios of absorbances: 260/280 and 260/230. Nucleic acid samples are generally considered pure enough for sequencing if 260/280 &gt; 1.8 and 260/230 &gt; 1.7.      Note: Both RNA and DNA absorb light at 260nm. Spectrophotometric analysis will tell you the total amount of nucleic acid (all moieties) in your sample. Also, this analysis does not give information about the quality of the nucleic acid in the sample. To assess amounts of specific nucleic acid, use a fluorescent dye-tagging quantification (fluorimetric) method; in addition, assessment of nucleic acid quality requires electrophoresis via TapeStation (see below).    Fluorimetric analysis   Fluorometric analysis (e.g. via a Qubit) relies on fluorescent dyes that bind to specific types of nucleic acid (RNA, dsDNA, ssDNA) or protein. Fluorescence assays can thus be more sensitive than spectrophotometric assays for nucleic acid quantification, can provide quantitation of specific moieties of nucleic acids even in mixed solutions, but do not give any information about nucleic acid purity or quality. Qubit systems are often in many labs around the Hutch and consist of a base reader as well as specific assays for the different types and concentrations of nucleic acid.   Quality Assessment Methods   TapeStation   To assess nucleic acid quality, it is necessary to perform electrophoresis. This process separates nucleic acid fragments by size and charge in order to evaluate the amount of fragmentation and degradation in a sample. Often these types of analysis will also quantitate the amount of nucleic acid in the sample but does not give any information about nucleic acid purity.   For RNA samples, you can use the results of electrophoresis to calculate an RNA Integrity Number (RIN) value. RIN is a calculated ratio of 28S and 18S rRNA amounts, and is used as a metric of RNA quality. Higher RINs reflect less fragmentation and higher quality RNA; lower RINs indicate RNA degradation. The extraction method itself can have an effect on RIN values and how they should be interpreted. Phenol-chloroform (precipitation-based) extractions may result in lower RIN due to inclusion small RNA types; conversely, column-based extractions may falsely inflate RIN due to exclusion of small RNAs/degradation products in the final isolate analyzed via TapeStation. In general, RIN values &gt; 8 are considered appropriate for sequencing.   Additional Resources           Consider emailing the reagents listserv in order to find laboratories near you that have a Qubit reader.            The Fred Hutch Genomics lab members are very helpful and can provide guidance for what works best for different types of genomics assays. The Hutch Genomics Core site is here along with the relevant contact info for the group.            The Fred Hutch Genomics lab also provides access to TapeStation analysis for nucleic acids (to determine RIN for RNA or fragment sizes for DNA). Samples can be submitted for fee-for-service via Hutchbase. Check in with the Genomics lab members for specific instructions on sample concentration and volume requirements as well as assay choice (there are several different assays that can be run on the TapeStation).            For more information on RIN definition and background for why it is important to consider when using RNA as an assay material, this article provides additional information.            Here is a useful reference outlining the amount/quality of a few FFPE nucleic acid extraction kits.      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/datagen_assayPrep/",
        "teaser":null},{
        "title": "DNA Analysis Approaches",
        "excerpt":"This page contains an outline of the platforms and approaches used to produce data sets from DNA that are available through the Fred Hutch Genomics Core. Please remember that this is a general resource, and example cost estimates and read depth, etc will vary study to study. To answer study-specific questions, please contact the staff at the Genomics Core.  Be prepared to discuss the sample types, sample numbers, goal datasets, and proposed platforms of interest.   DNA Sequencing for Variant Analysis   Capture-Based Sequencing - Example:  Whole Exome Sequencing   Exome sequencing is primarily used for large scale surveying of a large portion of the genome, typically all the coding regions, or coding regions plus UTRs, etc.  Other types of capture based sequencing panels exist that target more specific regions in the genome such as commonly mutated regions in cancer.   Regardless of the region of interest, the DNA libraries sequenced are generated by using capture probes specific to the particular regions covered in the panel to enrich for these regions in the final DNA sequences.  In most approaches, the capture uses pools of oligonucleotides (or probes) bound to streptavidin magnetic beads to selectively hybridize exon regions. After binding to genomic DNA, these probes are pulled down and washed, allowing exon regions to be selectively sequenced.   To perform exome or other capture based sequencing, multiple details should be taken into consideration.  These issues can be discussed in more detail in consultation with the Fred Hutch Genomics Core staff.           Region of coverage:                       Manufacturers often have multiple types of capture probe panels that cover different regions of the genome.  Choose one that covers the regions of most interest to the study - not all “exomes” are the same set of genomic regions!                                   Examples include Agilent selling SureSelect Human exome kits for All Exons alone, OR plus UTRs, or COSMIC regions, or other custom additions.                                    Illumina’s TruSeq DNA Exome kit will cover slightly different regions than the Agilent kits as well.   Genomics Core currently doesn’t offer this workflow.                                    NimbleGen SeqCap EZ cover offers  custom capture baits, Exome, and other bait sets.                                            Number of reactions per kit:                       Because reagents are bought in kits, the unused reactions will increase your total cost even though they may not be used for your study.                      For example the Agilent Human All Exon V6 kit is sold in groups of 16 or 96, while the Illumina TruSeq DNA Exome kit is sold in groups of 24 or 96.  NimbleGen is sold in multiple configurations and pricing tiers accordingly.                                Multiplexing the capture, library preparation and sequencing:                       The particular reagent kit chosen may have the option to multiplex the capture, reducing per-sample costs.                        The generation of a library also can be multiplexed and the particular barcode sequences used to identify the reads from different samples will need to be chosen in accordance with how they will be sequenced.                        Sequencing options on the HiSeq 2500 include Rapid Run mode (2 lanes only) and High Output mode (8 lanes).  How the libraries are split over these lanes can impact the per-sample costs, time-to-sequencing, and the upstream capture and library preparation steps.                        Input amount:                       Typically exome sequencing will require 50ng to 1ug of input DNA.                        Choosing a Low Input kit can have downstream sequencing quality implications, but is an option when input quantities are insufficient for a standard Input kit.                        Mean Read Depth/Coverage:              A mean read depth of 100X over exome regions is a fairly standard goal depth, and using Illumina’s Sequencing Calculator tool linked below in Available Resources is useful in understanding how multiplexing and sequencing layout impact sequencing depth and cost.                Costs:              While exomes are less expensive to generate than whole genomes, due to the moderate coverage of the genome of interest, sequencing costs are higher than for more targeted panels.           Amplicon Based Sequencing - Targeted DNA sequencing   As an alternative approach to exome sequencing, targeted sequencing panels can offer high read depth of specific genomic regions; available panels often focus on known genes of interest for a disease, actionable target genes, or genomic regions known to have prognostic significance.  Due to DNA secondary structure and PCR amplification efficiency, not all targeted regions will have the same coverage and this variability is larger than what is seen in capture based sequencing approaches. To ensure that all target regions have high enough coverage to confidently identify variants, each manufacturer will likely provide a statistic such as “ &gt; 500× coverage for &gt; 95% of amplicons at &gt; 5,000× average sequencing depth.”  This means that &lt;5% of the amplicons in the panel may have &lt;500x coverage in the final dataset even if the entire library is sequenced at 5,000x mean depth.  Following the manufacturer’s instructions for deciding on the goal sequencing depth for targeted DNA sequencing is particularly important for the data to be as complete as possible.   This tech note from Illumina describes how targeted DNA sequencing panels are designed and sequenced to maximize the data quality and coverage when used.  Understanding how these panels perform as compared to whole genome or whole exome sequencing is valuable in identifying whether the data are likely to be useful for the study goals.           Region of coverage:              There are a wide variety of different types of regions that can be targeted with panels, such as the TruSight panels from Illumina.  Focusing in on the most interesting regions can allow for larger sample sizes due to cost reductions.                Number of reactions per kit:              Because reagents are bought in kits, the unused reactions will increase your total cost per sample even though they may not be used for your study.  Often panels come in groups of 24, 36 or 96                Sequencing considerations:                       If the panel area is relatively small (~kb’s not ~Mbs+), depending on the desired average read depth, it is sometimes possible to sequence targeted libraries on the MiSeq.                        Sequencing options on the HiSeq 2500 include Rapid Run mode (2 lanes only), or the High Output mode (8 lanes) and if enough samples are being sequenced, it may be feasible and cost effective to put the entire cohort onto a number of lanes on a HiSeq.                        Input amount:                       Typically targeted DNA sequencing will require less input DNA, on the order of 50ng to 1ug of input.                        Choosing a lower input requirement kit has implications downstream and for your data quality, but conversely it may not be feasible to get reliable data from all the samples in a cohort.                        Mean Read Depth/Coverage:              Read depth for targeted panels is very dependent on the manufacturer and the particular panel of interest.  The panels themselves should come with recommended target sequencing depths and provide statistics about how many regions should be above some minimal threshold of read depth.  Choosing a target depth will depend on the study goals as well.                Costs:                       Targeted panels provide lower cost data, but at the expense of genomic ranges which depending on the study goals, can be no great loss and provide the opportunity to get data from a much higher sample number.                        The costs for these vary greatly based on the size of the covered region in the panel because this increases the sequencing costs of the libraries.                   SMRT sequencing (PacBio)   Both exome and targeted approaches rely on short length sequencing reads that are computational reconstructed into a longer sequence file. PacBio’s Single Molecule Real-time (SMRT) is an alternative to the short-read approach. SMRT offers single reads of 1-20kb of sequence, for sequencing the entire length of a single gene or a large genomic region.   Human Whole Genome Sequencing (WGS) Guidance  More to come about approaches to obtain and evaluate the logistics involved in using whole genome sequencing data in a research study.   Does My Research Questions Require WGS?     For Inherited variants WGS may be the right choice for you if your scientific question requires a comprehensive assessment of  inherited genome-wide constitutive copy number variants (CNVs), chromosomal structural variants (SVs), or you need to assess simple variation such as single nucleotide variants (SNVs) or small insertion/deletions (indels) that is not typically captured using standard SNP array approaches. However, it is important to have a sufficiently large participant sample size given the every that each individual will have  unique variants that differ from the human reference genome. Additional, there is substantial cost associated with generating, analyzing, and storing WGS data.  Contact the Reid Lab or the Peters Lab  for Hutch teams with experience with WGS.    For Somatic Variants there are several advantages to measuring somatic genetic variation using WGS, especially for cancer research. SNVs, indels, somatic CNVs (SCNAs), aneuploidy, whole genome doubling (WGD) and chromosomal structural variations are ubiquitous in cancer and there are different pros and cons of different WGS approaches for each variant type.    Selecting A WGS Vendor  There are many vendors available to generate WGS data.  Things to consider when comparing WGS vendors include the obvious such as cost, input DNA requirements, deliverables, technology, quality control steps, data security and data delivery methods, methods of communication between vendor and customer, customer service after sequence data is delivered, and turn-around time. In addition, the largest resource requirements will be at the level of analytical pipeline implementation and variant analysis. Many vendors offer tiered levels of service that go beyond providing sequencing to include informatic and computational support. Consider whether you simply need sequencing fee for service or whether your project would require a more collaborative model.   Important Considerations for WGS Analysis and Data Storage    Alternative Methods to bulk WGS    Available Resources           Genohub has a useful guide to exome sequencing approaches. The comparison chart of exome library preparations is of particular note.  Genohub’s Guide to Exome Sequencing            Illumina has several exome sequencing options. Illumina Whole Exome Sequencing            Agilent also has several exome sequencing options, including a more clinically focused option. Agilent Exome Sequencing             Roche NimbleGen has several exome and other bait sets. Roche NimbleGen Exome Sequencing            Illumina has a variety of ready-made targeted DNA sequencing panels. Users can also design custom panels.Illumina Targeted Sequencing Panels            Illumina has a sequencing depth calculator tool to estimate what type of depth to expect in certain combinations of setups.  More information on the available sequencing platforms in the Genomics Shared Resource can be found on this page. Illumina Sequencing Calculator            PacBio Single Molecule Real-time (SMRT) technology enables long-read (&gt;20kb) sequencing. PacBio SMRT Sequencing       DNA Microarrays   For quantitative studies of known targets, microarrays can be a very useful alternative to sequencing. The Genomics Core offers SNP and methylation arrays.   SNP Genotyping Arrays   More to come.   Illumina Methylation Microarrays   Methylation Arrays   More to come.   Illumina Genotyping Microarrays   DNA Sequencing for other analysis types   Another use of DNA sequencing is as part of another analysis involving upstream processing of the DNA in order to then interpret the DNA Sequencing results as part of an experimental condition such as looking at epigenetics or DNA-protein interactions.  These types of projects are even more study-specific but can be performed with the Fred Hutch Genomics Core as well.   Chromatin-immunoprecipitation Sequencing (ChIP-seq)   ChIP-seq combines chromatin immunoprecipitation (ChIP) with massively parallel DNA sequencing to identify the binding sites of DNA-associated proteins.   Reduced representation bisulfite sequencing (RRBS)   RRBS uses a combination of CpG island-specific restriction enzymes with bisulfite sequencing to enrich for methylation sites before sequencing.   Available Resources           Epigenie has a useful guide to getting started with ChIP-seq. Epigenie’s Guide to ChIP-seq            The genomics core uses the Kapa Hyper Prep kit for ChiP-seq. Kapa Hyper Prep for ChIP-seq            The Babraham Institute has a brief guide to the biology and bioinformatics of RRBS. Babraham Institute’s Guide to RRBS      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/datagen_dnaApproaches/",
        "teaser":null},{
        "title": "Guidance for Flow Cytometry Data",
        "excerpt":"This page provides guidance for Fred Hutch investigators generating or using flow cytometry data, and who may be using the Fred Hutch Flow Cytometry Shared Resource.      Note: This page is in development, please comment on this issue in GitHub if you’d like to see specific content be added or would like to add content yourself!  If you are not a GitHub user yet, please email sciwiki with your input.    Instrument configurations   For current instrument configurations available in the Fred Hutch Flow Cytometry Core, see these pdfs:     Aria 2 (1)   Aria 2 (2)   Aria 2 (3)   Canto 2 (1)   Canto 2 (2)   Celesta   LSR 2   Symphony   X-50   AutoMACS   CyTOF   Startup Instructions  For current startup and shut down instructions for the Aria, Canto and LSR, see our Resource Library entry.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/datagen_flowcytometry/",
        "teaser":null},{
        "title": "Guidance for Imaging Data",
        "excerpt":"This page provides guidance for Fred Hutch investigators generating or using imaging data, and who may be using the Fred Hutch Cellular Imaging Shared Resource.      Note: This page is in development, please comment on this issue in GitHub if you’d like to see specific content be added or would like to add content yourself!  If you are not a GitHub user yet, please email sciwiki with your input.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/datagen_imaging/",
        "teaser":null},{
        "title": "Overview of Large Scale Data Generation",
        "excerpt":"There are many forms of large scale data generated here at the Fred Hutch and each has it’s unique set of considerations.  We have endeavored to begin curating data-type specific guidance from around the Fred Hutch in this section.   Genomics Data  The Fred Hutch Genomics Shared Resource offers researchers, both at the Fred Hutch and externally, access to a wide range of genomics platforms in a fee for service context often accessible via Hutchbase.  Services that are available range from study planning, nucleic acid quality assessment, to sample or library preparation, to data generation and basic bioinformatics. This page provides an overview of the currently available platforms, the types of support available through Genomics, strengths and weaknesses of each technology, and guidance for who to contact for consulting about your specific project.  The Fred Hutch Genomics Shared Resource is a valuable wealth of knowledge about all types of genomic assays and are a first stop for more detailed experimental design advice and support for particular projects.   Flow Cytometry Data   Imaging Data   Proteomics Data  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/datagen_overview/",
        "teaser":null},{
        "title": "Genomics Platforms and Data Types",
        "excerpt":"This guide highlights some of the genomics platforms available through the Genomics Shared Resource at Fred Hutch.  This guide is intended to give general context to each platform.  Access to many of the submission processes involved in using the Genomics Shared Resource is via Hutchbase.      Note:  As technologies and reagents change, the relative costs of performing these experiments do as well.  It is important to discuss your particular experiment and needs with the Genomics and Bioinformatics Shared Resource during the planning stages of your project.  You can contact them by emailing genomics.    Sequencing Based Platforms  Sequencing based platforms currently available via the Genomics Core at the Fred Hutch include the Illumina sequencers (HiSeq/MiSeq) and the Pacific Biosciences Long Range sequencer.  These platforms can be used to sequence a variety of assay material types via different library preparation processes. In the RNA or DNA Approaches pages, we discuss different options for the creation of libraries for sequencing from either nucleic acid type and for different research questions.  Choosing the appropriate assay material QC and library preparation reagents depends in part on how the libraries will be sequenced.  Thus is it important to verify that all phases of the process are using compatible techniques.   Illumina Sequencers  The Illumina sequencers are high read number, short read sequencers that provide a range of sequencing capabilities for many different upstream library types. The primary approach of these sequencers is to sequence at high numbers, individual fragments of DNA generated by the library preparation process, then to reconstruct the sequences in the mixture by, for example, aligning the sequences of the short reads to a reference, or other bioinformatic approach. The Illumina HiSeq at the Fred Hutch currently has two options:  the High Output mode or the Rapid Run mode.  The basic details for each mode along with the corresponding information for the MiSeq are included below.  We have provided cost ranges to provide a basic idea of the relative costs of these sequencing platforms, but the exact costs of a sequencing run will depend on the read lengths, whether the sequencing is paired end or single end, as well as your affiliation (Fred Hutch vs external).                  Sequencer       Mode       Read Lengths       Approx. Reads per Lane       Lanes per Run                       NovaSeq 6000       Depends on Chip Type       50-150, depending on Chip Type       Variable       Variable                 HiSeq 2500       High Output       50, 100, 125       250M       8*                 HiSeq 2500       Rapid Run       50, 100, 150, 200, 250       150M       2                 MiSeq       Version 2 Reagents       up to 250       15M       1                 MiSeq       Version 3 Reagents       up to 300       25M       1           * High Output mode requires all 8 lanes to be run simultaneously, no partial runs.   When deciding how much sequencing is needed for a set of libraries to provide sufficient read depth (number of reads per genomic location in the genome covered in the library), issues such as the intended data type, sample type and quality, library preparation type, number of total samples, and the applicability of multiplexing approaches need to be considered.  Consulting with the Genomics Core can help provide more clarity for individual projects.   More from Illumina about Illumina Sequencing can be found here.   Pacific Biosciences (PacBio) Long Range Sequencer  The PacBio SMRT sequencer works differently than the Illumina sequencers in that the read length is not specified by the platform, but is limited by the library itself, with an associated reduction in confidence of the sequence as reads get longer and longer.  However, instead of being limited to sequencing only fragments of DNA, PacBio sequencing can provide long stretches of sequencing data that occur in the same fragment.  This allow for analyses such as full length isoform discovery, de novo small genome sequencing, assessing structural variants/translocations, and allele phasing.  On average the PacBio sequencer aims to provide up to 15kb of read length.      300-600k reads per SMRT cell   Insert size for sequencing of 200bp minimum up to 40kb fragments   Library prep            Reagents purchased by lab from PacBio and prepped by lab, brought to Genomics as completed library (see PacBio website for more info)       OR       Genomics Shared Resources can help with library prep for a service fee to process a QC’d sample into a Pac Bio library depending on library type:                    For amplicon library prep up to 5kb           For large insert library prep up to 20kb                           Multiple multiplexing schemas (in-line or ligated) - discuss details with Genomics Shared Resource to plan the approach (email genomics).   More about PacBio SMRT Sequencing.   Array Based Platforms  Microarrays are a sometimes less costly option that can in some cases be substituted for a wide variety of sequencing types; for example, there are SNP, gene expression, and whole exome arrays. While microarrays are not useful for discovery of novel targets, for well-established targets, assay chemistries and data analysis pipelines are well-vetted.  A discussion with the Genomics Core can be useful in helping you decide the best technologies for your work.   Single Nucleotide Polymorphism (SNP) Arrays or Methylation Arrays  The Genomics lab is equipped to run all Illumina genotyping and methylation beadchips (Illumina Microarrays).  Beadchip kits are sold in a variety of sample size kits and it is important to plan total sample sizes as well as randomizing batches to minimize batch-specific bias. Beadchip kits need to be purchased by the investigator and should be drop-shipped to the Genomics lab.  The Genomics lab then charges a fee for processing which covers all non-Illumina supplies, reagents, and labor.  Investigators should plan to provide genomic DNA for genotyping arrays, which has been quantified by a dsDNA specific method such as picogreen.  For methylation arrays, investigators may submit either genomic DNA or bisulfite converted DNA.  Again, the starting DNA should be quantified by picogreen.   Genotyping BeadChip kits vary in price ($50/sample - $600/sample), depending on content.  The Genomics Shared Resource can process samples to run for a service fee, regardless of the beadchip kit with an additional fee for DNA samples that require bisulfite conversion for methylation arrays.   Nanostring Hybridization Arrays for Gene Expression  Reagents for Nanostring arrays can be purchased from Nanostring and total RNA ready to be run can be brought to the Genomics Shared Resource for processing.   Library Preparation Reagents and Methods  The choice of genomics platform will dictate the needs of the library preparation method.  It is important to understand how library preparation can impact the final data, for example if biases towards detection of specific types of nucleic acids are introduced.  Meeting with an NGS specialist at the Genomics Core will help to guide your path.   Library Preparation for Sequencing  The four main steps in preparing RNA or DNA for NGS analysis are:      fragmenting and/or sizing the target sequences to a desired length (via physical, enzymatic, and chemical methods)   converting target to double-stranded DNA (if RNA)   attaching oligonucleotide adapters to the ends of target fragments–these adaptors are multi-purpose. They are used to index/barcode the fragments, and allow the fragments to be attached to the flow cell of the sequencer (where the sequencing of the fragments occurs)   quantitating the final library product for sequencing   10x Genomics Single Cell Library Preparation System  To obtain single cell gene expression data from RNA-seq, the Genomics lab uses the 10X Genomics Single Cell Expression platform.  Starting with a cell suspension, this process partitions cells into droplets for cDNA library preparation.  After library prep, the droplets are pooled, then sequenced on an Illumina sequencer. Unique molecular identifiers used in the library prep allow the sequencing results to be computationally traced back to individual cells.   Overview of the Sequencing Process     The adapter-ligated DNA library is loaded onto a flowcell.   The fragments are hybridized to the flow cell surface.   Each bound fragment is amplified into a cluster. This step is known as bridge amplification.   Fluorescently-labeled nucleotides and sequencing reagents are added; the flow cell is fluorescently imaged after the incorporation of each nucleotide. The color of the fluorescent dyes identifies which base was incorporated. This cycle of nucleotide incorporation and imaging is repeated n times for a n-reads of sequence.   Available Resources     The Hutch Genomics Shared Resource: The genomics core is VERY helpful if you need guidance about reagent and platform choice for your samples.  Email genomics to discuss your particular project.   Submission of some types of samples for services to the Hutch Genomics Shared Resource is via Hutchbase.   Genohub has a compendium of library prep kits, organized by NGS application type here.   Illumina has a few interactive methods guides to help you find the most appropriate library prep reagents and sequencing methods to use in your experiments.   A helpful reference in considering library prep methods is Ordoukhanian’s 2014 paper.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/datagen_platformsData/",
        "teaser":null},{
        "title": "Proteomic Data and Methods",
        "excerpt":"Proteomics refers to a collection of methods used to discover and quantify the abundance and state of proteins in cells and tissue. Given that proteins are the primary drivers of cellular function, understanding which proteins are present and how they relate to one another can provide powerful insight into cell and systems biology. Likewise, analyzing proteins as biomarkers has proven useful for diagnosing disease and stratifying patients for personalized medicine.   The most basic division among proteomics experiments is the use of top-down versus bottom-up approaches. Top-down approaches, such as 2D gel electrophoresis, attempt to quantify entire proteins. In contrast, bottom-up approaches break proteins into small fragments, often proteolytic peptides, and algorithmically reconstruct protein identities.   A second important distinction is between “discovery” and “targeted” proteomic methods. Discovery proteomics addresses global changes in the relative abundance of multiple proteins; whereas targeted proteomics precisely quantifies a priori selected proteins or protein isoforms, e.g. phosphorylated proteins.   Data Acquisition  This article primarily focuses on data acquisition using mass spectrometry (MS). MS-based approaches are the most widely used in the research community and the technology is rapidly evolving.   Discovery approaches  Likely, the most commonly used discovery proteomics method is so called “shotgun” proteomics which is used to identify multiple proteins in a mixture of unknown composition, e.g. cell lysate. The protein mixture is digested with proteolytic enzymes, typically trypsin. The resulting mixture of peptides is separated according to their relative hydrophobicity using reverse phase HPLC and sequentially eluted directly into a mass spectrometer. In the mass spectrometer, the vaporized peptides are ionized and focused through a series of electromagnetic chambers (traps). In a typical MS-MS-type experiment, peptides are electromagnetically separated according to their mass charge ratio (m/z) and ion intensity is quantified, e.g. by a PMT. Thus, the signal is recorded as a three dimensional chromatogram of ion intensity, peptide m/z, and elution time (MS1 scan). The acquisition software selects the most abundant ions and then refills the trap with peptides in a selected window of m/z. The peptides are bombarded with energized gas to randomly fragment the peptide. Since proteins bonds are weakest between amino acids, this creates a series of fragment peptides of varying length. The intensity of the fragment ions is then quantified (MS2). The data from the MS2 scans are used to determine the identity of the peptide, typically by matching the m/z profile of the various fragment ions to the expected fragmentation patterns from an in silico digested library of proteins (see data analysis below). Peptide abundance is generally calculated using the area under the curve for the elution profile of the precursor peptide in the MS1 scan. This acquisition scheme is called “data-dependent” because the data from the MS1 scan is used to determine which peptides are analyzed in the MS2 scan.   Targeted Approaches  Targeted proteomics or data-independent acquisition takes a somewhat different approach where individual peptides are selected ahead of time (often from separate preliminary discovery proteomics experiments). While the starting material and separation methods are similar, the instrument is programmed to look for peptides with a particular m/z. Synthetic peptides matching the peptide of interest are spiked into the sample for use as an internal reference standard. Peptide abundance is calculated from MS1 data.  More about targeted approaches to come.   Non-Mass Spectrometry Based Methods  In addition to mass spectrometry, other methods such as antibody microarrays and multidimensional polyacrylamide gel electrophoresis can be used to describe and quantify complex protein mixtures. These techniques are typically lower cost and may be better (particularly relative to shotgun proteomics) at quantifying proteins with relatively low abundance. However, targeted MS methods are largely supplanting these older methods except in specialized cases.   Sample Preparation     More content to come!       Whole cell proteomics   Analysis of post-translational modifications   Kinase profiling   Identification of protein-protein interactions   Protein identification   Instrumentation     More content to come!       Ion trap   Triple quad   Hybrid   Data Analysis     More content to come!    Software  A wide variety of commercial and open source software are available. Some of the more commonly used packages are list below:      Mascot   Proteome Discover   Protein Pilot   SEQUEST Sorcerer   Comet   MaxQuant/Andromeda   X!tandem   Skyline   Available Resources     FH Shared Resources page   UW Proteomics Resource   UW Genome Sciences, see MacCoss, Bruce, and Villen labs   Pacific Northwest Mass Spec Group  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/datagen_proteomics/",
        "teaser":null},{
        "title": "RNA Analysis Approaches",
        "excerpt":"This page contains an outline of the platforms and approaches used to produce various types of large scale data sets from RNA that are available through the Fred Hutch Genomics Core. Please remember that this is a general resource and example cost estimates and read depth, etc will vary study to study. To answer any study-specific questions, please contact the staff at the Genomics Shared Resource.  Be prepared to describe study details including sample types, numbers, goal datasets, and proposed platforms of interest.   Quantitative Approaches: Gene Expression  Often, researchers want to answer what seems to be a very simple question–what is different between the treatment groups?  In an RNA context, this approach is called differential gene expression (DGE). DGE quantitatively measures differences in the levels of gene expression between two or more treatments or groups.   RNA Sequencing  RNA sequencing is a common approach to assess DGE.  Simply stated, this involves comparing the counts for sequencing reads that come from each transcript. In practice, however, this involves a complex combination of RNA extraction, amplification, sequencing to get the raw sequence data, followed by a series of bioinformatic analysis steps to map those sequence data to regions of the transcript, then comparing the counts in each transcript region between treatment groups.   An important point to remember: the data output of RNA sequencing is a set of fastq files, which are files that contain the sequence and read quality information for sequencing reads from each sample.  In order to get fully processed DGE results, there is a significant amount of bioinformatic support needed, as well as statistical support to assess gene expression differences among groups.   Nanostring Gene Expression Panels  DGE is not exclusive to sequencing techniques.  Nanostring nCounter systems utilize nucleic acid probes, attached to fluorescent barcodes. The probes hybridize to single RNA molecules in each sample, then the fluorescent barcodes are counted to quantify expression of each target.   From Nanostring’s website: “Each color-coded barcode represents a single target molecule. Barcodes hybridize directly to your target molecules and can be individually counted without the need for amplification—providing sensitive digital data.”   The analysis of the resulting count data is processed via Nanostring’s proprietary software.  Thus, Nanostring data requires little to no bioinformatic support, but still requires statistical analysis to assess gene expression differences among groups.   Available Resources          Illumina Differential Gene Expression Approaches            Nanostring has a selection of curated gene expression panels, as well as custom panels. Nanostring Gene Expression Panels       Qualitative Approaches: Transcript and Splicing Discovery  The goal of a qualitative approach is to identify (or annotate) genes and gene structures.  Here, the most important parameter is that the sequence reads evenly cover each transcript, including both ends.  One only needs to sequence at a depth that ensures all target areas are represented in the sequencing output.   Qualitative approaches allow the discovery of novel transcripts and alternatives splicing events.  But, to be clear:the design of such experiments will most likely be different from those aimed at quantification of transcripts. For novel transcripts and splicing, longer reads are often favored, at lower read depth.  This increases the probability  that reads will contain the alternative splicing sites.   The raw files produced from this type of sequencing are similar to those made in quantitative analysis. However, bioinformatic analyses for transcript and splicing discovery will differ from quantitative analyses.   Other RNA Based Research Approaches   Example:  Small RNAs (e.g., miRNA)   The workflows necessary for small RNA sequencing often differ from other RNA sequencing protocols in a few ways:     RNA is isolated using methods that either retain or enrich small RNA molecule content. Often, total RNA isolation protocols (e.g.  phenol-chloroform isolation) or small RNA-specific column extractions.   Small RNAs are size-fractionated from total RNA, using either gel electrophoresis or size exclusion columns.  This step is sometimes performed as part of the library preparation.   Sequencing adapters are ligated onto the small RNAs, to allow library prep and sequencing.  There are varying methods for this ligation (e.g. RNA polyadenylation or T4 RNA ligase2 adapter ligation), depending on the library prep method.   The adapter-ligated small RNAs are amplified, during the process of library prep. Sometimes the size fractionation step follows this amplification.   NGS Sequencing is done, and specialized read alignment and bioinformatic approaches will be required.   Available Resources     Genohub (a general resource for NGS information) has a short guide to special considerations for small RNA sequencing. Genohub’s Guide to Small RNA sequencing  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/datagen_rnaApproaches/",
        "teaser":null},{
        "title": "Data Generation Overview",
        "excerpt":"“The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.” - John Tukey   “If you torture the data, it will confess to anything” - Ronald Coase.   Upfront consideration to the design and execution of large scale data generating research projects can help to prevent unfortunate or costly outcomes.  This section provides guidance on and resources related to study design, human subjects research, management of clinical and experimental data, and a review of factors to consider when choosing from some of the common large scale molecular data generating platforms.   Study Design and Funding  This section provides guidance for researchers looking to develop a hypothesis that will have reasonable statistical power, identify the appropriate set of samples, and execute a large scale data production from those samples.  There are the two general types of studies using large scale molecular data sets, which can loosely be thought of as “investigative” and “comparative.”  The two aren’t completely extricable and can each can serve as groundwork for future experiments for the other.  The process to perform these types of studies, however, can be very different.  The details specific to each study type are best addressed prior to generating materials or data sets.      Proposals and Funding   Hypothesis Development and Big Molecular Data   Multiple Testing   Consent, Privacy and Security  The policies and processes that relate to the human subject components of any large scale data generating or analyzing study are continually evolving as new issues arise and become more clear.  Keeping up with the particular issues that do or do not apply to a given research project can sometimes be a challenge, and these pages contain relevant guidance and links to the necessary information researchers need before, during and after a research project involving human specimens or data.      Consenting and Big Molecular Data   Data Privacy and Security   Compliance and Legal Agreements   De-identification   Data Sharing and Public Repositories   Clinical and Experimental Data  For a each study, the particular covariates associated with large scale data sets typically come from clinical or laboratory data. When these data are originating from human samples, certain protections need to be in place to ensure patient privacy.  There are resources at the Fred Hutch which can help researchers effectively manage these data so that they can be associated with downstream molecular data sets more consistently and securely.      Clinical Data   Specimen Banking   Experimental Data   Large-Scale Data Generation  The decisions required when generating large scale data sets themselves are informed by an understanding of the specimen cohort, any limitations imposed by the consent of the patients from which those specimens were obtained, and the specific hypothesis the researcher is intending to address.  This section contains guidance about generating or handling large scale data from a variety of sources, highlights the particularites of each, and include information for researchers interacting with various Fred Hutch Shared Resources.      Genomics Data   Flow Cytometry Data   Imaging Data   Proteomics Data   Other Laboratory Resources  Some of the challenges associated with executing large scale data projects can be mitigated by implementing good laboratory management and data systems before beginning the project.  These sections relate to the particular systems available to Fred Hutch researchers that help them manage data generated in laboratories and also specific types of guidance about good laboratory practices with large scale data generation in mind.      Assay Material Prep and QC   DNA-Based Approaches   RNA-Based Approaches   Support Software  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/gen_index/",
        "teaser":null},{
        "title": "Consenting and Large Scale Data",
        "excerpt":"Before beginning a study and during the proposal preparation process, an important issue to consider is whether the proposed research qualifies as human subjects research as defined by the Department of Health and Human Services Office for Human Research Protections (OHRP) and/or the National Institutes of Health. Take a look at the OHRP decision charts and/or the NIH questionnaire to find out what aspects of human subjects research may apply.   IRB Approval in Human Subjects Research   Institutional Review Boards (IRBs) exist to protect the rights and welfare of human research subjects. IRB oversight supports compliance with the current standards of human subjects research and with current regulations. When conducting its review for genomic research, the IRB will look at whether the research involves human subjects.   When starting a research project that may involve the generation of large-scale genetic or other large-scale molecular datasets, a first step is to ensure that the consent forms involved in the study or studies during which the intended specimens were collected include language specific to the possibility of generating these types of data and how those data can be used and shared.   In some cases, the use of human specimens to generate large-scale molecular datasets is not considered human subjects research, and thus not subject to the specific requirements of human subjects research, even though the dataset uses human data. This distinction allows studies which use human specimens/data and deemed not human subjects research to avoid the relatively high level of documentation and reporting requirements of human subjects research.   For NIH grant applications submitted after January 25, 2018, a new required form allows for more clarity if a study does not qualify as human subjects, but does use human specimens and/or data. More about this form, the PHS Human Subject and Clinical Trial Information form can be found here. This form requires the investigator to have reviewed the NIH questionnaire; if the study does involve human specimens and/or data, but is not deemed human subjects research, additional documentation or justification is required. An example of the Research Involving Private Information or Biological Specimens flowchart is here.   Available Resources           If you are affiliated with the the Fred Hutchinson Cancer Research Center or Seattle Cancer Care Alliance, a good place to start is the Fred Hutch IRB.            Similarly, if you work for the University of Washington, the UW IRB curates that information.            If you currently have an NIH grant, are considering applying for one, or are in the process of writing one, consider taking a look at the NIH’s human subjects research site.            For more information about using human specimens, cell lines or data in the context of a non-human subjects study, here is a pdf of a decision tree provided by the NIH.       Retrospectively Banked Specimens and IRB Review   It is important to be aware that the timeframe in which the specimens were banked may affect the IRB review of the foundational collection consents. On January 25, 2015, NIH policy for viewing and sharing genomic data changed. Consent documents associated with human specimens banked before this date will have different (and fewer) IRB review criteria than consent documents associated with human specimens banked after this date. For data made from human specimens banked after this date, the patient consent documents will be required to address broad sharing in order for broad data sharing to occur. Sharing may be possible for specimens banked earlier for which the consent documents may be ambiguous with respect to genomic datasets. It is important to consult with the relevant IRB if a consent with sharing requirements can limit the types of data which can be generated and if such a consent can restrict secondary usage or sharing of generated data.   NIH Genomic Data Sharing and Informed Consent   If you are working under an NIH grant and sharing data that may fall under the NIH Genomic Data Sharing (GDS) policy, you should be aware of whether the data you are receiving was collected under appropriate consent. The GDS Policy expects subjects who are asked to enroll in a study in which genomic data are obtained to also be asked for their informed consent to the future research use and broad sharing of their data. Only if potential subjects provide such consent would broad sharing of the data be permissible. If a subject does not consent, he or she may still be enrolled in the study, but their data may not be shared, or may be shared in a limited manner consistent with the specifics of the consent form. The IRB of the entity sharing the data will make a determination about what can be shared and any limitations.   In order to meet the NIH expectations under the GDS Policy, for research projects for which the IRB has granted a waiver of some or all of the required elements of informed consent under 45 CFR 46.116(d), or consent is not required because the activity is not subject to 45 CFR 46, investigators will still need to seek or document consent for future use and broad sharing of genomic and phenotypic data. At minimum, the information described below should be provided to prospective participants.   In order to meet the expectations for future research use and broad sharing under the GDS Policy, the consent should capture and convey in language understandable to prospective participants information along the following lines:           Genomic and phenotypic data and any other data relevant for the  study (such as exposure or disease status) will be generated and  may be used for future research on any topic and shared broadly in  a manner consistent with the consent and all applicable federal  and state laws and regulations.            Prior to submitting the data to an NIH-designated data repository,  data will be stripped of identifiers such as name, address,  account and other identification numbers and will be de-identified  by standards consistent with the Common Rule. Safeguards to  protect the data according to Federal standards for information  protection will be implemented.            Access to de-identified participant data will be controlled, unless  participants explicitly consent to allow unrestricted access to  and use of their data for any purpose. Because it may be possible  to re-identify de-identified genomic data, even if access to data  is controlled and data security standards are met, confidentiality  cannot be guaranteed, and re-identified data could potentially be  used to discriminate against or stigmatize participants, their  families, or groups. In addition, there may be unknown risks.            No direct benefits to participants are expected from any secondary  research that may be conducted.            Participants may withdraw consent for research use of genomic or  phenotypic data at any time without penalty or loss of benefits to  which the participant is otherwise entitled. In this event, data  will be withdrawn from any repository, if possible, but data  already distributed for research use will not be retrieved.            The name and contact information of an individual who is affiliated  with the institution and familiar with the research and will be  available to address participant questions.            Studies that include whole genome sequencing (WGS), whole exome sequencing (WES), epigenetic profiles, microbiotic profiles, and related forms of in-depth extra-genomic data generate immense  amounts of personal information about participants. It is  important to draw a distinction between targeted genetic research  and broader sequencing protocols, so that participants understand  the scope of data generation.       Available Resources           See also National Institutes of Health Points to Consider in Developing Effective Data Use Limitation Statements, prepared by the Office of Science Policy July 13, 2015).            The national Human Genome Research Institute has a page for  Informed Consent for Genomic Research.            The NIH Genomic Data Sharing Policy is  here.            Large-scale genomic data include genome-wide association studies  (GWAS), single nucleotide polymorphisms (SNP) arrays, and genome  sequence, transcriptomic, metagenomic, epigenomic, and gene  expression data. Examples of research that are subject to the GDS  Policy include, but are not limited to, projects that involve  generating the whole genome sequence data for more than one gene  from more than 1,000 individuals, or analyzing 300,000 or more  genetic variants in more than 1,000 individuals, or sequencing  more than a 100 isolates of infectious organisms such as bacteria.  The Supplemental Information to the NIH Genomic Data Sharing  Policy  includes detailed description of research under scope of the  policy and data submission expectations.            The Fred Hutch IRB policy information can be found here, sign-in required if off campus,  including the Genomic Data Sharing Supplement here.            NIH Information on Institutional Certifications is  here.  NOTE: Fred Hutch cannot issue a provisional or final  “Institutional Certificate” unless Fred Hutch is uploading the  final genomic data.      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/human_consentIRB/",
        "teaser":null},{
        "title": "De-identification of Specimens and Data",
        "excerpt":"What is de-identification?   De-identification refers to the removal or dissociation of direct patient identifiers from a research specimen or data set in order to inhibit the ability to deduce an individual identity. Ideally after de-identification, it would not be possible to use any remaining information alone or in combination with other readily available information to identify the subject from which the data originated. Furthermore, Human Subjects Protection dictates the identifiers of source subjects cannot be readily ascertained or otherwise associated with the data by the research staff or secondary data users (45 CFR 46.102(f)). The goal of de-identification is to reduce, to the greatest extent possible, the risk of identifying individuals from which specimens are obtained or associated genomic datasets are generated. In the setting of genomics, this would include considerations for the condition where genomic datasets generated from a human specimen may be deposited in a publicly available setting for the purposes of data sharing for further research. The most common method is the Safe Harbor method, removal of 18 identifiers as listed in HIPAA regulation (45 CFR 164.514(b)(2).   Ethnicity, gender, age, marital status, geographical location, and preferred language are types of descriptors (indirect identifiers) which when combined can enable a patient to be re-identified. This presents particular concerns with regard to privacy, stigmatization, and discrimination, since the ability to protect the confidentiality of these individuals or groups participating in the research is diminished. For example, members of an identifiable population may be stigmatized or discriminated against if research reveals that the group is at high risk of having a genetic variant associated with a particular disease. For some communities, close family relationships also may make it especially challenging to protect participants’ privacy, even if research samples are de-identified. To ensure confidentiality, not only direct identifiers should be removed. Indirect identifiers, such as date of birth, location, marital status, preferred language and ethnicity should also be reviewed and removed when possible.   Some types of individual-level genomic data can be used to identify an individual even without the 18 identifiers. Thus, de-identification of genomics data also heavily relies on additional methods of confidentiality and security that are unique to the particular data type involved, such as adherence to strong data use limitations and practices. Genetic data (generally considered to refer to the sequence of a person's genome, though there is still ambiguity about how that relates to different types of genomic datasets) is considered Strictly Confidential under the FH Data Classification and Handling Standard.   How can a specimen be de-identified?   Under HIPAA, specimens/private information can be de-identified by replacing direct (and indirect) identifiers with a masking/coding schema. Masking schema should use coding that does not include any component of the identifiable patient data or have a direct relationship with them which can be ensured via randomization of coded identifiers. For data that may need to be re-identified later, retaining appropriate documentation of the mapping between identifiable data from patients, specimens or datasets, and the coded identifiers generated is critical. Depending on the study design and protocol, the entity retaining this mapping documentation will be responsible for preventing non-approved re-identification. In some cases if the research group maintains this mapping documentation then their research will still be considered human subjects, though if the mapping is maintained by a non-involved 3rd party the research may be considered exempt.   There is often a question of whether the HIPAA de-identification process is enough to render de-identified specimens/private information sufficiently non-identifiable for exemption from human subjects research. A specimen de-identified under HIPAA is not the same as a non-identifiable specimen for research (Office of Human Research Protection). Under research, a specimen rendered non-identifiable under 45 CFR 46.102(f) could qualify as exempt from human subjects research. A non-identifiable specimen (thus not human subject research) must meet the following two requirements:           The private information or specimens are NOT collected for a specific research project through an interaction or intervention with living individuals; and            The investigator(s) cannot readily ascertain the identity of the individual(s) to whom the coded private information or specimens pertain because, for example:               The investigators and the holder of the key enter into an agreement prohibiting the release of the key to the investigators under any circumstances, until the individuals are deceased (note that the HHS regulations do not require the IRB to review and approve this agreement);            There are IRB-approved written policies and operating procedures for a repository or data management center that prohibit the release of the key to the investigators under any circumstances, until the individuals are deceased; or            There are other legal requirements prohibiting the release of the key to the investigators, until the individuals are deceased.       How can a genomic dataset be de-identified?   If all direct and indirect identifiers are removed from a genomic data set, the genomic data itself may still be able to identify a patient depending on the data type and degree of processing of the specific data entity. The inclusion of racial, ethnic and genders in scientific research however, can be a reason to retain some of these indirect identifiers in the context of research datasets. Thus, in addition, confidentiality is supported through strong use limitations, data use agreements and appropriate data security.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/human_deidentification/",
        "teaser":null},{
        "title": "Overview of Data Privacy and Security",
        "excerpt":"This section contains a variety of types of information about relevant regulations and guidance for implementing studies using human subjects, human data and/or human specimens. Given the evolving conditions and rules surrounding studies using genetic and genomic datasets, we aim to provide an up-to-date resource about the current state of the field relating to consenting, privacy, and data management. However, as this field is changing rapidly, if there are changes not reflected here, please email sciwiki to highlight issues for additional review.      The Fred Hutch Policy on IRB Review of Genomic Data Sharing Policies can be found here.  This policy provides an overview of Fred Hutch requirements and pathways for submitting for review proposals including genomic data.    Consenting and the IRB   Informed consent is a cornerstone of the ethical conduct of research involving humans. A goal of informed consent is to ensure that subjects are aware of the risks and potential benefits of proposed research and to make an informed and voluntary decision about participating in a research study. Unlike the risks presented by many biomedical research protocols considered by IRBs, the risks involved with genetic information can reach beyond the boundary of physical injury, and also include risks of social and psychological harm. This section highlights some of the issues and resources available to address the unique issues associated with consenting and human specimen/genomics research.   Data Privacy and Security   Every effort must be made to protect the identity of participants when human subjects, specimens or data are involved in a research project. In most instances, sharing data should be possible without compromising the confidentiality of participants, but if there are circumstances where data needs to be restricted due to the inability to protect confidentiality, this should be fully addressed in the data management and sharing plan.  This section addresses various aspects of data privacy and security including data that originates from human subjects or specimens.   Compliance and Legal Agreements  When sharing data, there are additional issues that might arise regarding appropriate use of those data.  This section provides information about how to obtain the relevant legal agreements involved when compliance with data use restrictions is required for a project.   De-Identification of Specimens and Data   De-identification generally refers to the removal of 18 identifiers as listed in HIPAA regulation 45 CFR 164.514(b). However, de-identification also means that in addition to the removal of these identifiers, the risk of re-identification, including applying methods which utilize publicly available data, is very small. Even without the 18 identifiers, individual-level genomics data could potentially identify an individual. Therefore, de-identification of genomics data also heavily relies on additional methods of privacy and security, such as adherence to strong data use limitations and practices, and strict security policy. In this section we address more specific approaches to address the need for de-identification of specimens and datasets for translational genomics studies.   Data Sharing and Public Repository Deposition   Data Sharing in the realm of genomics and large scale datasets has highlighted some specific new challenges and possibilities. The sharing of large scale research data has potential to strengthen academic medical research, the practice of medicine, and the integrity of the clinical trial system. Some benefits are obvious: when researchers have access to complete data, they can answer new questions, explore different lines of analysis, and more efficiently conduct large-scale analyses across trials or projects. However, our evolving collective understanding of data sharing practices when large-scale datasets are involved can tend to result in an unnecessary burden on the research(ers) that is actually both counterproductive and may not necessarily make the patient or researcher any safer. This section can help guide decision making and actions to successfully share and manage research data to allow for the most productivity and facilitation of the original research itself while balancing the data privacy and security needs of those involved.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/human_overview/",
        "teaser":null},{
        "title": "Data Privacy and Security",
        "excerpt":"Every effort must be made to protect the privacy and confidentiality of participants regardless of the type of data being used as part of a research project. A key protective measure is to de-identify datasets.  Typically, this involves removing 18 key direct identifiers as described under HIPAA, and indirect identifiers, such as age, gender, location, etc from any part of the data.   The aim of de-identification is to maintain confidentiality through elimination of identifiers in a way that eliminates or greatly diminishes the risk of re-identification of an individual patient (also see our page on de-identification of data or specimens). Maintaining strong data security processes is an additional key measure. Sharing data should be done securely, after the information is de-identified and within a data management and sharing plan. This plan should delineate data security, restrictions on data which cannot be sufficiently de-identified, data use agreements, naming conventions and other clearly specified requirements. In most instances, sharing data should be possible without compromising the confidentiality of participants.   Fred Hutch Information Classification Standard   Data should be handled according to the Fred Hutch Information Classification Standard, which describes security and handling standards commensurate with the risk of information mishandling. These standards provide guidelines for managing data at Fred Hutch and for sharing information within Fred Hutch and between Fred Hutch and an outside party. Genomic data, outside of public reference data, is considered confidential (Level II) or strictly confidential information (Level III).   If you are sharing data that includes a third party’s proprietary data or has third-party commercial restrictions, please contact Fred Hutch Business Development &amp; Strategy (BD&amp;S). When research is funded by a commercial sponsor, restrictions on data sharing may apply because of agreements with the sponsor. Any such restrictions should be highlighted in the data management and sharing plan. In the event that you apply for or receive commercial funding for any part of research, you should advise BD&amp;S of the situation without delay.   Data Classification Overview  Public - Level I  This level includes published research results, Fred Hutch publications and communications press announcements, public record documents, job postings, open source configuration list/code/recipes, reference genomes, released data sets, public cryptographic keys.   Confidential - Level II  This level includes pre–publication research information and analyses, medical expense information, invoices, legal instruments or agreements, transaction documents and reports, Fast file and Secure File server, building plans and information about the physical plant, de-identified research participant information, donor information, metadata, Human Resources data/Employee ID numbers, server names/IP addresses, corporate policies, DNS and LDAP information.   Strictly Confidential - Level III  This level includes Protected Health Information (PHI), Individually Identifiable Health Information (IIHI), Personally Identifiable Information (PII), passwords and encryption keys, proprietary information including that belonging to entities other than Fred Hutch, hardware and software authtorization or authentication keys, electronic communication and documents regarding personal or financial matters or other sensitive subjects.                  Level       Subject to FH Admin. Control       Access Requires Authentication       Logging/Audit       Encryption at Rest (Encryption effective mid-2019)       Encryption in Transit       Email       Paper-Based                       Public - Level I       No       No       No               No, On Premises                              No, Off-Premises (Cloud)       No Restrictions       No Restriction                 Confidential - Level II       Yes       Yes       Yes, if automated or manual system allows.       No, On Premises                              No, Off-Premises (Cloud)       No, On Premises                              Yes, Off-Premises (Cloud)       FH Supported Systems       Confidential Labeling                 Strictly Confidential - Level III       Yes, Both on and off Premises       Yes       Yes. Audited Semi-Annual       Yes, On Premises                              Yes, Off-Premises (Cloud)       Yes, On Premises                              Yes, Off-Premises (Cloud)       Encrypted and FH Approved Systems       Tamper-proof Envelope/Registered Mail/Signed Delivery           Patient Consent   All research involving 1) human subject participants, 2) patient information, or 3) tissue samples derived from patients/human participants must include appropriate safeguards to protect the privacy of research participants. You must ensure the necessary patient consent:      adheres to Human Subjects Protection by receiving IRB approval, and   is signed by the patient prior to data sharing.   Requirements to adhere to relevant regulatory, ethical, or institutional policy should be met, data security measures established and all IRO and patient permissions should be in place prior to disclosing any data. Requirements may dictate sharing through a data use agreement.   Security   Common mechanisms for sharing datasets:           Hosted by portals or other data sharing platforms as prescribed by specific federal agencies or journals            Posted online via a project or personal website            Submitted as supplemental material to be hosted on a journal publisher’s website            Deposited in an open repository or archive (such as GitHub or other open hosting site)            Deposited in an open repository and publish a “data paper” describing the data            Using FH Outlook email and Office 365 Encryption Security            Shared using Fred Hutch resources (such as Sharepoint, Aspera, Microsoft OneDrive or BaseCamp), described in the Center IT Collaboration Tools site and in this site’s Collaborative Data Storage page            Using properly licensed corporate tools or subscription applications such as DropBox or Box can be considered with guidance from the Information Security Office          Note:  Fred Hutch data should not be stored or shared via applications utilized for personal use.    Office 365 Email and OneDrive  The implementation of Office 365 provides the ability to secure email, storage and sharing.  Outlook email can be secured within the Fred Hutch environment and between Fred Hutch and external entities.  To trigger email encryption, type “secure” (without quotes) in the subject line of any email and additional instructions for securing email are provided here. For users of mobile devices, Outlook mobile app (iOS and Android) ca display encrypted email within the app without opening browser.   More information about using OneDrive to share data can be found on our Collaborative Data Storage page.   Final Notes  Generally, Investigators can more widely share their data to the scientific community by transferring it to a data archive facility to maintain documentation and meet reporting requirements. Data archives are particularly attractive for investigators concerned about managing a large volume of requests for data, vetting frivolous or inappropriate requests, or providing technical assistance for users seeking to help with analyses. Fred Hutch does not have a centralized data archive and does not have a preferred recommendation for one.   Datasets that cannot be distributed to the general public, due to confidentiality concerns or third-party licensing or use agreements that prohibit redistribution, can be accessed through a data enclave. Fred Hutch does not have a data enclave and does not have a preferred recommendation for one. A data enclave provides a controlled secure environment in which eligible researchers can perform analyses using restricted data resources.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/human_privacySecurity/",
        "teaser":null},{
        "title": "Data Sharing and Public Repositories",
        "excerpt":"Whether required for funding and publication or desired for its beneficial impact on research progress, understanding the why and how of data sharing is essential for modern biomedical research programs.   Benefits of Data Sharing   Some of the benefits of data sharing include:           reinforcing open scientific inquiry            encouraging diversity of analysis and opinion            promoting new research, testing of new or alternative hypotheses and methods of analysis            supporting studies on data collection methods and measurement            facilitating education of new researchers            enabling the exploration of topics not envisioned by the initial investigators            permitting the creation of new data sets by combining data from multiple sources            gaining for individual investigators additional insights from other investigators’ studies of the data            increasing the visibility and credibility of one's own research based on the data            opening opportunities for developing new collaborations and for access to complementary data sets       Data Sharing and Management Plans   A good resource for data sharing management plan development is DMPTool sponsored by the University of California. DMPTool lets data owners create, review and share data management plans and provides ancillary knowledge to support sharing and management.   The following should be considered when developing a data sharing plan:           Data Description: What data will be generated? How will you create the data? (simulated, observed, experimental, software, physical collections)            Existing Data: Will you be using existing data? What is the relationship between the data you are collecting and existing data?            Audience: Who will potentially use the data?            Confidentiality: What coding, systems, and gatekeeping procedures are in place to protect the confidentiality of the data subjects?            Access and Sharing: How will data files be shared? How will others access them?            Formats: What data formats will you be creating?            Metadata and Documentation: What documentation will you provide to describe the data? Metadata formats and standards?            Storage, backup, replication, versioning: Are the data files backed up regularly? Are there replicas in different locations? Are older versions of the data kept?            Security: Are the system and storage that will be used secure?            Budget: Any costs for preparing the data? Costs for storage and long-term access?            Privacy, Intellectual Property: Does the data contain private or confidential information? Any copyrights?            Archiving, Preservation, Long-term Access: What plans do you have to archive the data and other research products? Will it have long-term accessibility?            Adherence: How will you check for adherence of this plan?       Reading Material           Dataverse has basic best practices guidance for data management plans.            DMPTool from University of California            Duke University Data Management Guide            Stanford University Library includes some sample DMPs from NIH and other resources.            NIAID Sample Data Sharing Plan            FAIRSharing policies:  A catalogue of data preservation, management and sharing policies form international funding agencies, regulators and journals.       Data Sharing Responsibilities   The context of a data sharing project can mandate certain safety requirements and restrictions. Some of these considerations may include whether the data is shared pre- or post- publication, or whether the data is shared locally within the Fred Hutch Cancer Consortium or to a larger national or international audience.   While the underlying goal of sharing is to further research and its benefits, the underlying tenets of data sharing processes are to ensure:           Identification of who shall have access to the data            Data sharing does not compromise individual subjects’ rights and privacy, regardless of whether the data have been used in a publication            Data shared is restricted to only that data appropriate for a specific line of inquiry            The integrity and quality of shared data are preserved            Data sharing is done within the legal requirements of both sender and receiver            Shared data is easily readable or accessible by the receiver. Consideration can be made whether metadata should be provided along with the data to make it easily understood            Appropriate intellectual property, copyright, licensing issues are considered prior to sharing       Before sharing data, it is prudent to determine if sharing is permitted. Below are some relevant questions to help make this determination:           If the data is derived from human subjects research, does the associated IRB-approved informed consent (or waiver of informed consent) permit disclosure for the contemplated DUA purpose? If not, new IRB review, and a waiver of consent or re-consenting of subjects, may be required before sharing is permitted.            If the data was collected pursuant to a sponsored research project, has the sponsor placed restrictions on the subsequent transfer of the data? Stipulations may prevent sharing or require specific sharing restrictions.            If the data was initially received from, or derived from, data received from a third party pursuant to a contract, does that contract place restrictions on the subsequent transfer of the data? Stipulations may prevent sharing or require specific sharing restrictions.       Required Data Sharing   ClinicalTrials.gov   HHS developed a final rule, Clinical Trials Registration and Results Information Submission, made publicly available on September 16, 2016; and the NIH issued a complementary final policy, under which NIH-funded awardees and investigators are required to submit registration and results information for all NIH-funded clinical trials, whether or not the trials are covered by the FDAAA requirements.           More information about the specific policies related to ClinicalTrials.gov            Why register and report results to ClinicalTrials.gov?       Funding Agencies   Many (but not all) federal funding agencies require data management plans and data sharing plans as part of a grant proposal application. Persons seeking to make a grant application to a federal funding source should ascertain the data sharing plan requirements.   Some of the US federal granting agencies that require funded projects to provide some form of data management planning include NIH, NSF, DOE, DOD, HHS, and FDA.   Funding agencies may want to know:           What data you are producing that will be accessible (shared) with others?            When and how will you make it discoverable? Place in a repository, data commons or other location so others can find it?            How will you make accessible? Will there be restrictions to some or all the data and if so how do people obtain access?            How will you make it useable/reusable? Will there be documentation, definitions, descriptions of methodology, use of standard terminology, etc.       Data Sharing as Part of Publications   As of 1 July 2018, manuscripts submitted to journals within the International Committee of Medical Journal Editors (ICJME) reporting clinical trial data must contain a data sharing statement, indicating:           whether the authors intend to share individual de-identified participant data            what specific data they intend to share            what other study-related documents will be made available            how the data will be accessible            when and for how long they will be made available.       The statement may then be taken into account by ICMJE editors when considering the paper for publication. Furthermore, clinical trials that begin enrolling participants on or after 1 January 2019 must include a data sharing plan in the trial’s registration if they wish to publish results in ICMJE journals. Any deviations from this plan must be disclosed in the data sharing statement when published.   ICMJE believe that scientists have a moral obligation to share clinical data to maximize the knowledge obtained from these research efforts. The committee is working with a number of groups to solve the various practical issues that they acknowledge still exist to in order to achieve their goal of universal data sharing.   Genomic Data Sharing   American College of Medical Genetics and Genomics Board of Directors suggests that broad data sharing is necessary and will improve care by making available the best data possible, by which:           Key clinical attributes of the phenotype of those with genetic diseases can be described;            The qualitative strength of the association between genetic diseases and the underlying causative genes can be established;            The classification of genomic variants across the range of benign to pathogenic can be established;            Differences in variant interpretation among laboratories can be reconciled;            The appropriate classification of variants of uncertain significance can be made; and            Standards used in variant classification can be improved.       As noted by the NIH and others, the nature of genomic data requires several specific considerations be kept in mind. In addition to being personal and unique to each individual, genomic data may, for example:           Be stored and used indefinitely.            Inform individuals about susceptibility to a broad range of conditions (some of which are unexpected given personal or family history).            Carry with them risks that are uncertain or unclear.            Be reinterpreted and change in relevance over time.            Raise privacy concerns (in part because of the risk of re-identification).            Be relevant for family members and reproductive decision-making.       Transform the personal decision of sharing genetic data into one with   familial current and future ramifications.   There are many challenges to sharing phenotypic and genotypic data. As such, some genomic data sharing best practices are listed below:           Investigator(s) should use requested datasets solely in connection with the research project described in the approved data request, protocol or other vehicle that bounds the use and disclosure of the data.            Investigator(s) will make no attempt to identify or contact individual participants from whom these data were collected without appropriate approvals from the relevant IRBs.            Investigator(s) will use informed consent documents with specific language addressing the potential persistent harms identified data can carry, the rights of patients to withdraw consent, the type of data involved and the respect and trust of participants who give researchers the right to use their data and samples for “future unspecified research.”            Investigator(s) will not distribute these data to any entity or individual beyond those specified in the approved data request, protocol or other vehicle which bounds the use and disclosure of the data.            Investigator(s) will by default share de-identified data.            Investigator(s) will strive to harmonization of data collection and archiving methods (storage) tools and representation to ensure validation of scientific quality.            Investigator(s) will adhere to computer security practices that ensure only authorized individuals can gain access to data files, minimizes unintended access and otherwise adhere to institutional security requirements.       NIH Genomic Data Sharing (GDS) Policy   The GDS Policy expects that large-scale genomic research data from NIH-supported studies involving human specimens will be submitted to an NIH-designated data repository. Non-human data may be submitted to any widely used repository or to the same repositories they submitted specific types of data to previously. NIH has provided examples of relevant databases on the NIH Office of Science Policy website.           Investigators should submit large-scale human genomic data as well as relevant associated data (e.g., phenotype and exposure data) to an NIH-designated data repository in a timely manner.            Investigators should also submit any information necessary to interpret the submitted genomic data, such as study protocols, data instruments, and survey tools.            Grants applications and protocols should include details of the required data sharing plan.            In general, consent documents should include language that allows for the broad future sharing of genomic data. The NIH recognizes that there will be instances where broad sharing may not be appropriate and the policy outlines the exceptions.            Under the GDS Policy, the release of human data for secondary research can generally be deferred for up to six months after data submission, with no publication embargo upon data release.            An IRB must review the foundational consents (all versions) to confirm that consent was appropriately sought, includes broad sharing (if appropriate), and to identify any limitations on the upload.       NOTE: If you are generating genomic data that does not meet the definition of “large-scale,” consider including a section in your Resource Sharing Plan that says, “Genomic Data Sharing: Not Applicable” with a brief explanation. Grant applications are not always sufficiently detailed for staff to determine whether the GDS policy applies, in which case staff must contact the PI, which can cause delays.           The NIH Genomic Data Sharing Policy is here.            Large-scale genomic data include genome-wide association studies (GWAS), single nucleotide polymorphisms (SNP) arrays, and genome sequence, transcriptomic, metagenomic, epigenomic, and gene expression data. Examples of research that are subject to the GDS Policy include, but are not limited to, projects that involve generating the whole genome sequence data for more than one gene from more than 1,000 individuals, or analyzing 300,000 or more genetic variants in more than 1,000 individuals, or sequencing more than a 100 isolates of infectious organisms such as bacteria. The Supplemental Information to the NIH Genomic Data Sharing Policy includes detailed description of research under scope of the policy and data submission expectations.       NIH Public Genomic Data Repositories   NIH is committed to respecting the privacy and intentions of research participants with regard to how data pertaining to their individual information is used. Data access is therefore intended only for scientific investigators pursuing research questions that are consistent with the informed consent agreements provided by individual research participants. Furthermore, investigators provided access will be expected to utilize appropriate controls and abide by Data Use Limitation.   The dbGAP has two repositories into which researchers can deposit or withdraw de-identified data:           Public Access: Non-individual genomic data can be publicly accessed through the dbGAP website.            Controlled Access (Individual-level data): Individual-level data submitted to the dbGaP is required to be de-identified. No names or identifiable information is attached to the data. The genetic fingerprint however is embedded in individual’s genotype data, which is not de-identifiable. That is why, to protect individuals privacy, all individual level data is only distributed through the NIH Authorized Access System.       Additional information regarding the process of depositing data into NIH Public Repositories can be found from NCBI and from the NCI.   Some Public Repositories   We here provide some details on a few (by no mean exhaustive) public repositories. If you know of another repository that would like to see added to this page please contact us. Some of these repositories do require IRB approval to download the data. Please correspond with the proper administration authority.      GTEx: The Genotype-Tissue Expression  project is aiming to understand the difference and regulation of gene expression by tissue across 53 different tissues.   TCGA: The Cancer Genome Atlas is a joint effort by the NCI and the NHGRI to moleculary characterize approximately 20000 primary cancer in comparison to matched normal over 33 cancer types.   1000 genomes: The 1000 genomes project sequenced 1000 genomes to find genetic variants with at least 1% variation in the human population. A useful source   GEO: Gene expression Omnbinus  is a public genomics data repository.   gnomAD: The genotype aggression database  is working to aggregate and harmonize sequencing data across multiple studies.   TOPMed: TOPMed is brought to you by NIH and NHLBI. Trans-Omics for Precision Medicine (TOPmed) aims to “understand of the fundamental biological processes that underlie heart, lung, blood, and sleep (HLBS) disorders.”   dbsnp: Provides information about single nuclotide polymorphisms (SNPs), as well as tracking these SNPs publication history.   UK Biobank: Prospective cohort study with a wealth of genetic and health data on 500,000 participants.   cBioPortal:  This is an excellent resource for viewing/accessing TCGA cancer genomics data.   Sage’s Synapse.org is not only useful for sharing (privately or publicly) your own research data, but also where several open datasets are hosted, and also interact with the various DREAM Challenges they host.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/human_shareDeposits/",
        "teaser":null},{
        "title": "Overview of Laboratory Management Resources",
        "excerpt":"A large part of any data generation process involves the effective organization and management of specimens and lab resources.  Efficient lab management can streamline specimen receipt, processing, and any downstream data generation workflows.  This section will highlight a few useful tools for the management of physical lab resources–tools for ordering and organizing reagents, helpful equipment for common lab workflows and a variety of guidance about assay material preparation, with specific tips for RNA and DNA based analysis approaches.   Assay Material Prep and QC  A critical component of quality large scale molecular data is quality assay material, in this case, the nucleic acid itself. Multiple processes are involved in the isolation and preparation of specimens and nucleic acids upstream of data generation that can impact both what types of data are feasible to generate, as well as what types of hypotheses the data can be used to address. Different data-generating platforms are sensitive to certain types of specimen quality and quantity. While the hypothesis for a study will narrow the choices for data types required, the quality and quantity of nucleic acids from a cohort of specimens can have an even larger impact on what data types are feasible, as well as the relevance and interpretability of the resulting data sets to the intended question.  This page contains a summary of the types of nucleic acid isolation for different specimen types, the types of quality and quantity assessments and how these impact what downstream data generation process is applicable to the specimens of interest.   DNA-Based Approaches  Here we summarize some specific approaches to specimen processing, nucleic acid quality and quantity assessments, genomics platforms types, reagents, costs and data analysis requirements for DNA based data types such as hybridization or sequencing based techniques.  We outline a few types of projects and give examples of the particular scientific considerations involved as well as some general guidelines about the costs of different approaches. We focus on non-whole genome studies but intend to provide some guidance from Fred Hutch researchers about this type of project in the future.   RNA-Based Approaches  Here we summarize some specific approaches to specimen processing, nucleic acid quality and quantity assessments, genomics platforms types, reagents, costs and data analysis requirements for RNA based data types such as hybridization or sequencing based techniques.  We outline a few types of projects and give examples of the particular scientific considerations involved as well as some general guidelines about the costs of different approaches.   Support software  The organization of lab resource data is certainly not large scale data, but it can streamline lab workflows on a daily basis.  For example, having a searchable database of PCR primers and their locations in freezers and boxes can save time in the planning and execution of assays.  Alternatively, having an easily accessible communication platform among lab groups can facilitate transfer of knowledge about projects to new or existing lab members. This section will list some useful software platforms for organizing lab resources and communication.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/labman_overview/",
        "teaser":null},{
        "title": "Software for Research and Laboratory Management",
        "excerpt":"There are a number of software tools available that lend themselves well to the management of research labs and here we focus on those that have a free tier, are open source, or are supported by the Fred Hutch itself.   Reagent and Laboratory Organization  Quartzy is a free, web-based lab management software. Users can organize lab member order requests, track order receipts, and organize lab inventory.  Quartzy’s ordering system integrates with the Hutch Online Marketplace to streamline ordering and subsequent tracking of receiving in the lab, as well as lab storage and lot tracking. This software facilitates four main lab functions:      Submit supply requests – Lab members can browse Quartzy’s catalog of life science products and request supplies from their lab manager   Process supply requests – Lab managers/Supply managers can receive and incoming supply requests for approval.   Receive and Organize lab supplies – Users can mark supplies as received, automatically update inventory data, and alert lab members when supplies are back in stock and where to find them.   Search/Browse Lab inventory – Users with appropriate permissions can search the entire contents of the lab for reagents, or browse individual locations within the lab database.   Project Management  Asana is a web-based project management software.  It allows groups to create tasks and assign those tasks to group members. Groups can get task reminders, define timelines, comment on the status of tasks, and easily edit the scope of tasks as projects develop.  Tasks may be independent, or dependent on the status of other tasks–for example, when laying out a project where Task A must be finished before Task B can be performed, you can define Task B to be dependent on the completion of Task A. This functionality allows easy design of task workflows within projects. Asana can be accessed on mobile devices, laptops, desktops, and tablets, which allows easy group communication, regardless of platform.   Trello is another web-based project management software. Its functionality is very similar to Asana–groups can create and assign tasks, manage priorities and status, comment and attach files, and can be accessed on multiple device types.   Team Communication and Collaboration  Slack is a collaborative team messaging software. Communication in Slack happens in channels–which can be organized by project, topic, team, or whatever makes sense for a group. Conversations in Slack are searchable, which can facilitate group communication and easy onboarding of new group members. Slack also offers integration of other software tools (Google Drive, GitHub, Asana, Trello, etc.), which can be very useful for communicating project information. Several groups at Fred Hutch use Slack to communicate with members–including the Fred Hutch Bioinformatics Interest Group (FHBIG).  Individual groups or labs can create a private Slack workspace just for their group, or individuals can join other Slack workspaces as well as guests or members, depending on how the workspaces are set up by their administrators.   Available Resources     At FHBIG’s Slack Channel, you can see an example of how groups are using Slack to facilitate communication between bioinformatics-oriented researchers here at Fred Hutch.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/labman_software/",
        "teaser":null},{
        "title": "Multiple Testing",
        "excerpt":"Statistically, multiple testing is defined as the testing of more than one hypothesis at a time.  Pragmatically, this changes ‘omics significance levels based on the breadth of your assay and the number of different cohort arrangements on which statistical tests are run. The resources below will help you understand the broad strokes before consulting with a statistician about your project’s specific details.   Reading material  We aim to curate some useful reading material about multiple testing relevant to Fred Hutch studies.  Please email additional suggestions to Fred Hutch username sciwiki.     How does multiple testing correction work? (Noble 2010)   Hands on Example (R)  This example provides information on the Family Wise Error Rate (FWER) and the False Discovery Rate (FDR), using code run in R. We provide information on a few commonly used approaches for correcting for multiple testing as well. This is not an exhaustive list, as there is still ongoing research on how to adjust for multiple testing.   Hypothetical Study Design  For the sake of having an example on hand, let’s draw up a hypothetical study. Imagine we have observed gene expression for 1000 independent transcripts in 500 study participants. We are interested in testing if there is any association between any of those 1000 transcripts and reading this wiki page. For each of the 500 people, we have an indicator (1 or 0) for whether they have read the wiki page you are reading right now. For each gene we are testing the following hypothesis:      H0j: The gene expression at transcript j is not associated with having read this wiki.     H1j: The gene expression at transcript j is associated with having read this wiki.    H0j is the NULL hypothesis for transcript j and H1j is the alternative. For each test we have a p-value which we use as evidence against H0j.   Some definitions  Before continuing, it is useful to define some statistical terms.     Type I error: Probability that we incorrectly  reject H0j    This mean we observe a p-value that would lead us to reject the NULL hypothesis and give us evidence to suggest the alternative. Often, we want to control the Type I error at some pre-specified level α. For example, 0.05 is often used. With this threshold, if the p-value is less than 0.05, we reject the NULL hypothesis. The value of 0.05 is arbitrary and a significance cutoff should be decided prior to running any analyses and should be based on prevalent thought within the field.      FWER: Family Wise Error Rate: The Probability of making at least one Type I error     FDR: False Discovery Rate: Rate of tests that come back as significant that are actually false.    Analyze the Study Data   Now let us run this hypothetical study and see what happens. We provide code below to run, if you would like to follow along in R. We generate the gene expression data (Y) and the exposure information (X) of whether they have read this wiki page. We’re going to use as our significance threshold 0.05. Thus if the p-value is less than 0.05, we are going to reject the NULL hypothesis that there is no association between gene expression at that gene and reading this wiki page.   set.seed(20190101)  N&lt;-500 #the sample size M&lt;-1000 #number of \"genes\" we are testing X&lt;-rbinom(N,1,0.2) # Variable of interest Y&lt;-replicate(M,rnorm(N)) # The observed gene expression TestIt&lt;-matrix(nrow=M,ncol=4) for(i in 1:M){   R1&lt;-summary(lm(Y[,i]~X))   TestIt[i,]&lt;-coef(R1)[2,] }   If we look at the p-values (which are the fourth column of the R matrix TestIt above), we can count how many NULL hypothesis we would reject.  sum(TestIt[,4]&lt;0.05)   ## [1] 58   We can see that we improperly reject 58 tests solely by chance. This is not ideal. The reason for this is that when we are under the NULL and we are using a well-behaving test, the p-values follow something called a continuous uniform distribution from 0 to 1. Under this distribution, the probability that one p-value is less than 0.05 is 0.05, which is fine if we are only performing one test. However, when we are performing 1000 tests, the probability that at least one of those p-values is less than 0.05 is approximately 1. IE our FWER under this case is going to be 1.   Correcting for Multiple testing  We now get into the details of how to correct for multiple testing. There are two ways one can go about doing this, control the FWER or the FDR. Which one is used, should be decided prior to the analysis.   Consider the following table for our example (borrowed from Wikipedia).                          H0j is true       H1j is True       Total                       Test is declared significant       V       S       V+S                 Test is not declared significant       U       T       U+T                 Totals       V+U       S+T       1000           Family Wise Error Rate: FWER   Remember, the FWER is the probability that we perform at least one type I error in our analysis. This is the probability that the cell V is greater than or equal to 1 (i.e., the test is declared significant  but  H0j is true). When a study says they are controlling  the FWER at some level α, what that means is that:      P(V≥1)≤α    Let us say that we originally decided to say the test is significant if the p-value is less than 0.05. In that situation:      P(V≥1)≥0.05    Therefore, our FWER is not controlled at the rate of 0.05. We actually have more than a 5% chance of making a Type I error, which is not we want. If all 1000 tests are truly null, the probability of making at least one error is equal to 1-(1-0.05)^1000 or approximately 1.   Below we give two different approaches to control for the FWER. This is by no means exhaustive and there are other approaches out there. All two are implemented in the R function p.adjust.   Bonferroni   The most widely used approach. A test is declared significance if the corresponding p-value is less than α divided by the number of tests. In our example, we would use 0.05/1000 as opposed to 0.05. Here would have:      P(V≥1)=P(Any of the H0j are improperly rejected)≤(V+U)0.05/1000≤(1000)0.05/(1000)=0.05    Therefore we have that the P(V≥1)≤ 0.05, the desired result.   Holm procedure   The Holm correction is a stepwise procedure, where the significance threshold changes as you reject tests.      Look smallest p-value. If less than α/#tests reject the respective H0j.   Look at next smallest p-value. If this p-value is less than α/(#tests-1) reject H0j.   Continue looking at successively larger p-values in the same manner until you can no longer reject.   Implementation  Let us implement these in our example and see how they do. First the Bonferroni correction. The function p.adjust gives back a corrected p-value.   BonPval&lt;-p.adjust(TestIt[,4],method = \"bonferroni\") sum(BonPval&lt;0.05)   ## [1] 0   So we now no longer reject any tests. Next we do the Holm procedure:  HolmPval&lt;-p.adjust(TestIt[,4],method=\"holm\") sum(HolmPval&lt;0.05)   ## [1] 0 And we get the same result.   False Discovery Rate: FDR   The False Discovery Rate is a newer correction approach developed by Benjamini and Hochberg in the 1990’s. The goal of the FDR is to identify as many significant tests while controlling the number of false positives identified. This means we want to keep V/(V+S) from the above table below a certain proportion. The FDR in general is a less conservative approach to multiple testing than the FWER (though they are controlling separate things).  In general, if something is significant via the FWER at a level α, it will be significant at the FDR level α as well.   Below we give a few approaches, but again these are by no means conclusive. This is still an ongoing area of research in general.   Benjamini and Hochberg   If we are controlling at the level α, we order the p-values and find the kth ordered p-value such that it is less than α times k/m. This method is implemented within the p.adjust function in R.   q-value   A popular approach in bioinformatics, the q-value is a Bayesian approach to the FDR developed by John Storey. It is implemented in the Bioconductor R package q-value.   Implementation of these FDR methods   BHPval&lt;-p.adjust(TestIt[,4],method=\"BH\") sum(BHPval&lt;0.05)   ## [1] 0   And now qvalue   library(qvalue) TheQval&lt;-qvalue(TestIt[,4])$qv sum(TheQval&lt;0.05)   ## [1] 0   Similar to when we corrected for the FWER we no longer reject any tests.   Another Example of the FWER   Now let us run some examples to see what happens when we run this study over and over again to get an estimate of what the FWER is under these scenarios. This will tell us how well these testing procedures are really doing.   Numstudies&lt;-5000 # Number of studies   Define a function to use that will do our analyses a little quicker so we do not have to call lm.   fastReg.func&lt;-function(A,nY){   XX&lt;-cbind(1,A)   EY&lt;-solve(t(XX)%*%XX)%*%t(XX)%*%nY   residY&lt;-nY-XX%*%EY   SigEst&lt;-colSums(residY^2)/(length(A)-ncol(XX))   VarEst&lt;-SigEst*solve(t(XX)%*%XX)[2,2]   Pval.A.to.Y&lt;-2*pt(abs(EY[2,]/sqrt(VarEst)),df=length(A)-ncol(XX),lower.tail=F)   return(cbind(EY[2,],Pval.A.to.Y)) }   And run the simulation   set.seed(44511) EachStudy&lt;-matrix(nrow=Numstudies,ncol=4) for(k in 1:Numstudies){   X&lt;-rbinom(N,1,0.2) # Variable of interest   Y&lt;-replicate(M,rnorm(N))   AA&lt;-fastReg.func(X,Y)   EachStudy[k,1]&lt;-sum(AA[,2]&lt;0.05)   B1&lt;-p.adjust(AA[,2],\"bonferroni\")   EachStudy[k,2]&lt;-sum(B1&lt;0.05)   H1&lt;-p.adjust(AA[,2],\"holm\")   EachStudy[k,3]&lt;-sum(H1&lt;0.05)   BH1&lt;-p.adjust(AA[,2],\"BH\")   EachStudy[k,4]&lt;-sum(BH1&lt;0.05) }  In the above, we are saving from each study of 1000 genes four things. 1) How many uncorrected p-values are less than 0.05 (first column of the matrix EachStudy). 2) How many Bonferroni corrected p-values are less than 0.05 (second column of EachStudy). This is equivalent to how many uncorrected p-values are less than 0.05/1000. 3) How many Holm adjusted p-values are less than 0.05 (third column of EachStudy). 4) How many Benjamini Hochberg corrected p-values are less than 0.05 (fourth column of EachStudy). This is controlling the FDR.   We can now look at what the FWER is if we didn’t perform any correction at all by taking the mean of how many of these 5000 studies resulted in at least one p-value less than 0.05.   mean(EachStudy[,1]&gt;0)   ## [1] 1   It is 1 as we expected. Next we can look at how the Bonferroni and Holm corrected approach do by mean of how many of these 5000 studies resulted in at least one p-value less than 0.05.   mean(EachStudy[,2]&gt;0)   ## [1] 0.0488   mean(EachStudy[,3]&gt;0)   ## [1] 0.0488   We see that it is less than 0.05 (but still close) which is the desired result. Finally, we look at the FDR and we see a similar result (though now we are controlling the FDR).  mean(EachStudy[,4]&gt;0)   ## [1] 0.05  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/multiple_testing/",
        "teaser":null},{
        "title": "Compliance and Legal Agreements",
        "excerpt":"Data Use Agreements (DUA)   Data Use Agreements (DUAs) are contractual documents used for the transfer of nonpublic data that is subject to some restriction on its use. The data provider typically originates DUAs. Contact Fred Hutch Business Development &amp; Strategy to initiate a DUA or review a received DUA, using this questionnaire. DUAs serve to outline the terms and conditions of the transfer. Specifically, DUAs address important issues such as limitations on use of the data, obligations to safeguard the data, liability for harm arising from the use of the data, publication, data destruction or return and privacy rights that are associated with transfers of confidential or protected data.           DUAs serve to outline the terms and conditions of the transfer.            DUA are used most commonly for sharing data within research and/or collaboration. DUAs are most often used when transferring data between Fred Hutch and an external entity (outside of the UW/Cancer Consortium) and may be used for transferring sensitive information between Fred Hutch and its consortium partners.            DUAs typically originate with the sender. DUAs may be specific, such as those incorporated into the context of a Clinical Trial Agreement or a sponsored research agreement, or they can be general.            If a Fred Hutch clinical trial is part of a multi-site trial, DUAs may be required to conduct the trial.            Most data sharing involves de-identified data. Less common is sharing research data with identifiable individual health information for which patient informed consent and HIPAA authorization has been signed. Both types of data are required by Fred Hutch to have a DUA when sharing data.            The Privacy Rule permits sharing of a Limited Data Set, which includes some identifiers. A LDS may be disclosed for research activities if the disclosing entity and the LDS recipient enter into a data use agreement. Limited Data Sets may be used or disclosed only for purposes of research, public health, or health care operation.            A sample Fred Hutch MTA can be found here.  Contact Fred Hutch BD&amp;S for more information on Fred Hutch DUAs.       Material Transfer Agreement (MTA)   A Material Transfer Agreement (MTA) is a contract used to govern the exchange of research data or materials between organizations. Examples include:      Cell lines, cultures, bacteria, nucleotides, proteins, transgenic animals, pharmaceuticals, and chemicals, or data cell lines, cultures, bacteria, nucleotides, proteins, transgenic animals, pharmaceuticals, and chemicals, or data software, models, robots tangible research material or raw datasets (but not databases, which are protected by copyright)    The Fred Hutch Business Development &amp; Strategy (BD&amp;S) is responsible for negotiating and signing all MTAs with outside organizations. Fred Hutch BD&amp;S is responsible for           transferring materials from Fred Hutch (as Provider) to another academic or non-profit organization as Recipient), or            receiving materials from another organization or company.       BD&amp;S reviews MTAs to ensure contractual obligations are consistent with Fred Hutch policy and/or federal law.   Whether for inbound or outbound materials, MTAs are critical documents that require signatures from the Provider(s) and Recipient institution(s) to ensure that each party will comply with the terms of the agreement. Fred Hutch investigators cannot sign MTAs on behalf of the Fred Hutch. If the MTA is embedded in a Sponsored Research Agreement, is linked to a clinical trial, or is for clinical use or clinical studies in humans, the investigator should contact Business Development &amp; Strategy.   Uniform Biological Material Transfer Agreement (UBMTA)   Fred Hutch is a signatory to the Uniform Biological Material Transfer Agreement (UBMTA) Master Agreement, a contracting mechanism published by NIH on behalf of the U.S. Public Health Service (PHS) to facilitate the transfer of biological materials between academic institutions. For institutions which have agreed to the terms of the UBMTA Master Agreement, it is not necessary to negotiate individualized terms for each transfer of a biological material. Instead, an Implementing Letter is executed, which denotes the biological material as well as the providing institution and receiving institution. The Fred Hutch scientist/investigator and a Fred Hutch institutional representative both sign the UBMTA.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/security_legal/",
        "teaser":null},{
        "title": "Hypotheses in the Era of Large Scale Molecular Data",
        "excerpt":"Rules of the Large Scale Data Game:  Develop a hypothesis that will have reasonable statistical power and feasible sets of samples for a specific type of data.  Then analyze the data collected with the original hypothesis in mind and then stop.  Don’t mine the data indefinitely, because while you’ll find something eventually, it may not mean anything.   Study Design and Data Types  You could design a house without an architect, but the time and money you would spend while trying to do so are effectively wasted because there are people for whom this procedural and rote knowledge is their job.  The same is true for ‘omics - there is an incredible quantity of statistics, bioinformatics and sample preparation that goes into this process. To give a study the best chance of succeeding (and being possible with the $ resources available in a suitable amount of time), identify a set of collaborators familiar with the experimental design and statistics required for a study of this kind.   There are the two general types of studies using large scale molecular data sets, which can loosely be thought of as “investigative” and “comparative.”  The two aren’t completely extricable and can each can serve as groundwork for future experiments for the other.  The process to perform these types of studies, however can be very different, and the details specific to each are best addressed prior to beginning a process involved in generating materials or data sets.   Finding a collaborator to help identify if any biological feature supports or rejects your hypothesis using large scale molecular data is important because of the issues of multiple testing, determining biological significance and power issues unique to large data sets where the number of measurements (p) is much larger than the sample size (n).  The statistics required when n»p are very different than those when p»n and finding someone who is well versed in the specific issues that arise in this context is important.      The Clinical Biostatistics Shared Resource can also help with study design and guide in what is realistic statistically.  Email biostatistics.   The Bioinformatics Shared Resource associated with the Fred Hutch Genomics Shared Resource is a valuable source of guidance for developing a plan for a study and with the Genomics group can provide detailed information about available platforms, estimated costs and process-specific issues to be accounted for when choosing the type of data to acquire. Email bioinformatics.   Rigor and Reproducibility  The reproducibility of a bioinformatic or statistical analysis has been highlighted in recent years a major challenge especially in situations where the raw data are very large, private or otherwise challenging to share with reviewers of publications based on them.  Approaching a study with the intent to create a reproducible result involves a bit more forethought than usual, and a good handle on what types of tools area available to document processes.  The data management tools highlighted in this Wiki are good places to start to address reproducibility.   Confounding and Unintended Covariates  Issues we intend to expand upon in this section include the types of confounding covariates that can occur when using large scale molecular data, such as batch effects of assay material isolation or genomics data creation.  Randomization of samples based on intended covariates (group A vs group B relating to the specific hypothesis the study is designed to address) as well as unintended covariates (such as sex, specimen type, date of specimen acquisition, laboratory performing the preservation or isolation, or other variables that are not associated the primary hypothesis), is critical to plan for in a study especially considering the costs to generate these data as well as the use of limited human specimens.   Sample Size:  Power calculations in the ‘omics setting  More to come about how to identify what types of hypotheses have the possibility to be addressed with what sample sizes.  For now, have some reading material:          Defining the scientific method, an editorial published in Nature Methods in 2009.  “Science, it turns out, is whatever scientists do.” - David Goodstein            Wikipedia’s page on statistical power            GPower:  Downloadable software for a variety of tests, including graphical power analysis results.  Not ‘omics-specific, but a good reference.            Power and sample size calculations for high-throughput sequencing-based experiments(Li et al 2017):  Figure 1 and Table 1 have references to papers on computational bioinformatics tools for various power calculations      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/study_hypothesis/",
        "teaser":null},{
        "title": "Overview of Study Design",
        "excerpt":"In this section we will provide some guidance for researchers looking to develop a hypothesis that will have reasonable statistical power, identify the appropriate set of samples, and execute a large scale data production from those samples.   Proposals and Funding  You could design a house without an architect, but a large part of the time and money you would spend could be alleviated by involvement from people for whom this procedural and rote knowledge is their job.  The same is true for execution of a study involving large scale molecular datasets, where there is an incredible quantity of decision-making relating to study design, sample preparation, genomics data production, bioinformatics, and statistics. To give your study the best chance of succeeding (both in getting funding and answering hypotheses), it’s best to identify a set of collaborators familiar with aspects of the research for which your team does not already have expertise.   Hypothesis Development and Big Molecular Data  Clearly defining the experimental hypothesis will ease the process of evaluating the available tools and techniques.  Finding a collaborator to help identify if any biological feature supports or rejects your hypothesis can be critical when using large scale molecular data.  Multiple testing, difficulty determining biological significance, and power issues are common for large data sets where the number of measurements (p) is much larger than the sample size (n). There are different groups available at the Fred Hutch who collaborate with researchers using large scale datasets and can provide valuable insight into study design, data types and hypothesis generation as well as assistance with downstream analytics.   Multiple Testing  Statistically, multiple testing is defined as the testing of more than one hypothesis at a time.  Pragmatically, this changes ‘omics significance levels based on the breadth of your assay and the number of different cohort arrangements on which statistical tests are run. The resources here will help you understand the broad strokes before consulting with a statistician about your project’s specific details.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/study_overview/",
        "teaser":null},{
        "title": "Proposal Preparation and the Funding Process",
        "excerpt":"   An excellent starting place for more information on starting the funding application process in general is the Fred Hutch Office of Sponsored Research site and specifically the section on sponsored projects (meaning externally-funded research).    This section will more specifically provide information related to this process with large scale molecular data generation from human specimens in mind, as requirements for documentation and timing are continually evolving.  Please let us know by emailing sciwiki if you notice NIH or other funding agency policy changes and we can update our site and refer content reviews to the relevant staff at our institution.   Using De-Identified Human Specimens for Genomics Research   For funding agencies like the NIH, the increased use of genomic data sets from human specimens outside of the context of the definition of human subjects research has resulted in some changes when submitting proposals for funding. NIH provides the Infopath Questionnaire, found here. which can help determine if the study would need to be categorized as Human Subjects Research, or if it is exempt in some way. For Fred Hutch studies that propose to generate or use data generated from human specimens for which neither the investigator nor anyone on the team can re-identify the specimens, a new Fred Hutch document helping explain why data does not qualify as Human Subjects (the Human Subjects Research determination form, to be submitted along with a Human Specimens/Data Application) is required when submitting a protocol to FH review.   Use the information in the Infopath Questionnaire to describe why your particular study is not required to be subjected to Human Subjects based restrictions and reporting. When (not if!) the proposal is funded, the investigator will not be required to address the Human Subjects reporting (such as Inclusion Reports and training), but WILL be required to address the need to share any genomic data sets used via public repositories as is consistent with the consents of the original tissue donors.   Using Identified Human Specimens for Genomics Research   The NIH currently (as of 7/12/2018) requests assurance through an Institutional Certification that investigators intend to submit any datasets generated to the appropriate public repository, such as dbGaP. For those NIH grants that specify genomic data sets will be generated from human specimens, the requirement for submitting an institutional certification will be requested as part of the JIT. NIH information for dbGaP Study Registration and Data Submission to an NIH Designated Controlled-Access Data Repository can be found here. In the JIT information the investigator will see this request:      The current policy behind this request seems to be an ever-changing set of guidelines that depending on the type of data generated as part of the study, the date upon which the human specimen was collected, and the details of the consent in place at that time all currently have implications for the process the investigator needs to address. At the time of this writing, the NIH Office of Science Policy provides this site describing Institutional Certifications, though again, even the links in the JIT document from the NIH itself are already broken, so additional searching may be required.   For NIH Submissions, it is also prudent to check GDS requirements within the specific institutes within NIH. For example, The National Institute of Allergy and Infectious Diseases (NIAID) has data sharing requirements above and beyond NIH GDS. NIAID expectations can be found here.   NIH Funding of Research Involving Genomic Data  Data Sharing Plan and the NIH Genomic Data Sharing Policy   For NIH extramural investigators, genomic data sharing plans are required to be submitted as part of an application for funding if the project proposed will entail using or generating large scale genomic data sets.   Elements of a Genomic Data Sharing Plan:           Data Type            Data Repository            Data Submission and Release Timeline            IRB Assurance of the Genomic Data Sharing Plan            Appropriate Uses of the Data            If necessary, Request for an Exception to Submission       NIH Just-In-Time (JIT) Information   JIT allows applicants and signing officials to submit certain elements of a competing grant application after the peer review and as the application is being considered for funding. This includes information on Institutional Review Board (IRB) approval of the use of human subjects approval (or verification of IACUC’s approval of the proposed use of live vertebrate animals, etc). Upon the return of scientific review panel percentile rankings of an NIH proposal submitted to the NIH, if the percentile rank is in the range that is potentially fundable (typically an impact score of 30 or less), investigators will receive a request for more information called the Just-In-Time (JIT) Request. To some degree, the investigator can decide, given the Institute to which they applied, their particular percentile ranking and the previous paylines of that particular Institute, whether it is beneficial to immediately follow-up action to respond to the JIT is needed or to wait. You can learn more about support for Fred Hutch investigators regarding their JIT requests and using the Hutch Grants system to manage the submission of information, including from the NIH and Department of Defense at this Centernet site from the Office of Sponsored Projects.     NIH Application Revision on January 15, 2018   For NIH grant applications submitted after January 25, 2018, the application entitled “Form D” was updated to “Form E”. An annotated instruction sheet on Form E is here. Form E highlights include:           Consolidation of human subjects, inclusion enrollment, and clinical trial information previously collected across multiple agency forms       Expansion and use of discrete form fields for clinical trial information to:            provide the level of information needed for peer review;       lead applicants through clinical trial information collection requirements;       present key information to reviewers and agency staff in a consistent format; and       align with ClinicalTrials.gov (where possible) and position us for future data exchange with ClinicalTrials.gov           Incorporation of recent Grants.gov changes to R&amp;R Budget and SBIR/STTR Information forms   Additional Resources          A list of federal agencies involved in Genomic Research is listed at the NIH National Human Genome Research Institute.            The NIH Office of Science Policy FAQ on Genomic Data Sharing Policy can be found here.            NIH information for dbGaP Study Registration and Data Submission to an NIH-Designated Controlled-Access Data Repository can be found here.            The Fred Hutch Policy on IRB Review of Genomic Data Sharing Policies can be found here. This policy provides an overview of FH requirements and pathways for submitting for review proposals including genomic data.            A full NIH Genomic Data Sharing Plan plan template can be found here.            Information about all of the Fred Hutch Shared Resources in PHS 398 or SF 424 (R&amp;R) formats can be found here.            The Shared Resources Quick Reference list (current version 2017) is a useful list of existing resources and the relevant contact information which may be required or helpful for a new study.            Information about the data management and statistical assistance  provided by Collaborative Data Services can be found here.            Boilerplate text for proposals that require a description of what facilities, shared resources, instruments and services are available at the Fred Hutch can be obtained at the Shared Resources site.            A description of computational and storage resources from Scientific Computing for grant writers can be found here.      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/generation/study_proposal/",
        "teaser":null},{
        "title": "R Tips & Tricks",
        "excerpt":"Below are some general tips and tricks for working in R that don’t fit neatly into any articles.   Installing R packages from CRAN vs. Bioconductor vs. GitHub          You can’t use the CRAN instructions to install a BioC package, but you CAN do the reverse.  So if you have a bunch of packages to install, and some are in BioC and others are in CRAN,  you can install them all in one command if you use the BioC installation mechanism.            When installing the cutting-edge version of package X (where X is available in CRAN or Bioconductor  but the latest version is only available in GitHub), it’s recommended to first install the package via CRAN or Bioconductor,  then installing the new version of it using devtools::install_github(X). The reason is that devtools::install_github()  does not always install all the necessary dependencies      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/infdemos/R_tips_tricks/",
        "teaser":null},{
        "title": "Fred Hutch Inspired GitHub Pages Websites",
        "excerpt":"The Biomedical Data Science Wiki site, as well as the Fred Hutch Bioinformatics Interest Group are now using a user-customized common theme that is Fred Hutch “inspired”, though not official via Fred Hutch Communications and Marketing.  One important use of GitHub repositories for Bioinformatics is the ability to use GitHub Pages to spin up custom documentation or other content containing websites.  While the theme you use is completely up to you, after much tweaking this site and FHBig, we have created a Fred Hutch GitHub repository that contains a template website that will allow you to hit the ground running with your doc’s sites while also making the sites look a bit more Fred Hutch inspired.   The repository can be found here, at the Fred Hutch FH-minimal-mistakes repo.  It is a customized version of the theme Minimal Mistakes and uses this theme as a remote theme (meaning when the developer updates their site, yours will update too, forcing the majority of the maintenance to the developer rather than you!).   Of note, if you want to retain the Fred Hutch colors, you MUST leave this line in the config.yml unchanged:  minimal_mistakes_skin: \"contrast\"  If you change the Minimal Mistakes skin to something other than “contrast” you will lose the Fred Hutch colors.   Please contact Amy Paguirigan (apaguiri, sciwiki) or Chao-Jen Wong (cwon2, FHBig) for advice or commiseration in using this theme/repo, or go to the FHBig Slack Workspace (see the link at the bottom of this page), and ask your questions in the #githubteam channel.  Given that this was simply community service effort, our documentation on how to use the theme repo is minimal at this point but we will endeavor to keep updating it as we continue to use the theme as well.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/infdemos/github_pages-FHtheme/",
        "teaser":null},{
        "title": "How to Beagle",
        "excerpt":"Beagle clusters serve as an extension of gizmo into AWS cloud infrastructure. It is an alternative way to send your sbatch jobs to clusters other than gizmo. The transition is easy: use sbatch in a similar manner as on gizmo and add -M beagle to the command. The complete beagle cluster user guide is here on SciComp’s wiki. Below is a just quick start.   What is Beagle?   Beagle can be seen as an extension of our on-premise computing cluster gizmo into AWS cloud infrastructure.  To make transitioning as easy as possible for users, we’ve retained the familiar Slurm workload manager as well as extending on-campus storage to this system.  Thus, your files - shared fast directories, your home directory, the shared app tools directory - are all available at the same paths as on the gizmo compute nodes.   One significant difference in file systems is that the scratch file system is unique to Beagle and is in a different path.  The scratch directory created by slurm is in /mnt/beagle/scratch.  Temporary storage is available in /mnt/beagle/delete10 with subdirectories based on PI or group name.   Thus, all that is typically necessary are small changes to the Slurm commands to enable your jobs to run on Beagle nodes.      Note:  Access to data is much slower than access on campus.  On IO intensive workloads you may see up to a 3x slowdown on overall time to complete a job. See the section “Improving Data Performance” for further information on addressing this performance bottleneck.    Basic use   To summit a job, use sbatch in a similar manner as on gizmo and add -M beagle to the command:   sbatch -M beagle -n1 my_job.sh   This will request one CPU on an F class node (16GB RAM and four cores).  This job will share CPU and memory with other jobs.   Managing jobs   Similarly, use squeue, sacct and scancel and add -M beagle to the command.   squeue -M beagle -u &lt;my_username&gt;      Note that srun and salloc do not support the -M option: for interactive use you need to first log into the host fitzroy where you can run any of the Slurm commands without -M.    Partitions   There are three classes available: F, G  and H, each of which has 16GB, 60GB and 244GB of RAM, respectively. The default partition is campus which contains only F class nodes.  The other two classes of nodes are in the largenode partition.                  Class       CPUs       RAM       Partition                       F       4       16GB       campus, c5.2xlarge                 G       18       60 GB       largenode, c5.9xlarge                 H       16       244GB       largenode, r4.8xlarge           Use -p &lt;partition name&gt; to select the partition.  When selecting the largenode partition you will get a G node unless you request more memory than available on the G class.   Limits   Limits on Beagle are enforced in the same way as they are on gizmo: 300 core limit per PI.  The limits are typically higher and can be increased upon request.   Improving Performance   Cache-Fast   Access to data in Beagle is currently a significant bottleneck for job performance.  To address this we are making available what we’re calling cache-fast on the beagle nodes.  This is a read only and day old copy of some fast file directories.  The primary purpose of this file server is as a disaster-recovery copy of data, but we’ve re-purposed it to improve data access performance.  It’s available under the path /fh/cache-fast and from that point has the same structure as fast file.   The real problem with using fast file in Beagle is read performance as write performance is actually quite reasonable.  Thus, you will see significant improvement by using the cache-fast directory to read in data and the fast directory to write out results.  The process would be something like:      read in data from /fh/cache-fast/directory_p/path/to/data/file   process data   write data out to /fh/fast/directory_p/path/to/results   Read performance is significantly improved, up to 10 times faster.  However, it is important to note that this cached view can be up to 24 hours old.  There is a nightly process that synchronizes fast file to the cache.   Staging into Scratch   There is scratch space available in the path /mnt/beagle/delete10 that can be used to stage data into and out of Beagle.  This is a file server that is in the same location as Beagle so we have much better performance compared to accessing fast directly.   Examples   Basic: partition on campus  (F class)   sbatch -M beagle my_job.sh   Partition on G class without sharing   sbatch -M beagle --exclusive -n1 -p largenode my_job.sh   If --exclusive and -p largenode are set, you get the whole computer with 18 cores and 60 GB RAM (if assigned a G class node) and will not share with others.   Partition on G class with one task and a few cores:   sbatch -M beagle -n1 -c4 -p largenode my_job.sh   This assigns four cores to your job.  Without using --exclusive the job may share CPU and memory with other jobs.   Get a larger allocation on an H node   sbatch -M beagle --mem=200G -p largenode my_job.sh   Note that when you add the memory request your job will be limited to that amount- if it should exceed 200GB, the job will be terminated.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/infdemos/how_to_beagle/",
        "teaser":null},{
        "title": "Intro to plyranges (Bioconductor)",
        "excerpt":"plyranges provides dplyr-style operations to genomic range data infrastructure in Bioconductor. Spending 15 - 20 minutes going over this demo, you may find how plyranges enables us to create more clean, readable and reproducible codes for genomic data analysis.   This demo is extracted from a chapter in Bioconductor 2018 Workshop Compilation - Fluent Genomic Data Analysis with Plyranges.   Setup   It requires R &gt;= 3.5.0 to install BiocManager and plyanges. BiocManager is a new package for Bioconductor package management.   install.packages(\"BiocManager\") library(BiocManager) install(\"plyranges\"\")   Invoke R or Rstudio on rhino   If you intend to work on rhino, ml R and Rstudio.           Connect to rhino:       &gt; ssh -X HutchID@rhino                Load R and Rstudio modules (R &gt;= 3.5.0):       &gt; ml R/3.5.0-foss-2016b-fh1 &gt; ml rstudio/1.1.383 &gt; rstudio &amp;                In Rstudio, load the library       library(plyranges, quietly=TRUE)   Start with GRanges   GRanges is the basic, core genomic range data structure of Bioconductor. It has two core components:      seqname, ranges, strands columns (left side of the dotted line)   metadata columns: annotation, covariates (right side of the dotted line)        ## GRanges object with 6 ranges and 1 metadata column:     ##       seqnames    ranges strand |     gene_id     ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;     ##   [1]       VI 3322-3846      - |     YFL064C     ##   [2]       VI 3030-3338      - |     YFL065C     ##   [3]       VI 1437-2615      - |     YFL066C     ##   [4]       VI 5066-5521      + |     YFL063W     ##   [5]       VI 6426-7565      + |     YFL062W     ##   [6]       VI  836-1363      + |     YFL067W     ##   -------     ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   Construct a GRanges object   The conventional way to create a GRanges object is using Constructor GRanges():   #' Genomic range columns gr &lt;- GRanges(seqnames=\"VI\",               IRanges(start=c(3322, 3030, 1437,  5066, 6426, 836),                       end = c(3846, 3338, 2615, 5521, 7565, 1363)),               strand = c(\"-\", \"-\", \"-\", \"+\", \"+\", \"+\")) #' define the metadata columns mcols(gr) &lt;-   DataFrame(gene_id=c(\"YFL064C\", \"YFL065C\", \"YFL066C\",                       \"YFL063W\", \"YFL062W\", \"YFL067W\"))   Or we can use plyranges::as_granges() to create a GRanges object:   library(plyranges, quietly = TRUE) genes &lt;- data.frame(seqnames = \"VI\",                     start = c(3322, 3030, 1437,  5066, 6426, 836),                     end = c(3846, 3338, 2615, 5521, 7565, 1363),                     strand = c(\"-\", \"-\", \"-\", \"+\", \"+\", \"+\"),                     gene_id=c(\"YFL064C\", \"YFL065C\", \"YFL066C\",                               \"YFL063W\", \"YFL062W\", \"YFL067W\"),                     stringsAsFactors = FALSE) gr &lt;- as_granges(genes)   Core verbs   The code chunks perform actions on GRagnes objects using some verbs defined by plyranges grammer.   mutate()   Use plyranges::mutate() to add metadata columns:   set.seed(2018-07-28) gr2 &lt;- gr %&gt;%   mutate(gene_type = \"ORF\",          gc_content = runif(n())) %&gt;%   filter(width &gt; 400) gr2   ## GRanges object with 5 ranges and 3 metadata columns: ##       seqnames    ranges strand |     gene_id   gene_type ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; ##   [1]       VI 3322-3846      - |     YFL064C         ORF ##   [2]       VI 1437-2615      - |     YFL066C         ORF ##   [3]       VI 5066-5521      + |     YFL063W         ORF ##   [4]       VI 6426-7565      + |     YFL062W         ORF ##   [5]       VI  836-1363      + |     YFL067W         ORF ##              gc_content ##               &lt;numeric&gt; ##   [1]  0.49319754820317 ##   [2] 0.216616344172508 ##   [3] 0.747259315103292 ##   [4] 0.907683959929273 ##   [5] 0.221016310621053 ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   filter()   plyranges::filter() returns ranges if the expression evaluates to TRUE.   gr2 %&gt;%   filter(strand == \"+\", gc_content &gt; 0.5)   ## GRanges object with 2 ranges and 3 metadata columns: ##       seqnames    ranges strand |     gene_id   gene_type ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; ##   [1]       VI 5066-5521      + |     YFL063W         ORF ##   [2]       VI 6426-7565      + |     YFL062W         ORF ##              gc_content ##               &lt;numeric&gt; ##   [1] 0.747259315103292 ##   [2] 0.907683959929273 ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   summarise()   plyranges::summarise() performs tasks and return DataFrame.   gr2 %&gt;%   summarise(avg_gc = mean(gc_content),             n = n())   ## DataFrame with 1 row and 2 columns ##              avg_gc         n ##           &lt;numeric&gt; &lt;integer&gt; ## 1 0.517154695605859         5   group_by()   group_by() acts on each group on ragnes defined by the value:   gr2 %&gt;%   group_by(strand) %&gt;%   summarise(avg_gc = mean(gc_content),             n = n())   ## DataFrame with 2 rows and 3 columns ##   strand            avg_gc         n ##    &lt;Rle&gt;         &lt;numeric&gt; &lt;integer&gt; ## 1      + 0.625319861884539         3 ## 2      - 0.354906946187839         2   group_by() causes verbs to behave differently.   by_strand &lt;- gr2 %&gt;%   group_by(strand) by_strand   ## GRanges object with 5 ranges and 3 metadata columns: ## Groups: strand [2] ##       seqnames    ranges strand |     gene_id   gene_type ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; ##   [1]       VI 3322-3846      - |     YFL064C         ORF ##   [2]       VI 1437-2615      - |     YFL066C         ORF ##   [3]       VI 5066-5521      + |     YFL063W         ORF ##   [4]       VI 6426-7565      + |     YFL062W         ORF ##   [5]       VI  836-1363      + |     YFL067W         ORF ##              gc_content ##               &lt;numeric&gt; ##   [1]  0.49319754820317 ##   [2] 0.216616344172508 ##   [3] 0.747259315103292 ##   [4] 0.907683959929273 ##   [5] 0.221016310621053 ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   Now the verb works within each group, instead of in a GRanges object.   by_strand %&gt;%   filter(n() &gt; 2)   ## GRanges object with 3 ranges and 3 metadata columns: ## Groups: strand [1] ##       seqnames    ranges strand |     gene_id   gene_type ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; ##   [1]       VI 5066-5521      + |     YFL063W         ORF ##   [2]       VI 6426-7565      + |     YFL062W         ORF ##   [3]       VI  836-1363      + |     YFL067W         ORF ##              gc_content ##               &lt;numeric&gt; ##   [1] 0.747259315103292 ##   [2] 0.907683959929273 ##   [3] 0.221016310621053 ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   Find mean gc content within each group:   by_strand %&gt;%   mutate(avg_gc_strand = mean(gc_content))   ## GRanges object with 5 ranges and 4 metadata columns: ## Groups: strand [2] ##       seqnames    ranges strand |     gene_id   gene_type ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; ##   [1]       VI 3322-3846      - |     YFL064C         ORF ##   [2]       VI 1437-2615      - |     YFL066C         ORF ##   [3]       VI 5066-5521      + |     YFL063W         ORF ##   [4]       VI 6426-7565      + |     YFL062W         ORF ##   [5]       VI  836-1363      + |     YFL067W         ORF ##              gc_content     avg_gc_strand ##               &lt;numeric&gt;         &lt;numeric&gt; ##   [1]  0.49319754820317 0.354906946187839 ##   [2] 0.216616344172508 0.354906946187839 ##   [3] 0.747259315103292 0.625319861884539 ##   [4] 0.907683959929273 0.625319861884539 ##   [5] 0.221016310621053 0.625319861884539 ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   ungroup()   ungroup()   by_strand %&gt;%   ungroup()   ## GRanges object with 5 ranges and 3 metadata columns: ##       seqnames    ranges strand |     gene_id   gene_type ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; ##   [1]       VI 3322-3846      - |     YFL064C         ORF ##   [2]       VI 1437-2615      - |     YFL066C         ORF ##   [3]       VI 5066-5521      + |     YFL063W         ORF ##   [4]       VI 6426-7565      + |     YFL062W         ORF ##   [5]       VI  836-1363      + |     YFL067W         ORF ##              gc_content ##               &lt;numeric&gt; ##   [1]  0.49319754820317 ##   [2] 0.216616344172508 ##   [3] 0.747259315103292 ##   [4] 0.907683959929273 ##   [5] 0.221016310621053 ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   select()   select() selects the metadata columns.   gr2 %&gt;%   select(gene_id, gene_type)   ## GRanges object with 5 ranges and 2 metadata columns: ##       seqnames    ranges strand |     gene_id   gene_type ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; ##   [1]       VI 3322-3846      - |     YFL064C         ORF ##   [2]       VI 1437-2615      - |     YFL066C         ORF ##   [3]       VI 5066-5521      + |     YFL063W         ORF ##   [4]       VI 6426-7565      + |     YFL062W         ORF ##   [5]       VI  836-1363      + |     YFL067W         ORF ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   Arithmatic   Operation on the ranges - extend the width (to end?)   gr %&gt;%   mutate(width = width + 1)   ## GRanges object with 6 ranges and 1 metadata column: ##       seqnames    ranges strand |     gene_id ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; ##   [1]       VI 3322-3847      - |     YFL064C ##   [2]       VI 3030-3339      - |     YFL065C ##   [3]       VI 1437-2616      - |     YFL066C ##   [4]       VI 5066-5522      + |     YFL063W ##   [5]       VI 6426-7566      + |     YFL062W ##   [6]       VI  836-1364      + |     YFL067W ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   Arithmatic   plyranges has Arithmatic actions to modify genomic regions. An example to extend the width of the range from the “end” of the range:   gr %&gt;%   anchor_end() %&gt;%   mutate(width = width + 1)   ## GRanges object with 6 ranges and 1 metadata column: ##       seqnames    ranges strand |     gene_id ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; ##   [1]       VI 3321-3846      - |     YFL064C ##   [2]       VI 3029-3338      - |     YFL065C ##   [3]       VI 1436-2615      - |     YFL066C ##   [4]       VI 5065-5521      + |     YFL063W ##   [5]       VI 6425-7565      + |     YFL062W ##   [6]       VI  835-1363      + |     YFL067W ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   Anchor points include the start anchor_start(), end anchor_end(), midpoint anchor_center(), 3’ end anchor_3p() and 5’ end anchor_5p():   Genomic aggregation   reduce_range()   reduce_range() aggregates nearby neighbors:   gr %&gt;% reduce_ranges()   ## GRanges object with 5 ranges and 0 metadata columns: ##       seqnames    ranges strand ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; ##   [1]       VI  836-1363      * ##   [2]       VI 1437-2615      * ##   [3]       VI 3030-3846      * ##   [4]       VI 5066-5521      * ##   [5]       VI 6426-7565      * ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   Find out which genes are overlapping each other by aggregating over the gene_id column and storing the result in a List column:   gr %&gt;%   reduce_ranges(gene_id = List(gene_id))   ## GRanges object with 5 ranges and 1 metadata column: ##       seqnames    ranges strand |         gene_id ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;CharacterList&gt; ##   [1]       VI  836-1363      * |         YFL067W ##   [2]       VI 1437-2615      * |         YFL066C ##   [3]       VI 3030-3846      * | YFL065C,YFL064C ##   [4]       VI 5066-5521      * |         YFL063W ##   [5]       VI 6426-7565      * |         YFL062W ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   disjoin_ranges()   disjoin_ranges() takes the union of end points over all ranges, and results in an expanded range:   gr %&gt;%   disjoin_ranges(gene_id = List(gene_id))   ## GRanges object with 7 ranges and 1 metadata column: ##       seqnames    ranges strand |         gene_id ##          &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;CharacterList&gt; ##   [1]       VI  836-1363      * |         YFL067W ##   [2]       VI 1437-2615      * |         YFL066C ##   [3]       VI 3030-3321      * |         YFL065C ##   [4]       VI 3322-3338      * | YFL064C,YFL065C ##   [5]       VI 3339-3846      * |         YFL064C ##   [6]       VI 5066-5521      * |         YFL063W ##   [7]       VI 6426-7565      * |         YFL062W ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   Overlap   To demonstrate how overlap action works, we first construct some GRanges objects:   set.seed(66+105+111+99+49+56) pos &lt;- sample(1:10000, size = 100) size &lt;- sample(1:3, size = 100, replace = TRUE) rep1 &lt;- data.frame(chr = \"VI\", pos = pos,                   size = size,                   X = rnorm(100, mean = 2),                   Y = rnorm(100, mean = 1)) rep2 &lt;- data.frame(chrom = \"VI\", st = pos,                   width = size,                   X = rnorm(100, mean = 0.5, sd = 3),                   Y = rnorm(100, sd = 2)) rep3 &lt;- data.frame(chromosome = \"VI\",                   start = pos, width = size,                   X = rnorm(100, mean = 2, sd = 3),                   Y = rnorm(100, mean = 4, sd = 0.5))   Next convert data.frame to GRanges:   rep1 &lt;- as_granges(rep1, seqnames = chr,                    start = pos, width = size) rep2 &lt;- as_granges(rep2, seqnames = chrom, start = st) rep3 &lt;- as_granges(rep3, seqnames = chromosome)   Finally, constuct the final GRanges using arrange() and bind_ranges().   #' construct the final GRanges intensities &lt;- bind_ranges(rep1, rep2, rep3,                            .id = \"replicate\") arrange(intensities, start)   ## GRanges object with 300 ranges and 3 metadata columns: ##         seqnames    ranges strand |                  X                  Y ##            &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; |          &lt;numeric&gt;          &lt;numeric&gt; ##     [1]       VI    99-100      * |   2.18077108319727   1.15893283880961 ##     [2]       VI    99-100      * |  -1.14331853023759  -1.84545382593297 ##     [3]       VI    99-100      * |   4.42535734042167   3.53884540635964 ##     [4]       VI   110-111      * |   1.41581829875993 -0.262026041514519 ##     [5]       VI   110-111      * | 0.0203313104969627  -1.18095384044377 ##     ...      ...       ...    ... .                ...                ... ##   [296]       VI 9671-9673      * |  0.756423808063998  -0.24544579405238 ##   [297]       VI 9671-9673      * |  0.715559817063897    4.6963376859667 ##   [298]       VI 9838-9839      * |   1.83836043312615  0.267996156074214 ##   [299]       VI 9838-9839      * |  -4.62774336616852  -3.45271032367217 ##   [300]       VI 9838-9839      * | -0.285141455604857   4.16118336728783 ##           replicate ##         &lt;character&gt; ##     [1]           1 ##     [2]           2 ##     [3]           3 ##     [4]           1 ##     [5]           2 ##     ...         ... ##   [296]           2 ##   [297]           3 ##   [298]           1 ##   [299]           2 ##   [300]           3 ##   ------- ##   seqinfo: 1 sequence from an unspecified genome; no seqlengths   filter_by_overalsp()   filter_by_overlaps(query, subject) - related to GenomicRanges::findOverlaps() and subsetOverlaps()   olap &lt;- filter_by_overlaps(intensities, gr) length(olap)   ## [1] 108   intensities has 300 ranges and 108 are overlapping with gr.   More overlap actions   join_overlap_*(query, subject): how query and subject overlap: join_overlap_inner(query, subject), join_overlap_left() and join_overlap_intersect().   olap &lt;- join_overlap_inner(intensities, gr)   The returned Hit object obtains the ranges that the query and subject overlapp within.   R session info   sessionInfo()   ## R version 3.5.0 (2018-04-23) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.5 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               ##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     ##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8    ##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  ##  [9] LC_ADDRESS=C               LC_TELEPHONE=C             ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C        ## ## attached base packages: ## [1] stats4    parallel  stats     graphics  grDevices utils     datasets ## [8] methods   base      ## ## other attached packages: ## [1] plyranges_1.0.3      GenomicRanges_1.32.2 GenomeInfoDb_1.16.0 ## [4] IRanges_2.14.6       S4Vectors_0.18.1     BiocGenerics_0.26.0 ## [7] BiocInstaller_1.30.0 ## ## loaded via a namespace (and not attached): ##  [1] Rcpp_0.12.16                pillar_1.2.2                ##  [3] compiler_3.5.0              XVector_0.20.0              ##  [5] bindr_0.1.1                 bitops_1.0-6                ##  [7] tools_3.5.0                 zlibbioc_1.26.0             ##  [9] digest_0.6.15               tibble_1.4.2                ## [11] evaluate_0.10.1             lattice_0.20-35             ## [13] pkgconfig_2.0.1             rlang_0.2.0                 ## [15] Matrix_1.2-14               DelayedArray_0.6.0          ## [17] yaml_2.1.19                 bindrcpp_0.2.2              ## [19] GenomeInfoDbData_1.1.0      rtracklayer_1.40.2          ## [21] stringr_1.3.0               dplyr_0.7.4                 ## [23] knitr_1.20                  Biostrings_2.48.0           ## [25] tidyselect_0.2.4            rprojroot_1.3-2             ## [27] grid_3.5.0                  glue_1.2.0                  ## [29] Biobase_2.40.0              R6_2.2.2                    ## [31] XML_3.98-1.11               BiocParallel_1.14.2         ## [33] rmarkdown_1.9               purrr_0.2.4                 ## [35] tidyr_0.8.0                 magrittr_1.5                ## [37] backports_1.1.2             Rsamtools_1.32.2            ## [39] htmltools_0.3.6             matrixStats_0.53.1          ## [41] GenomicAlignments_1.16.0    assertthat_0.2.0            ## [43] SummarizedExperiment_1.10.0 stringi_1.2.2               ## [45] RCurl_1.95-4.10  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/infdemos/plyranges-workshop/",
        "teaser":null},{
        "title": "Python 3.6.5 Software Release",
        "excerpt":"I’m happy to announce the latest Python release for the Gizmo cluster.  This is our biggest and best Python release to date. To access use the modules command. ml Python/3.6.5-foss-2016b-fh3   New modules: Bitstring 3.1.5 is a pure Python module designed to help make the creation and analysis of binary data as simple and natural as possible.   NanoPlot 1.13.0 Plotting suite for Oxford Nanopore sequencing data and alignments   Albacore 2.3.1 A basecaller, that identifies DNA sequences directly from raw data, rather than utilizing an intermediary stage called ‘event detection’. This upgrade enhances accuracy of the single-read sequence data, contributing to high consensus accuracy for nanopore sequence data.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/scicompannounce/2018-05-26-pythonupdate/",
        "teaser":null},{
        "title": "June SciComp Updates",
        "excerpt":"Latest R:  R/3.5.0-foss-2016b-fh1 Latest Python:  Python/3.6.5-foss-2016b-fh3   For a complete list of modules built with R and Python visit: https://fredhutch.github.io/easybuild-life-sciences/   TeXlive Has been updated to version 2018.05.31.  This new version uses libraries that are compatible with our newest versions of R-3.4.2, 3.4.3 and 3.5.0.  If you have been getting errors with the pdflatex command please try this new build of texlive. Module name: texlive/20180531-foss-2016b   Python-3.6.0 fh3 - 155 Updated packages, 25 new packages. User requested packages: https://github.com/wdecoster/nanostat https://github.com/rrwick/Porechop https://github.com/wdecoster/NanoPlot   Nanopolish - Software package for signal-level analysis of Oxford Nanopore sequencing data. Nanopolish can calculate an improved consensus sequence for a draft genome assembly, detect base modifications, call SNPs and indels with respect to a reference genome and more. Module name: nanopolish/0.7.1-foss-2016b   Clairvoyante - A deep neural network based variant caller. Requires DISPLAY to be set. Only works with Python2. Module name: Clairvoyante/0.1   Assembly Stats Get assembly statistics from FASTA and FASTQ files. Module name: assembly-stats/1.0.1-foss-2016b   Minimap2 is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database. Typical use cases include: (1) mapping PacBio or Oxford Nanopore genomic reads to the human genome; (2) finding overlaps between long reads with error rate up to ~15%; (3) splice-aware alignment of PacBio Iso-Seq or Nanopore cDNA or Direct RNA reads against a reference genome; (4) aligning Illumina single- or paired-end reads; (5) assembly-to-assembly alignment; (6) full-genome alignment between two closely related species with divergence below ~15%. Module Name: minimap2/2.10-foss-2016b   IUPred Intrinsically unstructured/disordered proteins have no single well-defined tertiary structure in their native, functional state. Our server recognizes such regions from the amino acid sequence based on the estimated pairwise energy content. The underlying assumption is that globular proteins are composed of amino acids which have the potential to form a large number of favorable interactions, whereas intrinsically unstructured proteins (IUPs) adopt no stable structure because their amino acid composition does not allow sufficient favorable interactions to form.                        Modulename iupred/1.0-GCC-5.4.0-2.26   RAxML Randomized Axelerated Maximum Likelihood. Is a program for sequential and parallel Maximum Likelihood based inference of large phylogenetic trees. It can also be used for post analyses of sets of phylogenetic trees, analyses of alignments and, evolutionary placemen of short reads. Modulename: RAxML/8.2.11-foss-2016b-hybrid-avx2   Tesseract is an OCR engine.  Tesseract was originally developed at Hewlett-Packard Laboratories Bristol and at Hewlett-Packard Co, Greeley Colorado between 1985 and 1994, with some more changes made in 1996 to port to Windows, and some C++izing in 1998. In 2005 Tesseract was open sourced by HP. Since 2006 it is developed by Google. Modulename: tesseract/4.0.0-beta.1-foss-2016b   Pindel - Pindel can detect breakpoints of large deletions, medium sized insertions, inversions, tandem duplications and other structural variants at single-based resolution from next-gen sequence data. It uses a pattern growth approach to identify the breakpoints of these variants from paired-end short reads. Module name: Pindel/0.2.5b8-foss-2016b   R 3.5.0 The latest and largest R yet built. Over 800 packages. Updated with packages from BioConductor 3.7. Module Name: R/3.5.0-foss-2016b-fh1   Python 2.7.15 268 Packages, 187 Updated Packages Module Name: Python/2.7.15-foss-2016b-fh1   Leptonica Graphic libraries for image processing and image analysis applications. Module name: leptonica/1.75.3-foss-2016b   RGEOS  R- Interface to Geometry Engine - Open Source (‘GEOS’) using the C ‘API’ for topology operations on geometries. The ‘GEOS’ library is external to the package, and, when installing the package from source, must be correctly installed first. Windows and Mac Intel OS X binaries are provided on ‘CRAN’. Based on package GEOS-3.6.2 and R-3.5.0 Moduel Name: rgeos/0.3-26-foss-2016b-R-3.5.0   awscli/1.15.16 updated. ASW CLI changes almost weekly.  We are constantly updating this one package.   Steel Bank Common Lisp (SBCL) is a high-performance Common Lisp compiler. It is open source / free software, with a permissive license. In addition to the compiler and runtime system for ANSI Common Lisp, it provides an interactive environment including a debugger, a statistical profiler, a code coverage tool, and many other extensions. Module Name: sbcl/1.4.6   Sniffles is a structural variation caller using third generation sequencing (PacBio or Oxford Nanopore). It detects all types of SVs (10bp+) using evidence from split-read alignments, high-mismatch regions, and coverage analysis. Please note the current version of Sniffles requires sorted output from BWA-MEM (use -M and -x parameter) or NGMLR with the optional SAM attributes enabled! Module Name: Sniffles/1.0.8-foss-2016b   NanoSV NanoSV is a software package that can be used to identify structural genomic variations in long-read sequencing data, such as data produced by Oxford Nanopore Technologies’ MinION, GridION or PromethION instruments, or Pacific Biosciences sequencers Module Name: NanoSV/1.1.2-foss-2016b   SAMtools, BCFtools, HTSlib Samtools is a suite of programs for interacting with high-throughput sequencing data.  All three have been updated to version 1.8.   Python 3.6.5 111 Updates from 3.6.5-fh2.  8 New packages. 460 total packages Module Name: Python/3.6.5-foss-2016b-fh1  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/scicompannounce/2018-06-01-JuneComputingUpdates/",
        "teaser":null},{
        "title": "High Performance Computing Systems Upgrade 2019 Announcement",
        "excerpt":"The operating system used on all High Performance Computing (Gizmo, Rhino, etc) will reach end-of-life later this spring. As a result, Scientific Computing will need to upgrade the operating system on all High Performance Computing servers used at Fred Hutch this spring. (Note: the current operating system has been in use for over three years.)   For this upgrade we plan to use a phased approach that will not require taking the entire computing environment offline at once. Instead it will allow for continual use of the computing environment while it is upgraded.   Upgrade Schedule      Testing Phase: 3/1 - 4/1/2019:            Test cluster is available to try out new computing environment       This will be the best time to try out your workflow and tools in the new computing environment       SciComp will be available to advise, assist, and help with this process           Transition Phase: 4/1 - 5/1/2019:            High Performance Computing servers transitioned to new computing environment       Rhino and NoMachine servers converted to new computing environment       85% of Gizmo’s compute capacity converted to new computing environment       Beagle and Koshu converted to new computing environment           Cleanup Phase: 6/1 - 7/1/2019:            Scientific Computing staff will work with anyone still using old compute environment and assist to transition to new compute environment           What will be changing   During this upgrade we will be upgrading the operating system used on each server to Ubuntu 18.04LTS. There will be no change to the hardware currently used in our HPC environment or change in the computing capacity.   This operating system upgrade requires that we rebuild the tools in the /app file system on the new OS using a new toolchain. At this time, we don’t know the full extent of the changes required at this time. Additional information will be provided as this work proceeds.   Future Communications   Throughout this upgrade Scientific Computing staff will send announcements, additional technical details and schedule updates via the scicomp_announce mailing list and the Scientific Computing Announcements page on SciWiki. Scientific Computing staff will also hold weekly open houses where you can come and run your jobs on the new operating system. If any problems are found, we will be on-hand to help you update your jobs and get them working.   As always, please email us at scicomp at fhcrc.org with any questions and concerns.   Additional Technical Detail   Additional information will be added here shortly   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/scicompannounce/2019-01-22-hpc-systems-upgrade-2019/",
        "teaser":null},{
        "title": "Environment Modules after Upgrade",
        "excerpt":"Hello to all Data Scientists and HPC Users!   We hope you are already aware of our upcoming Operating System (OS) upgrade - we are updating all Linux systems from Ubuntu 14.04 to Ubuntu 18.04.   First Announcement   Second Announcement   There will be changes to our Environment Modules. Please refresh your understanding of Environment Modules.   Environment Module Information   The list of available Environment Modules on the new OS is different, and this may affect your scripts.   18.04 Environment Modules   This is the best way to avoid problems:   1) Refresh your understanding of Environment Modules    2) Check your use of Environment Modules in:        - scripts        - login configuration(s)(~/.bashrc, ~/.bash_profile, etc.)        - manual load/interactive shell sessions        - terminal multiplexer shell sessions (screen and/or tmux)    3) Find the same or replacement module on the 18.04 list    4) Prepare an updated copy of your script, configuration, and/or procedure    5) Test that copy when you see we have 18.04 test systems ready for use (soon)   There are two main cases where a new module may not work for you:   1) Established, ongoing jobs that need scientific reproducibility    2) Software packages not yet available on the 18.04 module list   Please send email to scicomp@fredhutch.org or stop by one of our Office Hours if you find yourself in one of the above categories, and we will work to produce a new module for you.   Links:   OS Upgrade Announcement One   OS Upgrade Announcement Two   Environment Modules Information   18.04 (new) Environment Module list  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/scicompannounce/2019-02-21-upgrade-env-modules/",
        "teaser":null}]
